{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "04a30440",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import relevant library\n",
    "\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import pickle\n",
    "import argparse\n",
    "from collections import deque\n",
    "import time\n",
    "from datetime import timedelta\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import IterableDataset, DataLoader, get_worker_info\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "8ad74ab6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5a29fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "## time the process\n",
    "def get_time_dif(start_time):\n",
    "    \"\"\"get the running time\"\"\"\n",
    "    end_time = time.time()\n",
    "    time_dif = end_time - start_time\n",
    "    return timedelta(seconds=int(round(time_dif)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4961396",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fairness_reprogramming(self, u, i, j):\n",
    "    \"\"\"Reprogramming phase:\n",
    "        1.Freeze the user and item embedding\n",
    "        2.Calculate the perturbation to achieve fairness objective\n",
    "        3.Add perturbation to the alr frozen embedding\n",
    "        4.Calculate the overall loss function after update\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize a fix random perturbation\n",
    "    perturbation = torch.rand(1)\n",
    "    \n",
    "    #load user and item embedding, which has been trained in BPR\n",
    "    u = list(model1.items())[0][1]\n",
    "    i = list(model1.items())[1][1][i]\n",
    "    j = list(model1.items())[1][1][j]\n",
    "        \n",
    "    # Add adversarial perturbation to embeddings, now we have q+delta, p+delta\n",
    "    x_ui_adv = torch.mul(u , i + perturbation).sum(dim=1)\n",
    "    x_uj_adv = torch.mul(u , j + perturbation).sum(dim=1)\n",
    "\n",
    "    # find difference between pos and neg item, then clip value\n",
    "    x_uij_adv = torch.clamp(x_ui_adv - x_uj_adv,min=-80.0,max=1e8)\n",
    "\n",
    "    # Calculate loss with perturbed embedding with logsigmoid\n",
    "    log_prob = F.logsigmoid(x_uij_adv).sum()\n",
    "            \n",
    "    #set up an adversary to identify group of items\n",
    "    adversary_rs        \n",
    "            \n",
    "    # modify the adversarial loss here\n",
    "    adv_loss = self.reg_adv *(-log_prob) + loss # this is adversarial loss (equation 4 in paper)\n",
    "    adv_loss.backward()\n",
    "\n",
    "    return adv_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "3a3b93ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6040, 64])"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load the results of BPR\n",
    "model1 = (torch.load('models/01_pytorch_workflow_model_1.pth'))\n",
    "list(model1.items())[0][1].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "87f05379",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 9.3768e-04, -4.2642e-02,  2.0659e-02,  ..., -2.2226e-02,\n",
       "          3.1339e-02, -3.3840e-02],\n",
       "        [ 1.5390e-04, -3.8164e-03,  1.1938e-02,  ..., -1.9504e-03,\n",
       "         -5.5167e-03,  2.5199e-02],\n",
       "        [-1.7836e-03, -3.0050e-02, -3.0792e-02,  ...,  1.1926e-02,\n",
       "         -1.7736e-02,  6.9641e-03],\n",
       "        ...,\n",
       "        [-3.4588e-02, -1.5128e-03, -5.1930e-02,  ...,  3.3526e-02,\n",
       "         -3.9308e-03,  1.3962e-02],\n",
       "        [ 1.6954e-05, -1.2272e-02,  3.1606e-02,  ..., -2.2976e-02,\n",
       "         -1.5850e-03, -2.3360e-02],\n",
       "        [ 1.0627e-02, -2.0485e-02, -3.4491e-02,  ..., -1.7362e-02,\n",
       "          3.7945e-03,  7.2581e-03]])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model1.items())[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "f51d8700",
   "metadata": {},
   "outputs": [],
   "source": [
    "#goal = input y and yhat and return the group of the item?\n",
    "\n",
    "# Build model\n",
    "class adversary_rs(nn.Module):\n",
    "    def __init__(self, input_features, output_features, hidden_units=8):\n",
    "        \"\"\"Initializes all required hyperparameters for a multi-class classification model.\n",
    "\n",
    "        Args:\n",
    "            input_features (int): Number of input features to the model.\n",
    "            out_features (int): Number of output features of the model\n",
    "              (how many classes there are).\n",
    "            hidden_units (int): Number of hidden units between layers, default 8.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.linear_layer_stack = nn.Sequential(\n",
    "            nn.Linear(in_features=input_features, out_features=hidden_units),\n",
    "            # nn.ReLU(), # <- does our dataset require non-linear layers? (try uncommenting and see if the results change)\n",
    "            nn.Linear(in_features=hidden_units, out_features=hidden_units),\n",
    "            # nn.ReLU(), # <- does our dataset require non-linear layers? (try uncommenting and see if the results change)\n",
    "            nn.Linear(in_features=hidden_units, out_features=output_features), # how many classes are there?\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.linear_layer_stack(x)\n",
    "\n",
    "# Create an instance of BlobModel and send it to the target device\n",
    "mlp = adversary_rs(input_features=1, \n",
    "                    output_features=6, \n",
    "                    hidden_units=500).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "e0990b22",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Define the loss function and optimizer\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(mlp.parameters(), lr=1e-4)\n",
    "  # Prepare CIFAR-10 dataset\n",
    "trainloader = torch.utils.data.DataLoader(X, batch_size=512, shuffle=True, num_workers=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "48f7ccb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate accuracy (a classification metric)\n",
    "def accuracy_fn(y_true, y_pred):\n",
    "    correct = torch.eq(y_true, y_pred).sum().item() # torch.eq() calculates where two tensors are equal\n",
    "    acc = (correct / len(y_pred)) * 100 \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "864d60f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Loss: 1.73429, Acc: 40.30% | Test Loss: 1.47605, Test Acc: 44.84%\n",
      "Epoch: 10 | Loss: 1.42034, Acc: 44.80% | Test Loss: 1.40173, Test Acc: 44.84%\n",
      "Epoch: 20 | Loss: 1.39443, Acc: 45.17% | Test Loss: 1.38122, Test Acc: 45.13%\n",
      "Epoch: 30 | Loss: 1.37248, Acc: 45.83% | Test Loss: 1.35702, Test Acc: 46.02%\n",
      "Epoch: 40 | Loss: 1.35933, Acc: 47.01% | Test Loss: 1.34779, Test Acc: 46.02%\n",
      "Epoch: 50 | Loss: 1.35134, Acc: 48.19% | Test Loss: 1.34180, Test Acc: 48.67%\n",
      "Epoch: 60 | Loss: 1.34569, Acc: 47.82% | Test Loss: 1.33921, Test Acc: 48.38%\n",
      "Epoch: 70 | Loss: 1.34156, Acc: 47.97% | Test Loss: 1.33752, Test Acc: 48.97%\n",
      "Epoch: 80 | Loss: 1.33857, Acc: 48.12% | Test Loss: 1.33625, Test Acc: 48.67%\n",
      "Epoch: 90 | Loss: 1.33634, Acc: 48.12% | Test Loss: 1.33610, Test Acc: 48.67%\n",
      "Epoch: 100 | Loss: 1.33465, Acc: 48.12% | Test Loss: 1.33587, Test Acc: 48.67%\n",
      "Epoch: 110 | Loss: 1.33335, Acc: 48.12% | Test Loss: 1.33594, Test Acc: 48.67%\n",
      "Epoch: 120 | Loss: 1.33233, Acc: 48.12% | Test Loss: 1.33612, Test Acc: 48.67%\n",
      "Epoch: 130 | Loss: 1.33153, Acc: 48.19% | Test Loss: 1.33632, Test Acc: 48.97%\n",
      "Epoch: 140 | Loss: 1.33090, Acc: 48.12% | Test Loss: 1.33662, Test Acc: 48.97%\n",
      "Epoch: 150 | Loss: 1.33040, Acc: 48.04% | Test Loss: 1.33695, Test Acc: 48.97%\n",
      "Epoch: 160 | Loss: 1.33000, Acc: 48.04% | Test Loss: 1.33730, Test Acc: 48.97%\n",
      "Epoch: 170 | Loss: 1.32969, Acc: 48.12% | Test Loss: 1.33765, Test Acc: 48.67%\n",
      "Epoch: 180 | Loss: 1.32944, Acc: 48.19% | Test Loss: 1.33800, Test Acc: 48.67%\n",
      "Epoch: 190 | Loss: 1.32925, Acc: 48.19% | Test Loss: 1.33834, Test Acc: 48.67%\n",
      "Epoch: 200 | Loss: 1.32910, Acc: 48.19% | Test Loss: 1.33866, Test Acc: 48.67%\n",
      "Epoch: 210 | Loss: 1.32898, Acc: 48.19% | Test Loss: 1.33897, Test Acc: 48.67%\n",
      "Epoch: 220 | Loss: 1.32889, Acc: 48.19% | Test Loss: 1.33925, Test Acc: 48.67%\n",
      "Epoch: 230 | Loss: 1.32883, Acc: 48.19% | Test Loss: 1.33950, Test Acc: 48.67%\n",
      "Epoch: 240 | Loss: 1.32878, Acc: 48.19% | Test Loss: 1.33974, Test Acc: 48.67%\n",
      "Epoch: 250 | Loss: 1.32874, Acc: 48.19% | Test Loss: 1.33995, Test Acc: 48.67%\n",
      "Epoch: 260 | Loss: 1.32871, Acc: 48.19% | Test Loss: 1.34014, Test Acc: 48.67%\n",
      "Epoch: 270 | Loss: 1.32869, Acc: 48.19% | Test Loss: 1.34031, Test Acc: 48.67%\n",
      "Epoch: 280 | Loss: 1.32868, Acc: 48.19% | Test Loss: 1.34045, Test Acc: 48.67%\n",
      "Epoch: 290 | Loss: 1.32867, Acc: 48.19% | Test Loss: 1.34058, Test Acc: 48.67%\n",
      "Epoch: 300 | Loss: 1.32866, Acc: 48.19% | Test Loss: 1.34069, Test Acc: 48.67%\n",
      "Epoch: 310 | Loss: 1.32865, Acc: 48.19% | Test Loss: 1.34079, Test Acc: 48.67%\n",
      "Epoch: 320 | Loss: 1.32865, Acc: 48.19% | Test Loss: 1.34087, Test Acc: 48.67%\n",
      "Epoch: 330 | Loss: 1.32865, Acc: 48.19% | Test Loss: 1.34094, Test Acc: 48.67%\n",
      "Epoch: 340 | Loss: 1.32865, Acc: 48.19% | Test Loss: 1.34099, Test Acc: 48.67%\n",
      "Epoch: 350 | Loss: 1.32864, Acc: 48.19% | Test Loss: 1.34104, Test Acc: 48.67%\n",
      "Epoch: 360 | Loss: 1.32864, Acc: 48.27% | Test Loss: 1.34108, Test Acc: 48.67%\n",
      "Epoch: 370 | Loss: 1.32864, Acc: 48.27% | Test Loss: 1.34112, Test Acc: 48.67%\n",
      "Epoch: 380 | Loss: 1.32864, Acc: 48.27% | Test Loss: 1.34114, Test Acc: 48.67%\n",
      "Epoch: 390 | Loss: 1.32864, Acc: 48.27% | Test Loss: 1.34117, Test Acc: 48.67%\n",
      "Epoch: 400 | Loss: 1.32864, Acc: 48.34% | Test Loss: 1.34118, Test Acc: 48.67%\n",
      "Epoch: 410 | Loss: 1.32864, Acc: 48.34% | Test Loss: 1.34120, Test Acc: 48.67%\n",
      "Epoch: 420 | Loss: 1.32864, Acc: 48.34% | Test Loss: 1.34121, Test Acc: 48.67%\n",
      "Epoch: 430 | Loss: 1.32864, Acc: 48.34% | Test Loss: 1.34122, Test Acc: 48.67%\n",
      "Epoch: 440 | Loss: 1.32864, Acc: 48.34% | Test Loss: 1.34123, Test Acc: 48.67%\n",
      "Epoch: 450 | Loss: 1.32864, Acc: 48.34% | Test Loss: 1.34123, Test Acc: 48.67%\n",
      "Epoch: 460 | Loss: 1.32864, Acc: 48.34% | Test Loss: 1.34124, Test Acc: 48.67%\n",
      "Epoch: 470 | Loss: 1.32864, Acc: 48.34% | Test Loss: 1.34124, Test Acc: 48.67%\n",
      "Epoch: 480 | Loss: 1.32864, Acc: 48.34% | Test Loss: 1.34125, Test Acc: 48.67%\n",
      "Epoch: 490 | Loss: 1.32864, Acc: 48.34% | Test Loss: 1.34125, Test Acc: 48.67%\n",
      "Epoch: 500 | Loss: 1.32864, Acc: 48.34% | Test Loss: 1.34125, Test Acc: 48.67%\n",
      "Epoch: 510 | Loss: 1.32864, Acc: 48.34% | Test Loss: 1.34125, Test Acc: 48.67%\n",
      "Epoch: 520 | Loss: 1.32864, Acc: 48.34% | Test Loss: 1.34125, Test Acc: 48.67%\n",
      "Epoch: 530 | Loss: 1.32864, Acc: 48.34% | Test Loss: 1.34125, Test Acc: 48.67%\n",
      "Epoch: 540 | Loss: 1.32864, Acc: 48.34% | Test Loss: 1.34125, Test Acc: 48.67%\n",
      "Epoch: 550 | Loss: 1.32864, Acc: 48.34% | Test Loss: 1.34125, Test Acc: 48.67%\n",
      "Epoch: 560 | Loss: 1.32864, Acc: 48.34% | Test Loss: 1.34125, Test Acc: 48.67%\n",
      "Epoch: 570 | Loss: 1.32864, Acc: 48.34% | Test Loss: 1.34125, Test Acc: 48.67%\n",
      "Epoch: 580 | Loss: 1.32864, Acc: 48.34% | Test Loss: 1.34125, Test Acc: 48.67%\n",
      "Epoch: 590 | Loss: 1.32864, Acc: 48.34% | Test Loss: 1.34125, Test Acc: 48.67%\n",
      "Epoch: 600 | Loss: 1.32864, Acc: 48.34% | Test Loss: 1.34125, Test Acc: 48.67%\n",
      "Epoch: 610 | Loss: 1.32864, Acc: 48.34% | Test Loss: 1.34125, Test Acc: 48.67%\n",
      "Epoch: 620 | Loss: 1.32864, Acc: 48.34% | Test Loss: 1.34125, Test Acc: 48.67%\n",
      "Epoch: 630 | Loss: 1.32864, Acc: 48.34% | Test Loss: 1.34126, Test Acc: 48.67%\n",
      "Epoch: 640 | Loss: 1.32864, Acc: 48.34% | Test Loss: 1.34126, Test Acc: 48.67%\n",
      "Epoch: 650 | Loss: 1.32864, Acc: 48.34% | Test Loss: 1.34126, Test Acc: 48.67%\n",
      "Epoch: 660 | Loss: 1.32864, Acc: 48.34% | Test Loss: 1.34126, Test Acc: 48.67%\n",
      "Epoch: 670 | Loss: 1.32864, Acc: 48.34% | Test Loss: 1.34126, Test Acc: 48.67%\n",
      "Epoch: 680 | Loss: 1.32864, Acc: 48.34% | Test Loss: 1.34126, Test Acc: 48.67%\n",
      "Epoch: 690 | Loss: 1.32864, Acc: 48.34% | Test Loss: 1.34126, Test Acc: 48.67%\n",
      "Epoch: 700 | Loss: 1.32864, Acc: 48.34% | Test Loss: 1.34126, Test Acc: 48.67%\n",
      "Epoch: 710 | Loss: 1.32864, Acc: 48.34% | Test Loss: 1.34126, Test Acc: 48.67%\n",
      "Epoch: 720 | Loss: 1.32864, Acc: 48.34% | Test Loss: 1.34126, Test Acc: 48.67%\n",
      "Epoch: 730 | Loss: 1.32864, Acc: 48.34% | Test Loss: 1.34126, Test Acc: 48.67%\n",
      "Epoch: 740 | Loss: 1.32864, Acc: 48.34% | Test Loss: 1.34126, Test Acc: 48.67%\n",
      "Epoch: 750 | Loss: 1.32864, Acc: 48.34% | Test Loss: 1.34126, Test Acc: 48.67%\n",
      "Epoch: 760 | Loss: 1.32864, Acc: 48.34% | Test Loss: 1.34126, Test Acc: 48.67%\n",
      "Epoch: 770 | Loss: 1.32864, Acc: 48.34% | Test Loss: 1.34126, Test Acc: 48.67%\n",
      "Epoch: 780 | Loss: 1.32864, Acc: 48.34% | Test Loss: 1.34126, Test Acc: 48.67%\n",
      "Epoch: 790 | Loss: 1.32864, Acc: 48.34% | Test Loss: 1.34126, Test Acc: 48.67%\n",
      "Epoch: 800 | Loss: 1.32864, Acc: 48.34% | Test Loss: 1.34126, Test Acc: 48.67%\n",
      "Epoch: 810 | Loss: 1.32864, Acc: 48.34% | Test Loss: 1.34126, Test Acc: 48.67%\n",
      "Epoch: 820 | Loss: 1.32864, Acc: 48.34% | Test Loss: 1.34126, Test Acc: 48.67%\n",
      "Epoch: 830 | Loss: 1.32864, Acc: 48.34% | Test Loss: 1.34126, Test Acc: 48.67%\n",
      "Epoch: 840 | Loss: 1.32864, Acc: 48.34% | Test Loss: 1.34126, Test Acc: 48.67%\n",
      "Epoch: 850 | Loss: 1.32864, Acc: 48.34% | Test Loss: 1.34126, Test Acc: 48.67%\n",
      "Epoch: 860 | Loss: 1.32864, Acc: 48.34% | Test Loss: 1.34126, Test Acc: 48.67%\n",
      "Epoch: 870 | Loss: 1.32864, Acc: 48.34% | Test Loss: 1.34126, Test Acc: 48.67%\n",
      "Epoch: 880 | Loss: 1.32864, Acc: 48.34% | Test Loss: 1.34126, Test Acc: 48.67%\n",
      "Epoch: 890 | Loss: 1.32864, Acc: 48.34% | Test Loss: 1.34126, Test Acc: 48.67%\n",
      "Epoch: 900 | Loss: 1.32864, Acc: 48.34% | Test Loss: 1.34126, Test Acc: 48.67%\n",
      "Epoch: 910 | Loss: 1.32864, Acc: 48.34% | Test Loss: 1.34126, Test Acc: 48.67%\n",
      "Epoch: 920 | Loss: 1.32864, Acc: 48.34% | Test Loss: 1.34126, Test Acc: 48.67%\n",
      "Epoch: 930 | Loss: 1.32864, Acc: 48.34% | Test Loss: 1.34126, Test Acc: 48.67%\n",
      "Epoch: 940 | Loss: 1.32864, Acc: 48.34% | Test Loss: 1.34126, Test Acc: 48.67%\n",
      "Epoch: 950 | Loss: 1.32864, Acc: 48.34% | Test Loss: 1.34126, Test Acc: 48.67%\n",
      "Epoch: 960 | Loss: 1.32864, Acc: 48.34% | Test Loss: 1.34126, Test Acc: 48.67%\n",
      "Epoch: 970 | Loss: 1.32864, Acc: 48.34% | Test Loss: 1.34126, Test Acc: 48.67%\n",
      "Epoch: 980 | Loss: 1.32864, Acc: 48.34% | Test Loss: 1.34126, Test Acc: 48.67%\n",
      "Epoch: 990 | Loss: 1.32864, Acc: 48.34% | Test Loss: 1.34126, Test Acc: 48.67%\n"
     ]
    }
   ],
   "source": [
    "# Fit the model\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Set number of epochs\n",
    "epochs = 1000\n",
    "\n",
    "test_df = pd.read_csv('test_adversary2.dat', sep=',', encoding=\"utf-8\",engine='python')\n",
    "test_df = test_df[['genres', 'rating']]\n",
    "\n",
    "X = torch.tensor(test_df['rating'].values).type(torch.float)\n",
    "Y = torch.tensor(test_df['genres'].astype('category').cat.codes).type(torch.LongTensor)   \n",
    "\n",
    "\n",
    "X_blob_train, X_blob_test, y_blob_train, y_blob_test = train_test_split(X,\n",
    "    Y,\n",
    "    test_size=0.2,\n",
    "    random_state=181\n",
    ")\n",
    "\n",
    "# Put data to target device\n",
    "X_blob_train, y_blob_train = X_blob_train.to(device).view(-1,1), y_blob_train.to(device)\n",
    "X_blob_test, y_blob_test = X_blob_test.to(device).view(-1,1), y_blob_test.to(device)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    ### Training\n",
    "    mlp.train()\n",
    "\n",
    "    # 1. Forward pass\n",
    "    y_logits = mlp(X_blob_train) # model outputs raw logits \n",
    "    y_pred = torch.softmax(y_logits, dim=1).argmax(dim=1) # go from logits -> prediction probabilities -> prediction labels\n",
    "    # print(y_logits)\n",
    "    # 2. Calculate loss and accuracy\n",
    "    loss = loss_fn(y_logits, y_blob_train) \n",
    "    acc = accuracy_fn(y_true=y_blob_train,\n",
    "                      y_pred=y_pred)\n",
    "\n",
    "    # 3. Optimizer zero grad\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # 4. Loss backwards\n",
    "    loss.backward()\n",
    "\n",
    "    # 5. Optimizer step\n",
    "    optimizer.step()\n",
    "\n",
    "    ### Testing\n",
    "    mlp.eval()\n",
    "    with torch.inference_mode():\n",
    "      # 1. Forward pass\n",
    "        test_logits = mlp(X_blob_test)\n",
    "        test_pred = torch.softmax(test_logits, dim=1).argmax(dim=1)\n",
    "      # 2. Calculate test loss and accuracy\n",
    "        test_loss = loss_fn(test_logits, y_blob_test)\n",
    "        test_acc = accuracy_fn(y_true=y_blob_test,\n",
    "                             y_pred=test_pred)\n",
    "\n",
    "    # Print out what's happening\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch: {epoch} | Loss: {loss:.5f}, Acc: {acc:.2f}% | Test Loss: {test_loss:.5f}, Test Acc: {test_acc:.2f}%\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "62627eff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.5287,  0.5252, -0.0628,  ...,  2.0688, -2.2299,  1.3420],\n",
       "        [-0.1782,  0.2102, -0.1189,  ...,  1.2165, -1.4039,  0.8133],\n",
       "        [-0.4711,  0.4735, -0.0720,  ...,  1.9288, -2.0942,  1.2551],\n",
       "        ...,\n",
       "        [-0.4046,  0.4137, -0.0827,  ...,  1.7671, -1.9374,  1.1548],\n",
       "        [-0.5507,  0.5451, -0.0593,  ...,  2.1225, -2.2818,  1.3753],\n",
       "        [-0.6048,  0.5937, -0.0507,  ...,  2.2539, -2.4092,  1.4568]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "d3bbafd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "# Set the hyperparameters for data creation\n",
    "NUM_CLASSES = 4\n",
    "NUM_FEATURES = 2\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# 1. Create multi-class data\n",
    "X_blob, y_blob = make_blobs(n_samples=1000,\n",
    "    n_features=NUM_FEATURES, # X features\n",
    "    centers=NUM_CLASSES, # y labels \n",
    "    cluster_std=1.5, # give the clusters a little shake up (try changing this to 1.0, the default)\n",
    "    random_state=RANDOM_SEED\n",
    ")\n",
    "\n",
    "# 2. Turn data into tensors\n",
    "X_blob = torch.from_numpy(X_blob).type(torch.float)\n",
    "y_blob = torch.from_numpy(y_blob).type(torch.LongTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "3599c501",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1000, 2])"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_blob.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "f6884dc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
       "         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
       "         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
       "         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
       "         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
       "         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
       "         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
       "         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
       "        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
       "        126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139,\n",
       "        140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153,\n",
       "        154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167,\n",
       "        168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
       "        182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195,\n",
       "        196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209,\n",
       "        210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223,\n",
       "        224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237,\n",
       "        238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251,\n",
       "        252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265,\n",
       "        266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279,\n",
       "        280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293,\n",
       "        294, 295, 296, 297, 298, 299, 300])"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.unique(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ce0ac0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
