{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35dba935",
   "metadata": {},
   "source": [
    "Thinking of APR. Are we doing the dual optimization with the classification model + the pertubation added?\n",
    "\n",
    "To do: think of how to split data to train/test/ tune 80/10/10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04a30440",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import relevant library\n",
    "\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import pickle\n",
    "import argparse\n",
    "from collections import deque\n",
    "import time\n",
    "from datetime import timedelta\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torch.utils.data import DataLoader\n",
    "# from torchvision import transforms\n",
    "from torch.utils.data import IterableDataset, DataLoader, get_worker_info\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f262aa84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5a29fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "## time the process\n",
    "def get_time_dif(start_time):\n",
    "    \"\"\"get the running time\"\"\"\n",
    "    end_time = time.time()\n",
    "    time_dif = end_time - start_time\n",
    "    return timedelta(seconds=int(round(time_dif)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85059586",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## set up the u,i,j triplet for BPR framework\n",
    "class GetTriplePair(IterableDataset):\n",
    "    # for ml-1m we load in 3760 item 6040 user and 994169 train pair\n",
    "    def __init__(self, item_size, user_list, pair, shuffle, num_epochs):\n",
    "        self.item_size = item_size\n",
    "        self.user_list = user_list\n",
    "        self.pair = pair\n",
    "        self.shuffle = shuffle\n",
    "        self.num_epochs = num_epochs\n",
    "\n",
    "    def __iter__(self):\n",
    "        self.example_size = self.num_epochs * len(self.pair)\n",
    "        self.example_index_queue = deque([])\n",
    "        self.seed = 0\n",
    "        self.start_list_index = None\n",
    "        self.num_workers = 1\n",
    "        self.index = 0\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        if self.index >= self.example_size:\n",
    "            raise StopIteration\n",
    "        # If `example_index_queue` is used up, replenish this list.\n",
    "        while len(self.example_index_queue) == 0:\n",
    "            index_list = list(range(len(self.pair)))\n",
    "            if self.shuffle:\n",
    "                random.Random(self.seed).shuffle(index_list)\n",
    "                self.seed += 1\n",
    "            if self.start_list_index is not None:\n",
    "                index_list = index_list[self.start_list_index::self.num_workers]\n",
    "\n",
    "                # Calculate next start index\n",
    "                self.start_list_index = (self.start_list_index + (self.num_workers - (len(self.pair) % self.num_workers))) % self.num_workers\n",
    "            self.example_index_queue.extend(index_list)\n",
    "        result = self._example(self.example_index_queue.popleft())\n",
    "        self.index += self.num_workers\n",
    "        return result\n",
    "\n",
    "    def _example(self, idx):\n",
    "        # in a train pair, format = (u,i), j = a random item which does not exist in user u's list of items\n",
    "        u = self.pair[idx][0]\n",
    "        i = self.pair[idx][1]\n",
    "        j = np.random.randint(self.item_size)\n",
    "        while j in self.user_list[u]:\n",
    "            j = np.random.randint(self.item_size)\n",
    "        return u, i, j\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6d922e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## chunk to define matrix factorization part\n",
    "class fair_reprogram(nn.Module):\n",
    "    def __init__(self, user_size, item_size, dim, reg, reg_adv, eps):\n",
    "        super().__init__()\n",
    "        ##init the embedding for U and I\n",
    "        self.W = nn.Parameter(torch.empty(user_size, dim))  # User embedding\n",
    "        self.H = nn.Parameter(torch.empty(item_size, dim))  # Item embedding\n",
    "        nn.init.xavier_normal_(self.W.data)\n",
    "        nn.init.xavier_normal_(self.H.data)\n",
    "        self.reg = reg\n",
    "        self.user_size = user_size\n",
    "        self.item_size = item_size\n",
    "        self.dim = dim\n",
    "        self.reg_adv = reg_adv\n",
    "        self.eps = eps\n",
    "        self.update_u = None\n",
    "        self.update_i = None\n",
    "        self.update_j = None\n",
    "\n",
    "## forward cal, this is the part that\n",
    "    def forward(self, u, i, j, epoch):\n",
    "\n",
    "        ##u,i,j respectively, each is a vector of dim embedding (default = 64)\n",
    "        u = self.W[u, :]\n",
    "        i = self.H[i, :]\n",
    "        j = self.H[j, :]\n",
    "\n",
    "        ## Enables this Tensor to have their grad populated during backward(), convert any non-leaf tensor into a leaf tensor,\n",
    "        ##https://stackoverflow.com/questions/73698041/how-retain-grad-in-pytorch-works-i-found-its-position-changes-the-grad-result\n",
    "        u.retain_grad()\n",
    "        u_clone = u.data.clone()\n",
    "        i.retain_grad()\n",
    "        i_clone = i.data.clone()\n",
    "        j.retain_grad()\n",
    "        j_clone = j.data.clone()\n",
    "\n",
    "        ## mf, dot product of user with pos/neg item\n",
    "        x_ui = torch.mul(u, i).sum(dim=1)\n",
    "        x_uj = torch.mul(u, j).sum(dim=1)\n",
    "\n",
    "\n",
    "        #similar to clip value, find diff between ui and uj\n",
    "        x_uij =torch.clamp(x_ui - x_uj,min=-80.0,max=1e8)\n",
    "        #logsigmoid this is equivalent to equation 1 in the paper (classic loss of bpr)\n",
    "        log_prob = F.logsigmoid(x_uij).sum()\n",
    "        # regularization = lambda * l2 norm of u, i, j\n",
    "        regularization = self.reg * (u.norm(dim=1).pow(2).sum() + i.norm(dim=1).pow(2).sum() + j.norm(dim=1).pow(2).sum())\n",
    "\n",
    "        ## original bpr loss,\n",
    "        loss = -log_prob + regularization\n",
    "\n",
    "        return loss\n",
    "        # add adv training after a certain number of epochs, here is the part which we add hypernet module\n",
    "        if epoch not in range(args.epochs, args.adv_epoch + args.epochs):\n",
    "            \"\"\"Normal training\"\"\"\n",
    "            loss.backward()\n",
    "            return loss\n",
    "\n",
    "        else:\n",
    "            \"\"\"Adversarial training:\n",
    "                    1.Backward to get grads\n",
    "                    2.Construct adversarial perturbation\n",
    "                    3.Add adversarial perturbation to embeddings\n",
    "                    4.Calculate APR loss\n",
    "            \"\"\"\n",
    "            # Backward to get grads\n",
    "            # this would be the part we change in defining delta, delta = HPN (phi)\n",
    "\n",
    "            # should we calculate based on gradient of the adv_loss instead of the loss function?, originally, computed based on loss function\n",
    "            loss.backward(retain_graph=True) ## need to retain graph here so as to we can backprop the adv_loss\n",
    "            ##recheck this\n",
    "            grad_u = u.grad\n",
    "            grad_i = i.grad\n",
    "            grad_j = j.grad\n",
    "\n",
    "            # Construct adversarial perturbation based on gradient of loss function, and normalize it with epsilon * norm\n",
    "            if grad_u is not None:\n",
    "                delta_u = nn.functional.normalize(grad_u, p=2, dim=1, eps=self.eps)\n",
    "            else:\n",
    "                delta_u = torch.rand(u.size())\n",
    "            if grad_i is not None:\n",
    "                delta_i = nn.functional.normalize(grad_i, p=2, dim=1, eps=self.eps)\n",
    "            else:\n",
    "                delta_i = torch.rand(i.size())\n",
    "            if grad_j is not None:\n",
    "                delta_j = nn.functional.normalize(grad_j, p=2, dim=1, eps=self.eps)\n",
    "            else:\n",
    "                delta_j = torch.rand(j.size())\n",
    "\n",
    "            # Add adversarial perturbation to embeddings, now we have q+delta, p+delta\n",
    "            x_ui_adv = torch.mul(u + delta_u, i + delta_i).sum(dim=1)\n",
    "            x_uj_adv = torch.mul(u + delta_u, j + delta_j).sum(dim=1)\n",
    "\n",
    "            # find difference between pos and neg item, then clip value\n",
    "            x_uij_adv = torch.clamp(x_ui_adv - x_uj_adv,min=-80.0,max=1e8)\n",
    "\n",
    "            # Calculate APR loss with logsigmoid\n",
    "            log_prob = F.logsigmoid(x_uij_adv).sum()\n",
    "            adv_loss = self.reg_adv *(-log_prob) + loss # this is adversarial loss (equation 4 in paper)\n",
    "            adv_loss.backward()\n",
    "\n",
    "            return adv_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4961396",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fairness_reprogramming(self, u, i, j):\n",
    "    \"\"\"Reprogramming phase:\n",
    "        1.Freeze the user and item embedding -- done by saving checkpoint\n",
    "        2.Calculate the perturbation to achieve fairness objective\n",
    "        3.Add perturbation to the alr frozen embedding\n",
    "        4.Calculate the overall loss function after update\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize a fix random perturbation\n",
    "    perturbation = torch.rand(1)\n",
    "    \n",
    "    #load user and item embedding, which has been trained in BPR\n",
    "    u = list(model1.items())[0][1]\n",
    "    i = list(model1.items())[1][1][i]\n",
    "    j = list(model1.items())[1][1][j]\n",
    "        \n",
    "    # Add adversarial perturbation to embeddings, now we have q+delta, p+delta\n",
    "    x_ui_adv = torch.mul(u , i + perturbation).sum(dim=1)\n",
    "    x_uj_adv = torch.mul(u , j + perturbation).sum(dim=1)\n",
    "\n",
    "    # find difference between pos and neg item, then clip value\n",
    "    x_uij_adv = torch.clamp(x_ui_adv - x_uj_adv,min=-80.0,max=1e8)\n",
    "\n",
    "    # Calculate loss with perturbed embedding with logsigmoid\n",
    "    log_prob = F.logsigmoid(x_uij_adv).sum()\n",
    "            \n",
    "    #set up an adversary to identify group of items\n",
    "    adversary_rs        \n",
    "            \n",
    "    # modify the adversarial loss here\n",
    "    adv_loss = self.reg_adv *(-log_prob) + loss # this is adversarial loss (equation 4 in paper)\n",
    "    adv_loss.backward()\n",
    "\n",
    "    return adv_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "3a3b93ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6040, 64])"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load the results of BPR\n",
    "model1 = (torch.load('models/01_pytorch_workflow_model_1.pth'))\n",
    "list(model1.items())[0][1].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "87f05379",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 9.3768e-04, -4.2642e-02,  2.0659e-02,  ..., -2.2226e-02,\n",
       "          3.1339e-02, -3.3840e-02],\n",
       "        [ 1.5390e-04, -3.8164e-03,  1.1938e-02,  ..., -1.9504e-03,\n",
       "         -5.5167e-03,  2.5199e-02],\n",
       "        [-1.7836e-03, -3.0050e-02, -3.0792e-02,  ...,  1.1926e-02,\n",
       "         -1.7736e-02,  6.9641e-03],\n",
       "        ...,\n",
       "        [-3.4588e-02, -1.5128e-03, -5.1930e-02,  ...,  3.3526e-02,\n",
       "         -3.9308e-03,  1.3962e-02],\n",
       "        [ 1.6954e-05, -1.2272e-02,  3.1606e-02,  ..., -2.2976e-02,\n",
       "         -1.5850e-03, -2.3360e-02],\n",
       "        [ 1.0627e-02, -2.0485e-02, -3.4491e-02,  ..., -1.7362e-02,\n",
       "          3.7945e-03,  7.2581e-03]])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model1.items())[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "f51d8700",
   "metadata": {},
   "outputs": [],
   "source": [
    "#goal = input y and yhat and return the group of the item?\n",
    "\n",
    "# Build model\n",
    "class adversary_rs(nn.Module):\n",
    "    def __init__(self, input_features, output_features, hidden_units=8):\n",
    "        \"\"\"Initializes all required hyperparameters for a multi-class classification model.\n",
    "\n",
    "        Args:\n",
    "            input_features (int): Number of input features to the model.\n",
    "            out_features (int): Number of output features of the model\n",
    "              (how many classes there are).\n",
    "            hidden_units (int): Number of hidden units between layers, default 8.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.linear_layer_stack = nn.Sequential(\n",
    "            nn.Linear(in_features=input_features, out_features=hidden_units),\n",
    "            # nn.ReLU(), # <- does our dataset require non-linear layers? (try uncommenting and see if the results change)\n",
    "            nn.Linear(in_features=hidden_units, out_features=hidden_units),\n",
    "            # nn.ReLU(), # <- does our dataset require non-linear layers? (try uncommenting and see if the results change)\n",
    "            nn.Linear(in_features=hidden_units, out_features=output_features), # how many classes are there?\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.linear_layer_stack(x)\n",
    "\n",
    "# Create an instance of BlobModel and send it to the target device\n",
    "mlp = adversary_rs(input_features=1, \n",
    "                    output_features=6, \n",
    "                    hidden_units=500).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "e0990b22",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Define the loss function and optimizer\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(mlp.parameters(), lr=1e-4)\n",
    "  # Prepare CIFAR-10 dataset\n",
    "trainloader = torch.utils.data.DataLoader(X, batch_size=512, shuffle=True, num_workers=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "97f3aca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate accuracy (a classification metric)\n",
    "def accuracy_fn(y_true, y_pred):\n",
    "    correct = torch.eq(y_true, y_pred).sum().item() # torch.eq() calculates where two tensors are equal\n",
    "    acc = (correct / len(y_pred)) * 100 \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "864d60f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Loss: 1.73429, Acc: 40.30% | Test Loss: 1.47605, Test Acc: 44.84%\n",
      "Epoch: 10 | Loss: 1.42034, Acc: 44.80% | Test Loss: 1.40173, Test Acc: 44.84%\n",
      "Epoch: 20 | Loss: 1.39443, Acc: 45.17% | Test Loss: 1.38122, Test Acc: 45.13%\n",
      "Epoch: 30 | Loss: 1.37248, Acc: 45.83% | Test Loss: 1.35702, Test Acc: 46.02%\n",
      "Epoch: 40 | Loss: 1.35933, Acc: 47.01% | Test Loss: 1.34779, Test Acc: 46.02%\n",
      "Epoch: 50 | Loss: 1.35134, Acc: 48.19% | Test Loss: 1.34180, Test Acc: 48.67%\n",
      "Epoch: 60 | Loss: 1.34569, Acc: 47.82% | Test Loss: 1.33921, Test Acc: 48.38%\n",
      "Epoch: 70 | Loss: 1.34156, Acc: 47.97% | Test Loss: 1.33752, Test Acc: 48.97%\n",
      "Epoch: 80 | Loss: 1.33857, Acc: 48.12% | Test Loss: 1.33625, Test Acc: 48.67%\n",
      "Epoch: 90 | Loss: 1.33634, Acc: 48.12% | Test Loss: 1.33610, Test Acc: 48.67%\n",
      "Epoch: 100 | Loss: 1.33465, Acc: 48.12% | Test Loss: 1.33587, Test Acc: 48.67%\n",
      "Epoch: 110 | Loss: 1.33335, Acc: 48.12% | Test Loss: 1.33594, Test Acc: 48.67%\n",
      "Epoch: 120 | Loss: 1.33233, Acc: 48.12% | Test Loss: 1.33612, Test Acc: 48.67%\n",
      "Epoch: 130 | Loss: 1.33153, Acc: 48.19% | Test Loss: 1.33632, Test Acc: 48.97%\n",
      "Epoch: 140 | Loss: 1.33090, Acc: 48.12% | Test Loss: 1.33662, Test Acc: 48.97%\n",
      "Epoch: 150 | Loss: 1.33040, Acc: 48.04% | Test Loss: 1.33695, Test Acc: 48.97%\n",
      "Epoch: 160 | Loss: 1.33000, Acc: 48.04% | Test Loss: 1.33730, Test Acc: 48.97%\n",
      "Epoch: 170 | Loss: 1.32969, Acc: 48.12% | Test Loss: 1.33765, Test Acc: 48.67%\n",
      "Epoch: 180 | Loss: 1.32944, Acc: 48.19% | Test Loss: 1.33800, Test Acc: 48.67%\n",
      "Epoch: 190 | Loss: 1.32925, Acc: 48.19% | Test Loss: 1.33834, Test Acc: 48.67%\n",
      "Epoch: 200 | Loss: 1.32910, Acc: 48.19% | Test Loss: 1.33866, Test Acc: 48.67%\n",
      "Epoch: 210 | Loss: 1.32898, Acc: 48.19% | Test Loss: 1.33897, Test Acc: 48.67%\n",
      "Epoch: 220 | Loss: 1.32889, Acc: 48.19% | Test Loss: 1.33925, Test Acc: 48.67%\n",
      "Epoch: 230 | Loss: 1.32883, Acc: 48.19% | Test Loss: 1.33950, Test Acc: 48.67%\n",
      "Epoch: 240 | Loss: 1.32878, Acc: 48.19% | Test Loss: 1.33974, Test Acc: 48.67%\n",
      "Epoch: 250 | Loss: 1.32874, Acc: 48.19% | Test Loss: 1.33995, Test Acc: 48.67%\n",
      "Epoch: 260 | Loss: 1.32871, Acc: 48.19% | Test Loss: 1.34014, Test Acc: 48.67%\n",
      "Epoch: 270 | Loss: 1.32869, Acc: 48.19% | Test Loss: 1.34031, Test Acc: 48.67%\n",
      "Epoch: 280 | Loss: 1.32868, Acc: 48.19% | Test Loss: 1.34045, Test Acc: 48.67%\n",
      "Epoch: 290 | Loss: 1.32867, Acc: 48.19% | Test Loss: 1.34058, Test Acc: 48.67%\n",
      "Epoch: 300 | Loss: 1.32866, Acc: 48.19% | Test Loss: 1.34069, Test Acc: 48.67%\n",
      "Epoch: 310 | Loss: 1.32865, Acc: 48.19% | Test Loss: 1.34079, Test Acc: 48.67%\n",
      "Epoch: 320 | Loss: 1.32865, Acc: 48.19% | Test Loss: 1.34087, Test Acc: 48.67%\n",
      "Epoch: 330 | Loss: 1.32865, Acc: 48.19% | Test Loss: 1.34094, Test Acc: 48.67%\n",
      "Epoch: 340 | Loss: 1.32865, Acc: 48.19% | Test Loss: 1.34099, Test Acc: 48.67%\n",
      "Epoch: 350 | Loss: 1.32864, Acc: 48.19% | Test Loss: 1.34104, Test Acc: 48.67%\n",
      "Epoch: 360 | Loss: 1.32864, Acc: 48.27% | Test Loss: 1.34108, Test Acc: 48.67%\n",
      "Epoch: 370 | Loss: 1.32864, Acc: 48.27% | Test Loss: 1.34112, Test Acc: 48.67%\n",
      "Epoch: 380 | Loss: 1.32864, Acc: 48.27% | Test Loss: 1.34114, Test Acc: 48.67%\n",
      "Epoch: 390 | Loss: 1.32864, Acc: 48.27% | Test Loss: 1.34117, Test Acc: 48.67%\n",
      "Epoch: 400 | Loss: 1.32864, Acc: 48.34% | Test Loss: 1.34118, Test Acc: 48.67%\n",
      "Epoch: 410 | Loss: 1.32864, Acc: 48.34% | Test Loss: 1.34120, Test Acc: 48.67%\n",
      "Epoch: 420 | Loss: 1.32864, Acc: 48.34% | Test Loss: 1.34121, Test Acc: 48.67%\n",
      "Epoch: 430 | Loss: 1.32864, Acc: 48.34% | Test Loss: 1.34122, Test Acc: 48.67%\n",
      "Epoch: 440 | Loss: 1.32864, Acc: 48.34% | Test Loss: 1.34123, Test Acc: 48.67%\n",
      "Epoch: 450 | Loss: 1.32864, Acc: 48.34% | Test Loss: 1.34123, Test Acc: 48.67%\n",
      "Epoch: 460 | Loss: 1.32864, Acc: 48.34% | Test Loss: 1.34124, Test Acc: 48.67%\n",
      "Epoch: 470 | Loss: 1.32864, Acc: 48.34% | Test Loss: 1.34124, Test Acc: 48.67%\n",
      "Epoch: 480 | Loss: 1.32864, Acc: 48.34% | Test Loss: 1.34125, Test Acc: 48.67%\n",
      "Epoch: 490 | Loss: 1.32864, Acc: 48.34% | Test Loss: 1.34125, Test Acc: 48.67%\n",
      "Epoch: 500 | Loss: 1.32864, Acc: 48.34% | Test Loss: 1.34125, Test Acc: 48.67%\n",
      "Epoch: 510 | Loss: 1.32864, Acc: 48.34% | Test Loss: 1.34125, Test Acc: 48.67%\n",
      "Epoch: 520 | Loss: 1.32864, Acc: 48.34% | Test Loss: 1.34125, Test Acc: 48.67%\n",
      "Epoch: 530 | Loss: 1.32864, Acc: 48.34% | Test Loss: 1.34125, Test Acc: 48.67%\n",
      "Epoch: 540 | Loss: 1.32864, Acc: 48.34% | Test Loss: 1.34125, Test Acc: 48.67%\n",
      "Epoch: 550 | Loss: 1.32864, Acc: 48.34% | Test Loss: 1.34125, Test Acc: 48.67%\n",
      "Epoch: 560 | Loss: 1.32864, Acc: 48.34% | Test Loss: 1.34125, Test Acc: 48.67%\n",
      "Epoch: 570 | Loss: 1.32864, Acc: 48.34% | Test Loss: 1.34125, Test Acc: 48.67%\n",
      "Epoch: 580 | Loss: 1.32864, Acc: 48.34% | Test Loss: 1.34125, Test Acc: 48.67%\n",
      "Epoch: 590 | Loss: 1.32864, Acc: 48.34% | Test Loss: 1.34125, Test Acc: 48.67%\n",
      "Epoch: 600 | Loss: 1.32864, Acc: 48.34% | Test Loss: 1.34125, Test Acc: 48.67%\n",
      "Epoch: 610 | Loss: 1.32864, Acc: 48.34% | Test Loss: 1.34125, Test Acc: 48.67%\n",
      "Epoch: 620 | Loss: 1.32864, Acc: 48.34% | Test Loss: 1.34125, Test Acc: 48.67%\n",
      "Epoch: 630 | Loss: 1.32864, Acc: 48.34% | Test Loss: 1.34126, Test Acc: 48.67%\n",
      "Epoch: 640 | Loss: 1.32864, Acc: 48.34% | Test Loss: 1.34126, Test Acc: 48.67%\n",
      "Epoch: 650 | Loss: 1.32864, Acc: 48.34% | Test Loss: 1.34126, Test Acc: 48.67%\n",
      "Epoch: 660 | Loss: 1.32864, Acc: 48.34% | Test Loss: 1.34126, Test Acc: 48.67%\n",
      "Epoch: 670 | Loss: 1.32864, Acc: 48.34% | Test Loss: 1.34126, Test Acc: 48.67%\n",
      "Epoch: 680 | Loss: 1.32864, Acc: 48.34% | Test Loss: 1.34126, Test Acc: 48.67%\n",
      "Epoch: 690 | Loss: 1.32864, Acc: 48.34% | Test Loss: 1.34126, Test Acc: 48.67%\n",
      "Epoch: 700 | Loss: 1.32864, Acc: 48.34% | Test Loss: 1.34126, Test Acc: 48.67%\n",
      "Epoch: 710 | Loss: 1.32864, Acc: 48.34% | Test Loss: 1.34126, Test Acc: 48.67%\n",
      "Epoch: 720 | Loss: 1.32864, Acc: 48.34% | Test Loss: 1.34126, Test Acc: 48.67%\n",
      "Epoch: 730 | Loss: 1.32864, Acc: 48.34% | Test Loss: 1.34126, Test Acc: 48.67%\n",
      "Epoch: 740 | Loss: 1.32864, Acc: 48.34% | Test Loss: 1.34126, Test Acc: 48.67%\n",
      "Epoch: 750 | Loss: 1.32864, Acc: 48.34% | Test Loss: 1.34126, Test Acc: 48.67%\n",
      "Epoch: 760 | Loss: 1.32864, Acc: 48.34% | Test Loss: 1.34126, Test Acc: 48.67%\n",
      "Epoch: 770 | Loss: 1.32864, Acc: 48.34% | Test Loss: 1.34126, Test Acc: 48.67%\n",
      "Epoch: 780 | Loss: 1.32864, Acc: 48.34% | Test Loss: 1.34126, Test Acc: 48.67%\n",
      "Epoch: 790 | Loss: 1.32864, Acc: 48.34% | Test Loss: 1.34126, Test Acc: 48.67%\n",
      "Epoch: 800 | Loss: 1.32864, Acc: 48.34% | Test Loss: 1.34126, Test Acc: 48.67%\n",
      "Epoch: 810 | Loss: 1.32864, Acc: 48.34% | Test Loss: 1.34126, Test Acc: 48.67%\n",
      "Epoch: 820 | Loss: 1.32864, Acc: 48.34% | Test Loss: 1.34126, Test Acc: 48.67%\n",
      "Epoch: 830 | Loss: 1.32864, Acc: 48.34% | Test Loss: 1.34126, Test Acc: 48.67%\n",
      "Epoch: 840 | Loss: 1.32864, Acc: 48.34% | Test Loss: 1.34126, Test Acc: 48.67%\n",
      "Epoch: 850 | Loss: 1.32864, Acc: 48.34% | Test Loss: 1.34126, Test Acc: 48.67%\n",
      "Epoch: 860 | Loss: 1.32864, Acc: 48.34% | Test Loss: 1.34126, Test Acc: 48.67%\n",
      "Epoch: 870 | Loss: 1.32864, Acc: 48.34% | Test Loss: 1.34126, Test Acc: 48.67%\n",
      "Epoch: 880 | Loss: 1.32864, Acc: 48.34% | Test Loss: 1.34126, Test Acc: 48.67%\n",
      "Epoch: 890 | Loss: 1.32864, Acc: 48.34% | Test Loss: 1.34126, Test Acc: 48.67%\n",
      "Epoch: 900 | Loss: 1.32864, Acc: 48.34% | Test Loss: 1.34126, Test Acc: 48.67%\n",
      "Epoch: 910 | Loss: 1.32864, Acc: 48.34% | Test Loss: 1.34126, Test Acc: 48.67%\n",
      "Epoch: 920 | Loss: 1.32864, Acc: 48.34% | Test Loss: 1.34126, Test Acc: 48.67%\n",
      "Epoch: 930 | Loss: 1.32864, Acc: 48.34% | Test Loss: 1.34126, Test Acc: 48.67%\n",
      "Epoch: 940 | Loss: 1.32864, Acc: 48.34% | Test Loss: 1.34126, Test Acc: 48.67%\n",
      "Epoch: 950 | Loss: 1.32864, Acc: 48.34% | Test Loss: 1.34126, Test Acc: 48.67%\n",
      "Epoch: 960 | Loss: 1.32864, Acc: 48.34% | Test Loss: 1.34126, Test Acc: 48.67%\n",
      "Epoch: 970 | Loss: 1.32864, Acc: 48.34% | Test Loss: 1.34126, Test Acc: 48.67%\n",
      "Epoch: 980 | Loss: 1.32864, Acc: 48.34% | Test Loss: 1.34126, Test Acc: 48.67%\n",
      "Epoch: 990 | Loss: 1.32864, Acc: 48.34% | Test Loss: 1.34126, Test Acc: 48.67%\n"
     ]
    }
   ],
   "source": [
    "# Fit the model\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Set number of epochs\n",
    "epochs = 1000\n",
    "\n",
    "test_df = pd.read_csv('test_adversary2.dat', sep=',', encoding=\"utf-8\",engine='python')\n",
    "test_df = test_df[['genres', 'rating']]\n",
    "\n",
    "X = torch.tensor(test_df['rating'].values).type(torch.float)\n",
    "Y = torch.tensor(test_df['genres'].astype('category').cat.codes).type(torch.LongTensor)   \n",
    "\n",
    "\n",
    "X_blob_train, X_blob_test, y_blob_train, y_blob_test = train_test_split(X,\n",
    "    Y,\n",
    "    test_size=0.2,\n",
    "    random_state=181\n",
    ")\n",
    "\n",
    "# Put data to target device\n",
    "X_blob_train, y_blob_train = X_blob_train.to(device).view(-1,1), y_blob_train.to(device)\n",
    "X_blob_test, y_blob_test = X_blob_test.to(device).view(-1,1), y_blob_test.to(device)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    ### Training\n",
    "    mlp.train()\n",
    "\n",
    "    # 1. Forward pass\n",
    "    y_logits = mlp(X_blob_train) # model outputs raw logits \n",
    "    y_pred = torch.softmax(y_logits, dim=1).argmax(dim=1) # go from logits -> prediction probabilities -> prediction labels\n",
    "    # print(y_logits)\n",
    "    # 2. Calculate loss and accuracy\n",
    "    loss = loss_fn(y_logits, y_blob_train) \n",
    "    acc = accuracy_fn(y_true=y_blob_train,\n",
    "                      y_pred=y_pred)\n",
    "\n",
    "    # 3. Optimizer zero grad\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # 4. Loss backwards\n",
    "    loss.backward()\n",
    "\n",
    "    # 5. Optimizer step\n",
    "    optimizer.step()\n",
    "\n",
    "    ### Testing\n",
    "    mlp.eval()\n",
    "    with torch.inference_mode():\n",
    "      # 1. Forward pass\n",
    "        test_logits = mlp(X_blob_test)\n",
    "        test_pred = torch.softmax(test_logits, dim=1).argmax(dim=1)\n",
    "      # 2. Calculate test loss and accuracy\n",
    "        test_loss = loss_fn(test_logits, y_blob_test)\n",
    "        test_acc = accuracy_fn(y_true=y_blob_test,\n",
    "                             y_pred=test_pred)\n",
    "\n",
    "    # Print out what's happening\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch: {epoch} | Loss: {loss:.5f}, Acc: {acc:.2f}% | Test Loss: {test_loss:.5f}, Test Acc: {test_acc:.2f}%\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "ac77aa2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear_layer_stack.0.weight torch.Size([500, 1])\n",
      "linear_layer_stack.0.bias torch.Size([500])\n",
      "linear_layer_stack.1.weight torch.Size([500, 500])\n",
      "linear_layer_stack.1.bias torch.Size([500])\n",
      "linear_layer_stack.2.weight torch.Size([6, 500])\n",
      "linear_layer_stack.2.bias torch.Size([6])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for name, param in mlp.state_dict().items():\n",
    "    print(name, param.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d6f57c8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of       Unnamed: 0  movie_id    genres    rating\n",
       "0              4         5    Comedy  3.006757\n",
       "1              8         9    Action  2.656863\n",
       "2             13        14     Drama  3.542484\n",
       "3             17        18  Thriller  3.337580\n",
       "4             18        19    Comedy  2.480720\n",
       "...          ...       ...       ...       ...\n",
       "1689        3700      3947  Thriller  3.472727\n",
       "1690        3701      3948    Comedy  3.635731\n",
       "1691        3702      3949     Drama  4.115132\n",
       "1692        3703      3950     Drama  3.666667\n",
       "1693        3704      3951     Drama  3.900000\n",
       "\n",
       "[1694 rows x 4 columns]>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.read_csv('test_adversary2.dat', sep=',', encoding=\"utf-8\",engine='python')\n",
    "test_df.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "8a63a116",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1000, 2])"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_blob.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "dbbb0b2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
       "         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
       "         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
       "         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
       "         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
       "         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
       "         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
       "         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
       "        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
       "        126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139,\n",
       "        140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153,\n",
       "        154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167,\n",
       "        168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
       "        182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195,\n",
       "        196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209,\n",
       "        210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223,\n",
       "        224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237,\n",
       "        238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251,\n",
       "        252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265,\n",
       "        266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279,\n",
       "        280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293,\n",
       "        294, 295, 296, 297, 298, 299, 300])"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.unique(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e6a683d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dee3bf7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
