{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35dba935",
   "metadata": {},
   "source": [
    "Thinking of APR. Are we doing the dual optimization with the classification model + the pertubation added?\n",
    "\n",
    "1. Run BPR and retrieve the model parameters + predict with parameters\n",
    "2. Create input output for adversary and train + save adversary\n",
    "3. tune data with the discussed methodology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "04a30440",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import relevant library\n",
    "\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import pickle\n",
    "import argparse\n",
    "from collections import deque\n",
    "import time\n",
    "import utility\n",
    "from datetime import timedelta\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "# from torchvision.datasets import CIFAR10\n",
    "from torch.utils.data import DataLoader\n",
    "# from torchvision import transforms\n",
    "from torch.utils.data import IterableDataset, DataLoader, get_worker_info\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f262aa84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d897bb8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## time the process\n",
    "def get_time_dif(start_time):\n",
    "    \"\"\"get the running time\"\"\"\n",
    "    end_time = time.time()\n",
    "    time_dif = end_time - start_time\n",
    "    return timedelta(seconds=int(round(time_dif)))\n",
    "\n",
    "\n",
    "## set up the u,i,j triplet for BPR framework\n",
    "class GetTriplePair(IterableDataset):\n",
    "    # for ml-1m we load in 3760 item 6040 user and 994169 train pair\n",
    "    def __init__(self, item_size, user_list, pair, shuffle, num_epochs):\n",
    "        self.item_size = item_size\n",
    "        self.user_list = user_list\n",
    "        self.pair = pair\n",
    "        self.shuffle = shuffle\n",
    "        self.num_epochs = num_epochs\n",
    "\n",
    "    def __iter__(self):\n",
    "        self.example_size = self.num_epochs * len(self.pair)\n",
    "        self.example_index_queue = deque([])\n",
    "        self.seed = 0\n",
    "        self.start_list_index = None\n",
    "        self.num_workers = 1\n",
    "        self.index = 0\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        if self.index >= self.example_size:\n",
    "            raise StopIteration\n",
    "        # If `example_index_queue` is used up, replenish this list.\n",
    "        while len(self.example_index_queue) == 0:\n",
    "            index_list = list(range(len(self.pair)))\n",
    "            if self.shuffle:\n",
    "                random.Random(self.seed).shuffle(index_list)\n",
    "                self.seed += 1\n",
    "            if self.start_list_index is not None:\n",
    "                index_list = index_list[self.start_list_index::self.num_workers]\n",
    "\n",
    "                # Calculate next start index\n",
    "                self.start_list_index = (self.start_list_index + (self.num_workers - (len(self.pair) % self.num_workers))) % self.num_workers\n",
    "            self.example_index_queue.extend(index_list)\n",
    "        result = self._example(self.example_index_queue.popleft())\n",
    "        self.index += self.num_workers\n",
    "        return result\n",
    "\n",
    "    def _example(self, idx):\n",
    "        # in a train pair, format = (u,i), j = a random item which does not exist in user u's list of items\n",
    "        u = self.pair[idx][0]\n",
    "        i = self.pair[idx][1]\n",
    "        j = np.random.randint(self.item_size)\n",
    "        while j in self.user_list[u]:\n",
    "            j = np.random.randint(self.item_size)\n",
    "        return u, i, j\n",
    "\n",
    "## chunk to define matrix factorization part\n",
    "class MF(nn.Module):\n",
    "    def __init__(self, user_size, item_size, dim, reg, reg_adv, eps):\n",
    "        super().__init__()\n",
    "        ##init the embedding for U and I\n",
    "        self.W = nn.Parameter(torch.empty(user_size, dim))  # User embedding\n",
    "        self.H = nn.Parameter(torch.empty(item_size, dim))  # Item embedding\n",
    "        nn.init.xavier_normal_(self.W.data)\n",
    "        nn.init.xavier_normal_(self.H.data)\n",
    "        self.reg = reg\n",
    "        self.user_size = user_size\n",
    "        self.item_size = item_size\n",
    "        self.dim = dim\n",
    "        self.reg_adv = reg_adv\n",
    "        self.eps = eps\n",
    "        self.update_u = None\n",
    "        self.update_i = None\n",
    "        self.update_j = None\n",
    "\n",
    "## forward cal\n",
    "    def forward(self, u, i, j, epoch):\n",
    "        \"\"\"Return loss value.\n",
    "\n",
    "        Args:\n",
    "            u(torch.LongTensor): tensor stored user indexes. [batch_size,]\n",
    "            i(torch.LongTensor): tensor stored item indexes which is prefered by user. [batch_size,]\n",
    "            j(torch.LongTensor): tensor stored item indexes which is not prefered by user. [batch_size,]\n",
    "            epoch\n",
    "\n",
    "        Returns:\n",
    "            torch.FloatTensor\n",
    "        \"\"\"\n",
    "        ##u,i,j respectively, each is a vector of dim embedding (default = 64)\n",
    "        u = self.W[u, :]\n",
    "        i = self.H[i, :]\n",
    "        j = self.H[j, :]\n",
    "\n",
    "        ## Enables this Tensor to have their grad populated during backward(), convert any non-leaf tensor into a leaf tensor,\n",
    "        ##https://stackoverflow.com/questions/73698041/how-retain-grad-in-pytorch-works-i-found-its-position-changes-the-grad-result\n",
    "        u.retain_grad()\n",
    "        u_clone = u.data.clone()\n",
    "        i.retain_grad()\n",
    "        i_clone = i.data.clone()\n",
    "        j.retain_grad()\n",
    "        j_clone = j.data.clone()\n",
    "\n",
    "        ## mf, dot product of user with pos/neg item\n",
    "        x_ui = torch.mul(u, i).sum(dim=1)\n",
    "        x_uj = torch.mul(u, j).sum(dim=1)\n",
    "\n",
    "\n",
    "        #similar to clip value, find diff between ui and uj\n",
    "        x_uij =torch.clamp(x_ui - x_uj,min=-80.0,max=1e8)\n",
    "        #logsigmoid this is equivalent to equation 1 in the paper (classic loss of bpr)\n",
    "        log_prob = F.logsigmoid(x_uij).sum()\n",
    "        # regularization = lambda * l2 norm of u, i, j\n",
    "        regularization = self.reg * (u.norm(dim=1).pow(2).sum() + i.norm(dim=1).pow(2).sum() + j.norm(dim=1).pow(2).sum())\n",
    "\n",
    "        ## original bpr loss,\n",
    "        loss = -log_prob + regularization\n",
    "\n",
    "        loss.backward()\n",
    "        return loss\n",
    "        # add adv training after a certain number of epochs, here is the part which we add hypernet module\n",
    "        if epoch not in range(args.epochs, args.adv_epoch + args.epochs):\n",
    "            \"\"\"Normal training\"\"\"\n",
    "            loss.backward()\n",
    "            return loss\n",
    "\n",
    "        else:\n",
    "            \"\"\"Adversarial training:\n",
    "                    1.Backward to get grads\n",
    "                    2.Construct adversarial perturbation\n",
    "                    3.Add adversarial perturbation to embeddings\n",
    "                    4.Calculate APR loss\n",
    "            \"\"\"\n",
    "            # Backward to get grads\n",
    "            # this would be the part we change in defining delta, delta = HPN (phi)\n",
    "\n",
    "            # should we calculate based on gradient of the adv_loss instead of the loss function?, originally, computed based on loss function\n",
    "            loss.backward(retain_graph=True) ## need to retain graph here so as to we can backprop the adv_loss\n",
    "            ##recheck this\n",
    "            grad_u = u.grad\n",
    "            grad_i = i.grad\n",
    "            grad_j = j.grad\n",
    "\n",
    "            # Construct adversarial perturbation based on gradient of loss function, and normalize it with epsilon * norm\n",
    "            if grad_u is not None:\n",
    "                delta_u = nn.functional.normalize(grad_u, p=2, dim=1, eps=self.eps)\n",
    "            else:\n",
    "                delta_u = torch.rand(u.size())\n",
    "            if grad_i is not None:\n",
    "                delta_i = nn.functional.normalize(grad_i, p=2, dim=1, eps=self.eps)\n",
    "            else:\n",
    "                delta_i = torch.rand(i.size())\n",
    "            if grad_j is not None:\n",
    "                delta_j = nn.functional.normalize(grad_j, p=2, dim=1, eps=self.eps)\n",
    "            else:\n",
    "                delta_j = torch.rand(j.size())\n",
    "\n",
    "            # Add adversarial perturbation to embeddings, now we have q+delta, p+delta\n",
    "            x_ui_adv = torch.mul(u + delta_u, i + delta_i).sum(dim=1)\n",
    "            x_uj_adv = torch.mul(u + delta_u, j + delta_j).sum(dim=1)\n",
    "\n",
    "            # find difference between pos and neg item, then clip value\n",
    "            x_uij_adv = torch.clamp(x_ui_adv - x_uj_adv,min=-80.0,max=1e8)\n",
    "\n",
    "            # Calculate APR loss with logsigmoid\n",
    "            log_prob = F.logsigmoid(x_uij_adv).sum()\n",
    "            adv_loss = self.reg_adv *(-log_prob) + loss # this is adversarial loss (equation 4 in paper)\n",
    "            adv_loss.backward()\n",
    "\n",
    "            return adv_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "c1a455a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_k(user_emb, item_emb, train_user_list, test_user_list, klist, batch=512):\n",
    "    \"\"\"Compute HR and NDCG at k.\n",
    "\n",
    "    Args:\n",
    "        user_emb (torch.Tensor): embedding for user [user_num, dim]\n",
    "        item_emb (torch.Tensor): embedding for item [item_num, dim]\n",
    "        train_user_list (list(set)):\n",
    "        test_user_list (list(set)):\n",
    "        k (list(int)):\n",
    "    Returns:\n",
    "        (torch.Tensor, torch.Tensor) HR and NDCG at k\n",
    "    \"\"\"\n",
    "\n",
    "    # Calculate max k value\n",
    "    max_k = max(klist)\n",
    "    result = None\n",
    "\n",
    "    # no iteration = user_num / batch size (which is 512)\n",
    "    for i in range(0, user_emb.shape[0], batch):\n",
    "\n",
    "        # Construct mask for each batch\n",
    "\n",
    "        #new_ones returns a Tensor of size size filled with 1\n",
    "\n",
    "        # size of the mask vector = (min of batch or user embed) * item+embed\n",
    "        mask = user_emb.new_ones([min([batch, user_emb.shape[0]-i]), item_emb.shape[0]])\n",
    "        for j in range(batch):\n",
    "            if i+j >= user_emb.shape[0]:\n",
    "                break\n",
    "            mask[j].scatter_(dim=0, index=torch.tensor(list(train_user_list[i + j])), value=torch.tensor(0.0))\n",
    "\n",
    "        # Get current result\n",
    "        cur_result = torch.mm(user_emb[i:i+min(batch, user_emb.shape[0]-i), :], item_emb.t())\n",
    "        cur_result = torch.sigmoid(cur_result)\n",
    "        assert not torch.any(torch.isnan(cur_result))\n",
    "\n",
    "        # Make zero for already observed item\n",
    "        cur_result = torch.mul(mask, cur_result)\n",
    "        _, cur_result = torch.topk(cur_result, k=max_k, dim=1)\n",
    "        result = cur_result if result is None else torch.cat((result, cur_result), dim=0)\n",
    "\n",
    "\n",
    "    ## basically this chunk collects the results\n",
    "    result = result.cpu()\n",
    "\n",
    "    # Sort indice and get HR_NDCG_topk\n",
    "    HRs, NDCGs = [], []\n",
    "    for k in klist:\n",
    "        ndcg, hr = 0, 0\n",
    "        #for all user\n",
    "        for i in range(user_emb.shape[0]):\n",
    "            #set helps to identify unique members in a list\n",
    "            test = set(test_user_list[i])\n",
    "            #top k item from prediction list\n",
    "            pred = set(result[i, :k].numpy().tolist())\n",
    "            #if topk lies on both test and pred list\n",
    "            val = len(test & pred)\n",
    "            #hit ratio = %item hit\n",
    "            hr += val / max([len(test), 1])\n",
    "            #convert pred back to list\n",
    "            pred = list(pred)\n",
    "            if test_user_list[i] == []:\n",
    "                continue\n",
    "            else:\n",
    "                for x in test_user_list[i]:\n",
    "                    x = int(x)\n",
    "                    ## check if x is in the prediction where x = 1st member of user list\n",
    "                    if pred.count(x) != 0:\n",
    "                        position = pred.index(x)\n",
    "                        ndcg += math.log(2) / math.log(position + 2) if position < k else 0\n",
    "                    else:\n",
    "                        ndcg += 0\n",
    "        NDCGs.append(ndcg / user_emb.shape[0])\n",
    "        HRs.append(hr / user_emb.shape[0])\n",
    "        NDCGs.append(ndcg / user_emb.shape[0])\n",
    "    return HRs, NDCGs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "f204e348",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('preprocessed/ml-1m-2.pickle', 'rb') as f:\n",
    "    dataset = pickle.load(f)\n",
    "    user_size, item_size = dataset['user_size'], dataset['item_size']\n",
    "    train_user_list, test_user_list = dataset['train_user_list'], dataset['test_user_list']\n",
    "    train_pair = dataset['train_pair']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "2e7ffdb1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create dataset, model, optimizer\n",
    "dataset = GetTriplePair(item_size, train_user_list, train_pair, True, 1000)\n",
    "\n",
    "# load batch of 512 item triplets\n",
    "loader = DataLoader(dataset, batch_size=512)\n",
    "model = MF(user_size, item_size, 64, 0, 1, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "d655d783",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time 0:00:00\n",
      "BPR-MF Epoch [20/1000]\n",
      "loss: 354.8221\n",
      "HR@50: 0.0327, HR@100: 0.0672, NDCG@50: 0.0827, NDCG@100: 0.0827\n",
      "BPR-MF Epoch [40/1000]\n",
      "loss: 354.7227\n",
      "HR@50: 0.0331, HR@100: 0.0682, NDCG@50: 0.0825, NDCG@100: 0.0825\n",
      "BPR-MF Epoch [60/1000]\n",
      "loss: 354.8884\n",
      "HR@50: 0.0336, HR@100: 0.0667, NDCG@50: 0.0847, NDCG@100: 0.0847\n",
      "BPR-MF Epoch [80/1000]\n",
      "loss: 354.8069\n",
      "HR@50: 0.0336, HR@100: 0.0680, NDCG@50: 0.0865, NDCG@100: 0.0865\n",
      "BPR-MF Epoch [100/1000]\n",
      "loss: 354.6473\n",
      "HR@50: 0.0336, HR@100: 0.0667, NDCG@50: 0.0896, NDCG@100: 0.0896\n",
      "BPR-MF Epoch [120/1000]\n",
      "loss: 354.8765\n",
      "HR@50: 0.0357, HR@100: 0.0688, NDCG@50: 0.0937, NDCG@100: 0.0937\n",
      "BPR-MF Epoch [140/1000]\n",
      "loss: 354.6185\n",
      "HR@50: 0.0385, HR@100: 0.0740, NDCG@50: 0.1001, NDCG@100: 0.1001\n",
      "BPR-MF Epoch [160/1000]\n",
      "loss: 354.3387\n",
      "HR@50: 0.0420, HR@100: 0.0810, NDCG@50: 0.1124, NDCG@100: 0.1124\n",
      "BPR-MF Epoch [180/1000]\n",
      "loss: 354.3116\n",
      "HR@50: 0.0481, HR@100: 0.0914, NDCG@50: 0.1299, NDCG@100: 0.1299\n",
      "BPR-MF Epoch [200/1000]\n",
      "loss: 353.4748\n",
      "HR@50: 0.0576, HR@100: 0.1062, NDCG@50: 0.1591, NDCG@100: 0.1591\n",
      "time 0:00:42\n",
      "BPR-MF Epoch [220/1000]\n",
      "loss: 351.4788\n",
      "HR@50: 0.0720, HR@100: 0.1293, NDCG@50: 0.1988, NDCG@100: 0.1988\n",
      "BPR-MF Epoch [240/1000]\n",
      "loss: 349.5712\n",
      "HR@50: 0.0874, HR@100: 0.1577, NDCG@50: 0.2479, NDCG@100: 0.2479\n",
      "BPR-MF Epoch [260/1000]\n",
      "loss: 347.3130\n",
      "HR@50: 0.1080, HR@100: 0.1899, NDCG@50: 0.3052, NDCG@100: 0.3052\n",
      "BPR-MF Epoch [280/1000]\n",
      "loss: 343.8051\n",
      "HR@50: 0.1282, HR@100: 0.2213, NDCG@50: 0.3602, NDCG@100: 0.3602\n",
      "BPR-MF Epoch [300/1000]\n",
      "loss: 335.9357\n",
      "HR@50: 0.1470, HR@100: 0.2544, NDCG@50: 0.4088, NDCG@100: 0.4088\n",
      "time 0:01:04\n",
      "BPR-MF Epoch [320/1000]\n",
      "loss: 330.7104\n",
      "HR@50: 0.1663, HR@100: 0.2822, NDCG@50: 0.4497, NDCG@100: 0.4497\n",
      "BPR-MF Epoch [340/1000]\n",
      "loss: 321.4340\n",
      "HR@50: 0.1836, HR@100: 0.3061, NDCG@50: 0.4830, NDCG@100: 0.4830\n",
      "BPR-MF Epoch [360/1000]\n",
      "loss: 309.7041\n",
      "HR@50: 0.1982, HR@100: 0.3245, NDCG@50: 0.5094, NDCG@100: 0.5094\n",
      "BPR-MF Epoch [380/1000]\n",
      "loss: 300.5055\n",
      "HR@50: 0.2113, HR@100: 0.3422, NDCG@50: 0.5317, NDCG@100: 0.5317\n",
      "BPR-MF Epoch [400/1000]\n",
      "loss: 278.1360\n",
      "HR@50: 0.2249, HR@100: 0.3553, NDCG@50: 0.5531, NDCG@100: 0.5531\n",
      "time 0:01:24\n",
      "BPR-MF Epoch [420/1000]\n",
      "loss: 269.9681\n",
      "HR@50: 0.2366, HR@100: 0.3692, NDCG@50: 0.5703, NDCG@100: 0.5703\n",
      "BPR-MF Epoch [440/1000]\n",
      "loss: 261.4644\n",
      "HR@50: 0.2440, HR@100: 0.3810, NDCG@50: 0.5824, NDCG@100: 0.5824\n",
      "BPR-MF Epoch [460/1000]\n",
      "loss: 253.5040\n",
      "HR@50: 0.2525, HR@100: 0.3888, NDCG@50: 0.5937, NDCG@100: 0.5937\n",
      "BPR-MF Epoch [480/1000]\n",
      "loss: 247.7039\n",
      "HR@50: 0.2601, HR@100: 0.3950, NDCG@50: 0.6062, NDCG@100: 0.6062\n",
      "BPR-MF Epoch [500/1000]\n",
      "loss: 220.3567\n",
      "HR@50: 0.2669, HR@100: 0.4022, NDCG@50: 0.6129, NDCG@100: 0.6129\n",
      "time 0:01:45\n",
      "BPR-MF Epoch [520/1000]\n",
      "loss: 220.9166\n",
      "HR@50: 0.2745, HR@100: 0.4071, NDCG@50: 0.6214, NDCG@100: 0.6214\n",
      "BPR-MF Epoch [540/1000]\n",
      "loss: 214.3093\n",
      "HR@50: 0.2803, HR@100: 0.4111, NDCG@50: 0.6289, NDCG@100: 0.6289\n",
      "BPR-MF Epoch [560/1000]\n",
      "loss: 191.9080\n",
      "HR@50: 0.2853, HR@100: 0.4151, NDCG@50: 0.6342, NDCG@100: 0.6342\n",
      "BPR-MF Epoch [580/1000]\n",
      "loss: 208.1477\n",
      "HR@50: 0.2901, HR@100: 0.4184, NDCG@50: 0.6411, NDCG@100: 0.6411\n",
      "BPR-MF Epoch [600/1000]\n",
      "loss: 195.4392\n",
      "HR@50: 0.2941, HR@100: 0.4220, NDCG@50: 0.6457, NDCG@100: 0.6457\n",
      "time 0:02:05\n",
      "BPR-MF Epoch [620/1000]\n",
      "loss: 201.1450\n",
      "HR@50: 0.2974, HR@100: 0.4259, NDCG@50: 0.6499, NDCG@100: 0.6499\n",
      "BPR-MF Epoch [640/1000]\n",
      "loss: 187.5263\n",
      "HR@50: 0.3004, HR@100: 0.4289, NDCG@50: 0.6537, NDCG@100: 0.6537\n",
      "BPR-MF Epoch [660/1000]\n",
      "loss: 183.0886\n",
      "HR@50: 0.3036, HR@100: 0.4322, NDCG@50: 0.6574, NDCG@100: 0.6574\n",
      "BPR-MF Epoch [680/1000]\n",
      "loss: 226.7212\n",
      "HR@50: 0.3060, HR@100: 0.4340, NDCG@50: 0.6608, NDCG@100: 0.6608\n",
      "BPR-MF Epoch [700/1000]\n",
      "loss: 197.6776\n",
      "HR@50: 0.3088, HR@100: 0.4352, NDCG@50: 0.6630, NDCG@100: 0.6630\n",
      "BPR-MF Epoch [720/1000]\n",
      "loss: 201.4046\n",
      "HR@50: 0.3111, HR@100: 0.4376, NDCG@50: 0.6670, NDCG@100: 0.6670\n",
      "BPR-MF Epoch [740/1000]\n",
      "loss: 192.7744\n",
      "HR@50: 0.3125, HR@100: 0.4393, NDCG@50: 0.6693, NDCG@100: 0.6693\n",
      "BPR-MF Epoch [760/1000]\n",
      "loss: 200.1867\n",
      "HR@50: 0.3149, HR@100: 0.4410, NDCG@50: 0.6726, NDCG@100: 0.6726\n",
      "BPR-MF Epoch [780/1000]\n",
      "loss: 166.7385\n",
      "HR@50: 0.3151, HR@100: 0.4420, NDCG@50: 0.6751, NDCG@100: 0.6751\n",
      "BPR-MF Epoch [800/1000]\n",
      "loss: 200.9524\n",
      "HR@50: 0.3166, HR@100: 0.4423, NDCG@50: 0.6780, NDCG@100: 0.6780\n",
      "BPR-MF Epoch [820/1000]\n",
      "loss: 213.6721\n",
      "HR@50: 0.3178, HR@100: 0.4440, NDCG@50: 0.6804, NDCG@100: 0.6804\n",
      "BPR-MF Epoch [840/1000]\n",
      "loss: 186.0199\n",
      "HR@50: 0.3188, HR@100: 0.4467, NDCG@50: 0.6836, NDCG@100: 0.6836\n",
      "BPR-MF Epoch [860/1000]\n",
      "loss: 186.0209\n",
      "HR@50: 0.3199, HR@100: 0.4493, NDCG@50: 0.6854, NDCG@100: 0.6854\n",
      "BPR-MF Epoch [880/1000]\n",
      "loss: 161.3351\n",
      "HR@50: 0.3212, HR@100: 0.4517, NDCG@50: 0.6881, NDCG@100: 0.6881\n",
      "BPR-MF Epoch [900/1000]\n",
      "loss: 169.5204\n",
      "HR@50: 0.3225, HR@100: 0.4543, NDCG@50: 0.6903, NDCG@100: 0.6903\n",
      "BPR-MF Epoch [920/1000]\n",
      "loss: 175.8014\n",
      "HR@50: 0.3240, HR@100: 0.4565, NDCG@50: 0.6924, NDCG@100: 0.6924\n",
      "BPR-MF Epoch [940/1000]\n",
      "loss: 189.1587\n",
      "HR@50: 0.3244, HR@100: 0.4588, NDCG@50: 0.6939, NDCG@100: 0.6939\n",
      "BPR-MF Epoch [960/1000]\n",
      "loss: 197.4830\n",
      "HR@50: 0.3249, HR@100: 0.4608, NDCG@50: 0.6969, NDCG@100: 0.6969\n",
      "BPR-MF Epoch [980/1000]\n",
      "loss: 183.4399\n",
      "HR@50: 0.3260, HR@100: 0.4627, NDCG@50: 0.6984, NDCG@100: 0.6984\n",
      "BPR-MF Epoch [1000/1000]\n",
      "loss: 196.0934\n",
      "HR@50: 0.3266, HR@100: 0.4649, NDCG@50: 0.7004, NDCG@100: 0.7004\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=0.00025)\n",
    "\n",
    "# Training\n",
    "start_time = time.time()\n",
    "eval_best_loss = float('inf')\n",
    "\n",
    "##zero_grad: zeroes the grad attribute of all the parameters passed to the optimizer upon construction\n",
    "optimizer.zero_grad()\n",
    "epoch = 0\n",
    "HR_history = []\n",
    "NDCG_history = []\n",
    "# result_history = []\n",
    "#loader has batch size of 512. In each batch there are 3 tensors of u i j accordingly\n",
    "for u, i, j in loader:\n",
    "    if epoch in range(1000):\n",
    "        loss = model(u, i, j, epoch)\n",
    "\n",
    "        ##  updates the value of those parameters according to the optimization strategy implemented by the specific optimizer.\n",
    "        optimizer.step()\n",
    "        HR_list, NDCG_list = evaluate_k(model.W.detach(),\n",
    "                                        model.H.detach(),\n",
    "                                        train_user_list,\n",
    "                                        test_user_list,\n",
    "                                        klist=[50, 100])\n",
    "        if epoch % 20 == (20- 1):\n",
    "            if epoch in range(1000):\n",
    "                print('BPR-MF Epoch [{}/{}]'.format(epoch + 1, 1000))\n",
    "            print('loss: %.4f' % loss)\n",
    "            print('HR@50: %.4f, HR@100: %.4f, NDCG@50: %.4f, NDCG@100: %.4f' % (\n",
    "                HR_list[0], HR_list[1], NDCG_list[0], NDCG_list[1]))\n",
    "        HR_history.append(HR_list[1])\n",
    "        NDCG_history.append(NDCG_list[1])\n",
    "        if epoch % 100 == 0:\n",
    "            if loss < eval_best_loss:\n",
    "                eval_best_loss = loss\n",
    "                dirname = os.path.dirname(os.path.abspath('output/bpr_manual'))\n",
    "                os.makedirs(dirname, exist_ok=True)\n",
    "                torch.save(model.state_dict(), 'output/bpr_manual')\n",
    "                time_dif = get_time_dif(start_time)\n",
    "                print(\"time\", time_dif)\n",
    "        epoch += 1\n",
    "    else:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b039d9",
   "metadata": {},
   "source": [
    "for ml-1m-6 data set with continue , epoch 1000 has\n",
    "loss: 160.6638\n",
    "HR@50: 0.3218, HR@100: 0.4558, NDCG@50: 0.0939, NDCG@100: 0.0939\n",
    "\n",
    "for ml-1m-6 data set with continue + for loop for X in test user set , epoch 1000 has\n",
    "loss: 196.0934\n",
    "HR@50: 0.3266, HR@100: 0.4649, NDCG@50: 0.7004, NDCG@100: 0.7004\n",
    "\n",
    "\n",
    "ml-1m normal dataset\n",
    "loss: 150.8645\n",
    "HR@50: 0.1437, HR@100: 0.2245, NDCG@50: 0.0365, NDCG@100: 0.0365\n",
    "\n",
    "test ml-1m normal dataset with for loop for X in test user set\n",
    "loss: 181.7949\n",
    "HR@50: 0.1480, HR@100: 0.2252, NDCG@50: 0.0373, NDCG@100: 0.0373"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "29d7f1e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig_HR = plt.figure(edgecolor='blue')\n",
    "ax1 = fig_HR.add_subplot(111)\n",
    "plt.ylabel('HR@100')\n",
    "plt.xlabel('Epoch')\n",
    "plt.title('ML-1M')\n",
    "ax1.plot(range(len(HR_history)), HR_history, c=np.array([255, 71, 90]) / 255.)\n",
    "plt.show()\n",
    "fig_P = plt.figure(edgecolor='blue')\n",
    "ax1 = fig_P.add_subplot(111)\n",
    "plt.ylabel('NDCG@100')\n",
    "plt.xlabel('Epoch')\n",
    "plt.title('ML-1M')\n",
    "ax1.plot(range(len(NDCG_history)), NDCG_history, c=np.array([255, 71, 90]) / 255.)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "886dcf79",
   "metadata": {},
   "source": [
    "Running a saving output of BPR model\n",
    "\n",
    "Next up = train adversary based on fixed parameters of BPR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "b6ae75b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "key_genre = pd.read_pickle(r'ml1m-6/key_genre.pkl')  \n",
    "item_idd_genre_list = pd.read_pickle(r'ml1m-6/item_idd_genre_list.pkl')   \n",
    "genre_item_vector = pd.read_pickle(r'ml1m-6/genre_item_vector.pkl')    \n",
    "genre_count = pd.read_pickle(r'ml1m-6/genre_count.pkl')      \n",
    "user_genre_count = pd.read_pickle(r'ml1m-6/user_genre_count.pkl') \n",
    "\n",
    "num_item = len(train_df['item_id'].unique())\n",
    "num_user = len(train_df['user_id'].unique())\n",
    "num_genre = len(key_genre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "364be573",
   "metadata": {},
   "outputs": [],
   "source": [
    "item_genre_list = []\n",
    "for u in range(num_item):\n",
    "    gl = item_idd_genre_list[u]\n",
    "    tmp = []\n",
    "    for g in gl:\n",
    "        if g in key_genre:\n",
    "            tmp.append(g)\n",
    "    item_genre_list.append(tmp)\n",
    "\n",
    "item_genre = np.zeros((num_item, num_genre))\n",
    "for i in range(num_item):\n",
    "    gl = item_genre_list[i]\n",
    "    for k in range(num_genre):\n",
    "        if key_genre[k] in gl:\n",
    "            item_genre[i, k] = 1.0\n",
    "\n",
    "genre_count_mean_reciprocal = []\n",
    "for k in key_genre:\n",
    "    genre_count_mean_reciprocal.append(1.0 / genre_count[k])\n",
    "genre_count_mean_reciprocal = (np.array(genre_count_mean_reciprocal)).reshape((num_genre, 1))\n",
    "genre_error_weight = np.dot(item_genre, genre_count_mean_reciprocal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "2326714e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 1., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 1., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item_genre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "55f5af0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1481, 6])"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item_genre = torch.from_numpy(item_genre).type(torch.float)\n",
    "item_genre.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "362e1981",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    " \n",
    "# Function to generate\n",
    "# and append them\n",
    "# start = starting range,\n",
    "# end = ending range\n",
    "# num = number of\n",
    "# elements needs to be appended\n",
    "def Rand(start, end, num):\n",
    "    res = []\n",
    " \n",
    "    for j in range(num):\n",
    "        res.append(random.randint(start, end))\n",
    " \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "2000011e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([6036, 64]), torch.Size([1481, 64]))"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load the results of BPR\n",
    "model1 = (torch.load('output/bpr_manual'))\n",
    "list(model1.items())[0][1].size(),list(model1.items())[1][1].size()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "0c24762f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6036, 1481])"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Rec = np.matmul(list(model1.items())[0][1], list(model1.items())[1][1].T)\n",
    "Rec.size()\n",
    "# Rec[1,:].size()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "fcf00749",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_id = Rand(0,list(model1.items())[1][1].size()[0],1200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "90785b71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1184, 297, 1184, 297)"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(Rec.T, \n",
    "                                                    item_genre, \n",
    "                                                    test_size=0.2, # 20% test, 80% train\n",
    "                                                    random_state=42) # make the random split reproducible\n",
    "\n",
    "len(X_train), len(X_test), len(y_train), len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "494ea526",
   "metadata": {},
   "outputs": [],
   "source": [
    "#adv, linear-relu, linear-sigmoid last layer, numlayer = 4, 512,256,128,64 hidden units \n",
    "\n",
    "adversary = nn.Sequential(\n",
    "    nn.Linear(list(model1.items())[0][1].size()[0], 256),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(256, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128, 64),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(64, 6),\n",
    "    nn.Sigmoid()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "8bf3ee5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a loss function\n",
    "# loss_fn = nn.BCELoss() # BCELoss = no sigmoid built-in\n",
    "loss_fn = nn.CrossEntropyLoss() # BCEWithLogitsLoss = sigmoid built-in\n",
    "\n",
    "# Create an optimizer\n",
    "optimizer = torch.optim.SGD(params=adversary.parameters(), \n",
    "                            lr=0.00025)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "896d5686",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate accuracy (a classification metric)\n",
    "def accuracy_fn(y_true, y_pred):\n",
    "    correct = torch.eq(y_true, y_pred).sum().item() # torch.eq() calculates where two tensors are equal\n",
    "    acc = (correct / len(y_pred)) * 100 \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "28bda2f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Loss: 2.15301, Accuracy: 282.77% | Test loss: 2.06401, Test acc: 275.42%\n",
      "Epoch: 10 | Loss: 2.15295, Accuracy: 283.02% | Test loss: 2.06395, Test acc: 276.09%\n",
      "Epoch: 20 | Loss: 2.15289, Accuracy: 283.36% | Test loss: 2.06388, Test acc: 276.77%\n",
      "Epoch: 30 | Loss: 2.15283, Accuracy: 283.61% | Test loss: 2.06382, Test acc: 277.10%\n",
      "Epoch: 40 | Loss: 2.15277, Accuracy: 284.54% | Test loss: 2.06376, Test acc: 277.44%\n",
      "Epoch: 50 | Loss: 2.15271, Accuracy: 284.46% | Test loss: 2.06369, Test acc: 278.45%\n",
      "Epoch: 60 | Loss: 2.15265, Accuracy: 284.88% | Test loss: 2.06363, Test acc: 279.46%\n",
      "Epoch: 70 | Loss: 2.15259, Accuracy: 285.30% | Test loss: 2.06357, Test acc: 280.13%\n",
      "Epoch: 80 | Loss: 2.15254, Accuracy: 286.15% | Test loss: 2.06351, Test acc: 280.47%\n",
      "Epoch: 90 | Loss: 2.15248, Accuracy: 286.57% | Test loss: 2.06345, Test acc: 280.47%\n",
      "Epoch: 100 | Loss: 2.15242, Accuracy: 287.25% | Test loss: 2.06339, Test acc: 281.82%\n",
      "Epoch: 110 | Loss: 2.15236, Accuracy: 288.18% | Test loss: 2.06332, Test acc: 281.82%\n",
      "Epoch: 120 | Loss: 2.15231, Accuracy: 288.77% | Test loss: 2.06326, Test acc: 281.82%\n",
      "Epoch: 130 | Loss: 2.15225, Accuracy: 289.36% | Test loss: 2.06320, Test acc: 282.49%\n",
      "Epoch: 140 | Loss: 2.15219, Accuracy: 289.70% | Test loss: 2.06314, Test acc: 282.83%\n",
      "Epoch: 150 | Loss: 2.15214, Accuracy: 290.79% | Test loss: 2.06308, Test acc: 282.83%\n",
      "Epoch: 160 | Loss: 2.15208, Accuracy: 291.64% | Test loss: 2.06302, Test acc: 284.51%\n",
      "Epoch: 170 | Loss: 2.15202, Accuracy: 292.06% | Test loss: 2.06296, Test acc: 285.86%\n",
      "Epoch: 180 | Loss: 2.15197, Accuracy: 292.40% | Test loss: 2.06290, Test acc: 286.87%\n",
      "Epoch: 190 | Loss: 2.15191, Accuracy: 292.65% | Test loss: 2.06284, Test acc: 288.22%\n",
      "Epoch: 200 | Loss: 2.15186, Accuracy: 293.33% | Test loss: 2.06278, Test acc: 288.55%\n",
      "Epoch: 210 | Loss: 2.15180, Accuracy: 294.00% | Test loss: 2.06272, Test acc: 288.55%\n",
      "Epoch: 220 | Loss: 2.15174, Accuracy: 294.76% | Test loss: 2.06266, Test acc: 288.89%\n",
      "Epoch: 230 | Loss: 2.15169, Accuracy: 295.44% | Test loss: 2.06260, Test acc: 289.23%\n",
      "Epoch: 240 | Loss: 2.15163, Accuracy: 295.86% | Test loss: 2.06254, Test acc: 290.24%\n",
      "Epoch: 250 | Loss: 2.15158, Accuracy: 296.11% | Test loss: 2.06248, Test acc: 289.90%\n",
      "Epoch: 260 | Loss: 2.15152, Accuracy: 296.20% | Test loss: 2.06242, Test acc: 290.24%\n",
      "Epoch: 270 | Loss: 2.15146, Accuracy: 296.62% | Test loss: 2.06236, Test acc: 290.57%\n",
      "Epoch: 280 | Loss: 2.15141, Accuracy: 297.13% | Test loss: 2.06231, Test acc: 291.25%\n",
      "Epoch: 290 | Loss: 2.15135, Accuracy: 297.72% | Test loss: 2.06225, Test acc: 292.26%\n",
      "Epoch: 300 | Loss: 2.15130, Accuracy: 298.06% | Test loss: 2.06219, Test acc: 292.59%\n",
      "Epoch: 310 | Loss: 2.15125, Accuracy: 298.48% | Test loss: 2.06213, Test acc: 292.59%\n",
      "Epoch: 320 | Loss: 2.15119, Accuracy: 298.56% | Test loss: 2.06207, Test acc: 292.26%\n",
      "Epoch: 330 | Loss: 2.15114, Accuracy: 298.48% | Test loss: 2.06202, Test acc: 292.59%\n",
      "Epoch: 340 | Loss: 2.15108, Accuracy: 298.40% | Test loss: 2.06196, Test acc: 292.93%\n",
      "Epoch: 350 | Loss: 2.15103, Accuracy: 298.40% | Test loss: 2.06190, Test acc: 293.27%\n",
      "Epoch: 360 | Loss: 2.15098, Accuracy: 298.56% | Test loss: 2.06185, Test acc: 292.93%\n",
      "Epoch: 370 | Loss: 2.15092, Accuracy: 298.56% | Test loss: 2.06179, Test acc: 293.27%\n",
      "Epoch: 380 | Loss: 2.15087, Accuracy: 298.90% | Test loss: 2.06174, Test acc: 293.27%\n",
      "Epoch: 390 | Loss: 2.15082, Accuracy: 299.32% | Test loss: 2.06168, Test acc: 293.27%\n",
      "Epoch: 400 | Loss: 2.15076, Accuracy: 299.32% | Test loss: 2.06162, Test acc: 293.60%\n",
      "Epoch: 410 | Loss: 2.15071, Accuracy: 299.58% | Test loss: 2.06157, Test acc: 293.60%\n",
      "Epoch: 420 | Loss: 2.15066, Accuracy: 299.66% | Test loss: 2.06151, Test acc: 294.28%\n",
      "Epoch: 430 | Loss: 2.15061, Accuracy: 299.75% | Test loss: 2.06146, Test acc: 293.94%\n",
      "Epoch: 440 | Loss: 2.15055, Accuracy: 300.08% | Test loss: 2.06140, Test acc: 293.94%\n",
      "Epoch: 450 | Loss: 2.15050, Accuracy: 300.17% | Test loss: 2.06135, Test acc: 293.94%\n",
      "Epoch: 460 | Loss: 2.15045, Accuracy: 300.34% | Test loss: 2.06130, Test acc: 294.28%\n",
      "Epoch: 470 | Loss: 2.15040, Accuracy: 300.42% | Test loss: 2.06124, Test acc: 294.28%\n",
      "Epoch: 480 | Loss: 2.15035, Accuracy: 300.42% | Test loss: 2.06119, Test acc: 294.28%\n",
      "Epoch: 490 | Loss: 2.15030, Accuracy: 300.51% | Test loss: 2.06114, Test acc: 294.28%\n",
      "Epoch: 500 | Loss: 2.15025, Accuracy: 300.59% | Test loss: 2.06109, Test acc: 294.28%\n",
      "Epoch: 510 | Loss: 2.15020, Accuracy: 300.59% | Test loss: 2.06103, Test acc: 294.95%\n",
      "Epoch: 520 | Loss: 2.15015, Accuracy: 300.68% | Test loss: 2.06098, Test acc: 294.95%\n",
      "Epoch: 530 | Loss: 2.15010, Accuracy: 300.76% | Test loss: 2.06093, Test acc: 295.62%\n",
      "Epoch: 540 | Loss: 2.15005, Accuracy: 300.93% | Test loss: 2.06088, Test acc: 295.62%\n",
      "Epoch: 550 | Loss: 2.15000, Accuracy: 300.93% | Test loss: 2.06083, Test acc: 295.62%\n",
      "Epoch: 560 | Loss: 2.14995, Accuracy: 301.27% | Test loss: 2.06078, Test acc: 295.96%\n",
      "Epoch: 570 | Loss: 2.14990, Accuracy: 301.44% | Test loss: 2.06073, Test acc: 295.62%\n",
      "Epoch: 580 | Loss: 2.14985, Accuracy: 301.44% | Test loss: 2.06068, Test acc: 294.95%\n",
      "Epoch: 590 | Loss: 2.14981, Accuracy: 301.44% | Test loss: 2.06063, Test acc: 295.29%\n",
      "Epoch: 600 | Loss: 2.14976, Accuracy: 301.60% | Test loss: 2.06059, Test acc: 295.96%\n",
      "Epoch: 610 | Loss: 2.14972, Accuracy: 301.77% | Test loss: 2.06054, Test acc: 295.96%\n",
      "Epoch: 620 | Loss: 2.14967, Accuracy: 301.94% | Test loss: 2.06049, Test acc: 295.96%\n",
      "Epoch: 630 | Loss: 2.14962, Accuracy: 301.94% | Test loss: 2.06044, Test acc: 295.62%\n",
      "Epoch: 640 | Loss: 2.14958, Accuracy: 302.03% | Test loss: 2.06040, Test acc: 295.96%\n",
      "Epoch: 650 | Loss: 2.14953, Accuracy: 302.03% | Test loss: 2.06035, Test acc: 295.62%\n",
      "Epoch: 660 | Loss: 2.14949, Accuracy: 302.03% | Test loss: 2.06030, Test acc: 295.62%\n",
      "Epoch: 670 | Loss: 2.14945, Accuracy: 301.94% | Test loss: 2.06026, Test acc: 295.62%\n",
      "Epoch: 680 | Loss: 2.14940, Accuracy: 302.11% | Test loss: 2.06021, Test acc: 295.62%\n",
      "Epoch: 690 | Loss: 2.14936, Accuracy: 302.20% | Test loss: 2.06017, Test acc: 295.29%\n",
      "Epoch: 700 | Loss: 2.14931, Accuracy: 302.11% | Test loss: 2.06012, Test acc: 295.29%\n",
      "Epoch: 710 | Loss: 2.14927, Accuracy: 302.20% | Test loss: 2.06008, Test acc: 295.29%\n",
      "Epoch: 720 | Loss: 2.14923, Accuracy: 302.11% | Test loss: 2.06003, Test acc: 295.62%\n",
      "Epoch: 730 | Loss: 2.14918, Accuracy: 302.20% | Test loss: 2.05999, Test acc: 295.62%\n",
      "Epoch: 740 | Loss: 2.14914, Accuracy: 302.11% | Test loss: 2.05994, Test acc: 295.62%\n",
      "Epoch: 750 | Loss: 2.14910, Accuracy: 302.28% | Test loss: 2.05990, Test acc: 295.62%\n",
      "Epoch: 760 | Loss: 2.14905, Accuracy: 302.28% | Test loss: 2.05986, Test acc: 295.29%\n",
      "Epoch: 770 | Loss: 2.14901, Accuracy: 302.20% | Test loss: 2.05981, Test acc: 294.95%\n",
      "Epoch: 780 | Loss: 2.14897, Accuracy: 301.94% | Test loss: 2.05977, Test acc: 294.95%\n",
      "Epoch: 790 | Loss: 2.14893, Accuracy: 301.77% | Test loss: 2.05972, Test acc: 294.61%\n",
      "Epoch: 800 | Loss: 2.14888, Accuracy: 301.52% | Test loss: 2.05968, Test acc: 294.61%\n",
      "Epoch: 810 | Loss: 2.14884, Accuracy: 301.27% | Test loss: 2.05964, Test acc: 294.28%\n",
      "Epoch: 820 | Loss: 2.14880, Accuracy: 301.10% | Test loss: 2.05959, Test acc: 294.28%\n",
      "Epoch: 830 | Loss: 2.14876, Accuracy: 300.34% | Test loss: 2.05955, Test acc: 294.95%\n",
      "Epoch: 840 | Loss: 2.14872, Accuracy: 300.08% | Test loss: 2.05950, Test acc: 294.61%\n",
      "Epoch: 850 | Loss: 2.14867, Accuracy: 299.92% | Test loss: 2.05946, Test acc: 293.94%\n",
      "Epoch: 860 | Loss: 2.14863, Accuracy: 299.83% | Test loss: 2.05942, Test acc: 293.60%\n",
      "Epoch: 870 | Loss: 2.14859, Accuracy: 299.16% | Test loss: 2.05937, Test acc: 293.60%\n",
      "Epoch: 880 | Loss: 2.14855, Accuracy: 298.48% | Test loss: 2.05933, Test acc: 293.60%\n",
      "Epoch: 890 | Loss: 2.14850, Accuracy: 298.14% | Test loss: 2.05929, Test acc: 292.93%\n",
      "Epoch: 900 | Loss: 2.14846, Accuracy: 297.80% | Test loss: 2.05924, Test acc: 291.25%\n",
      "Epoch: 910 | Loss: 2.14842, Accuracy: 296.96% | Test loss: 2.05920, Test acc: 288.22%\n",
      "Epoch: 920 | Loss: 2.14838, Accuracy: 295.10% | Test loss: 2.05916, Test acc: 286.87%\n",
      "Epoch: 930 | Loss: 2.14834, Accuracy: 294.09% | Test loss: 2.05911, Test acc: 286.53%\n",
      "Epoch: 940 | Loss: 2.14829, Accuracy: 293.33% | Test loss: 2.05907, Test acc: 285.86%\n",
      "Epoch: 950 | Loss: 2.14825, Accuracy: 292.82% | Test loss: 2.05902, Test acc: 286.20%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 960 | Loss: 2.14821, Accuracy: 292.23% | Test loss: 2.05898, Test acc: 284.51%\n",
      "Epoch: 970 | Loss: 2.14817, Accuracy: 291.64% | Test loss: 2.05894, Test acc: 283.16%\n",
      "Epoch: 980 | Loss: 2.14812, Accuracy: 291.13% | Test loss: 2.05889, Test acc: 283.50%\n",
      "Epoch: 990 | Loss: 2.14808, Accuracy: 289.95% | Test loss: 2.05885, Test acc: 281.48%\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(18)\n",
    "\n",
    "# Set the number of epochs\n",
    "epochs = 1000\n",
    "\n",
    "# Put data to target device\n",
    "X_train, y_train = X_train.to(device), y_train.to(device)\n",
    "X_test, y_test = X_test.to(device), y_test.to(device)\n",
    "\n",
    "# Build training and evaluation loop\n",
    "for epoch in range(epochs):\n",
    "    ### Training\n",
    "    adversary.train()\n",
    "\n",
    "    # 1. Forward pass (model outputs raw logits)\n",
    "    y_logits = adversary(X_train).squeeze() # squeeze to remove extra `1` dimensions, this won't work unless model and data are on same device \n",
    "    y_pred = torch.round((y_logits)) # turn logits -> pred probs -> pred labls\n",
    "  \n",
    "    # 2. Calculate loss/accuracy\n",
    "    # loss = loss_fn(torch.sigmoid(y_logits), # Using nn.BCELoss you need torch.sigmoid()\n",
    "    #                y_train) \n",
    "    loss = loss_fn(y_logits, # Using nn.BCEWithLogitsLoss works with raw logits\n",
    "                   y_train) \n",
    "    acc = accuracy_fn(y_true=y_train, \n",
    "                      y_pred=y_pred) \n",
    "\n",
    "    # 3. Optimizer zero grad\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # 4. Loss backwards\n",
    "    loss.backward()\n",
    "\n",
    "    # 5. Optimizer step\n",
    "    optimizer.step()\n",
    "\n",
    "    ### Testing\n",
    "    adversary.eval()\n",
    "    with torch.inference_mode():\n",
    "        # 1. Forward pass\n",
    "        test_logits = adversary(X_test).squeeze() \n",
    "        test_pred = torch.round((test_logits))\n",
    "        # 2. Caculate loss/accuracy\n",
    "        test_loss = loss_fn(test_logits,\n",
    "                            y_test)\n",
    "        test_acc = accuracy_fn(y_true=y_test,\n",
    "                               y_pred=test_pred)\n",
    "\n",
    "    # Print out what's happening every 10 epochs\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch: {epoch} | Loss: {loss:.5f}, Accuracy: {acc:.2f}% | Test loss: {test_loss:.5f}, Test acc: {test_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97014932",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
