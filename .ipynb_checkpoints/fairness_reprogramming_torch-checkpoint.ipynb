{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35dba935",
   "metadata": {},
   "source": [
    "Thinking of APR. Are we doing the dual optimization with the classification model + the pertubation added?\n",
    "\n",
    "1. Run BPR and retrieve the model parameters + predict with fixed parameters\n",
    "2. Use predictions by BPR, train an adversary, save the trained parameters of the MLPs\n",
    "3. Load parameters of BPR, optimize for a perturbations with the new overall objective\n",
    "    Loss BPR (with fixed params + perturbation) + Loss adversary (with fixed params + perturbation)\n",
    "\n",
    "\n",
    "To Do?\n",
    "\n",
    "\n",
    "1. How to compute P(g = G1)? How to retrieve $P_{adv}(i)$ and $P_{adv}(j)$. As of now it is computed as, for a batch of 512 predictions for group membership, compute % of item predicted as G1, G2,...,Gn\n",
    "2. Audit the formula for REO and RSP\n",
    "3. Tranformation function is necessary, how can we define such transformation? i.e. cannot directly optimize embedding + $\\delta$ if $\\delta$ is just a fixed tensor size = embedding dim\n",
    "4. Tuning for the adversary\n",
    "5. Test on more extensive dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04a30440",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import relevant library\n",
    "\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import pickle\n",
    "import argparse\n",
    "from collections import deque\n",
    "import time\n",
    "import utility\n",
    "import tqdm\n",
    "import copy\n",
    "from operator import itemgetter\n",
    "from datetime import timedelta\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "# from torchvision.datasets import CIFAR10\n",
    "from torch.utils.data import DataLoader\n",
    "# from torchvision import transforms\n",
    "from torch.utils.data import IterableDataset, DataLoader, get_worker_info\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f262aa84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d897bb8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## time the process\n",
    "def get_time_dif(start_time):\n",
    "    \"\"\"get the running time\"\"\"\n",
    "    end_time = time.time()\n",
    "    time_dif = end_time - start_time\n",
    "    return timedelta(seconds=int(round(time_dif)))\n",
    "\n",
    "\n",
    "## set up the u,i,j triplet for BPR framework\n",
    "class GetTriplePair(IterableDataset):\n",
    "    # for ml-1m we load in 3760 item 6040 user and 994169 train pair\n",
    "    def __init__(self, item_size, user_list, pair, shuffle, num_epochs):\n",
    "        self.item_size = item_size\n",
    "        self.user_list = user_list\n",
    "        self.pair = pair\n",
    "        self.shuffle = shuffle\n",
    "        self.num_epochs = num_epochs\n",
    "\n",
    "    def __iter__(self):\n",
    "        self.example_size = self.num_epochs * len(self.pair)\n",
    "        self.example_index_queue = deque([])\n",
    "        self.seed = 0\n",
    "        self.start_list_index = None\n",
    "        self.num_workers = 1\n",
    "        self.index = 0\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        if self.index >= self.example_size:\n",
    "            raise StopIteration\n",
    "        # If `example_index_queue` is used up, replenish this list.\n",
    "        while len(self.example_index_queue) == 0:\n",
    "            index_list = list(range(len(self.pair)))\n",
    "            if self.shuffle:\n",
    "                random.Random(self.seed).shuffle(index_list)\n",
    "                self.seed += 1\n",
    "            if self.start_list_index is not None:\n",
    "                index_list = index_list[self.start_list_index::self.num_workers]\n",
    "\n",
    "                # Calculate next start index\n",
    "                self.start_list_index = (self.start_list_index + (self.num_workers - (len(self.pair) % self.num_workers))) % self.num_workers\n",
    "            self.example_index_queue.extend(index_list)\n",
    "        result = self._example(self.example_index_queue.popleft())\n",
    "        self.index += self.num_workers\n",
    "        return result\n",
    "\n",
    "    def _example(self, idx):\n",
    "        # in a train pair, format = (u,i), j = a random item which does not exist in user u's list of items\n",
    "        u = self.pair[idx][0]\n",
    "        i = self.pair[idx][1]\n",
    "        j = np.random.randint(self.item_size)\n",
    "        while j in self.user_list[u]:\n",
    "            j = np.random.randint(self.item_size)\n",
    "        return u, i, j\n",
    "\n",
    "## chunk to define matrix factorization part\n",
    "class MF(nn.Module):\n",
    "    def __init__(self, user_size, item_size, dim, reg, reg_adv, eps):\n",
    "        super().__init__()\n",
    "        ##init the embedding for U and I\n",
    "        self.W = nn.Parameter(torch.empty(user_size, dim))  # User embedding\n",
    "        self.H = nn.Parameter(torch.empty(item_size, dim))  # Item embedding\n",
    "        nn.init.xavier_normal_(self.W.data)\n",
    "        nn.init.xavier_normal_(self.H.data)\n",
    "        self.reg = reg\n",
    "        self.user_size = user_size\n",
    "        self.item_size = item_size\n",
    "        self.dim = dim\n",
    "        self.reg_adv = reg_adv\n",
    "        self.eps = eps\n",
    "        self.update_u = None\n",
    "        self.update_i = None\n",
    "        self.update_j = None\n",
    "\n",
    "## forward cal\n",
    "    def forward(self, u, i, j, epoch):\n",
    "        \"\"\"Return loss value.\n",
    "\n",
    "        Args:\n",
    "            u(torch.LongTensor): tensor stored user indexes. [batch_size,]\n",
    "            i(torch.LongTensor): tensor stored item indexes which is prefered by user. [batch_size,]\n",
    "            j(torch.LongTensor): tensor stored item indexes which is not prefered by user. [batch_size,]\n",
    "            epoch\n",
    "\n",
    "        Returns:\n",
    "            torch.FloatTensor\n",
    "        \"\"\"\n",
    "        ##u,i,j respectively, each is a vector of dim embedding (default = 64)\n",
    "        u = self.W[u, :]\n",
    "        i = self.H[i, :]\n",
    "        j = self.H[j, :]\n",
    "\n",
    "        ## Enables this Tensor to have their grad populated during backward(), convert any non-leaf tensor into a leaf tensor,\n",
    "        ##https://stackoverflow.com/questions/73698041/how-retain-grad-in-pytorch-works-i-found-its-position-changes-the-grad-result\n",
    "        u.retain_grad()\n",
    "        u_clone = u.data.clone()\n",
    "        i.retain_grad()\n",
    "        i_clone = i.data.clone()\n",
    "        j.retain_grad()\n",
    "        j_clone = j.data.clone()\n",
    "\n",
    "        ## mf, dot product of user with pos/neg item\n",
    "        x_ui = torch.mul(u, i).sum(dim=1)\n",
    "        x_uj = torch.mul(u, j).sum(dim=1)\n",
    "\n",
    "\n",
    "        #similar to clip value, find diff between ui and uj\n",
    "        x_uij =torch.clamp(x_ui - x_uj,min=-80.0,max=1e8)\n",
    "        #logsigmoid this is equivalent to equation 1 in the paper (classic loss of bpr)\n",
    "        log_prob = F.logsigmoid(x_uij).sum()\n",
    "        # regularization = lambda * l2 norm of u, i, j\n",
    "        regularization = self.reg * (u.norm(dim=1).pow(2).sum() + i.norm(dim=1).pow(2).sum() + j.norm(dim=1).pow(2).sum())\n",
    "\n",
    "        ## original bpr loss,\n",
    "        loss = -log_prob + regularization\n",
    "\n",
    "        loss.backward()\n",
    "        return loss\n",
    "        # add adv training after a certain number of epochs, here is the part which we add hypernet module\n",
    "        if epoch not in range(args.epochs, args.adv_epoch + args.epochs):\n",
    "            \"\"\"Normal training\"\"\"\n",
    "            loss.backward()\n",
    "            return loss\n",
    "\n",
    "        else:\n",
    "            \"\"\"Adversarial training:\n",
    "                    1.Backward to get grads\n",
    "                    2.Construct adversarial perturbation\n",
    "                    3.Add adversarial perturbation to embeddings\n",
    "                    4.Calculate APR loss\n",
    "            \"\"\"\n",
    "            # Backward to get grads\n",
    "            # this would be the part we change in defining delta, delta = HPN (phi)\n",
    "\n",
    "            # should we calculate based on gradient of the adv_loss instead of the loss function?, originally, computed based on loss function\n",
    "            loss.backward(retain_graph=True) ## need to retain graph here so as to we can backprop the adv_loss\n",
    "            ##recheck this\n",
    "            grad_u = u.grad\n",
    "            grad_i = i.grad\n",
    "            grad_j = j.grad\n",
    "\n",
    "            # Construct adversarial perturbation based on gradient of loss function, and normalize it with epsilon * norm\n",
    "            if grad_u is not None:\n",
    "                delta_u = nn.functional.normalize(grad_u, p=2, dim=1, eps=self.eps)\n",
    "            else:\n",
    "                delta_u = torch.rand(u.size())\n",
    "            if grad_i is not None:\n",
    "                delta_i = nn.functional.normalize(grad_i, p=2, dim=1, eps=self.eps)\n",
    "            else:\n",
    "                delta_i = torch.rand(i.size())\n",
    "            if grad_j is not None:\n",
    "                delta_j = nn.functional.normalize(grad_j, p=2, dim=1, eps=self.eps)\n",
    "            else:\n",
    "                delta_j = torch.rand(j.size())\n",
    "\n",
    "            # Add adversarial perturbation to embeddings, now we have q+delta, p+delta\n",
    "            x_ui_adv = torch.mul(u + delta_u, i + delta_i).sum(dim=1)\n",
    "            x_uj_adv = torch.mul(u + delta_u, j + delta_j).sum(dim=1)\n",
    "\n",
    "            # find difference between pos and neg item, then clip value\n",
    "            x_uij_adv = torch.clamp(x_ui_adv - x_uj_adv,min=-80.0,max=1e8)\n",
    "\n",
    "            # Calculate APR loss with logsigmoid\n",
    "            log_prob = F.logsigmoid(x_uij_adv).sum()\n",
    "            adv_loss = self.reg_adv *(-log_prob) + loss # this is adversarial loss (equation 4 in paper)\n",
    "            adv_loss.backward()\n",
    "\n",
    "            return adv_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1a455a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_k(user_emb, item_emb, train_user_list, test_user_list, klist, batch=512):\n",
    "    \"\"\"Compute HR and NDCG at k.\n",
    "\n",
    "    Args:\n",
    "        user_emb (torch.Tensor): embedding for user [user_num, dim]\n",
    "        item_emb (torch.Tensor): embedding for item [item_num, dim]\n",
    "        train_user_list (list(set)):\n",
    "        test_user_list (list(set)):\n",
    "        k (list(int)):\n",
    "    Returns:\n",
    "        (torch.Tensor, torch.Tensor) HR and NDCG at k\n",
    "    \"\"\"\n",
    "\n",
    "    # Calculate max k value\n",
    "    max_k = max(klist)\n",
    "    result = None\n",
    "\n",
    "    # no iteration = user_num / batch size (which is 512)\n",
    "    for i in range(0, user_emb.shape[0], batch):\n",
    "\n",
    "        # Construct mask for each batch\n",
    "\n",
    "        #new_ones returns a Tensor of size size filled with 1\n",
    "\n",
    "        # size of the mask vector = (min of batch or user embed) * item+embed\n",
    "        mask = user_emb.new_ones([min([batch, user_emb.shape[0]-i]), item_emb.shape[0]])\n",
    "        for j in range(batch):\n",
    "            if i+j >= user_emb.shape[0]:\n",
    "                break\n",
    "            mask[j].scatter_(dim=0, index=torch.tensor(list(train_user_list[i + j])), value=torch.tensor(0.0))\n",
    "\n",
    "        # Get current result\n",
    "        cur_result = torch.mm(user_emb[i:i+min(batch, user_emb.shape[0]-i), :], item_emb.t())\n",
    "        cur_result = torch.sigmoid(cur_result)\n",
    "        assert not torch.any(torch.isnan(cur_result))\n",
    "\n",
    "        # Make zero for already observed item\n",
    "        cur_result = torch.mul(mask, cur_result)\n",
    "        _, cur_result = torch.topk(cur_result, k=max_k, dim=1)\n",
    "        result = cur_result if result is None else torch.cat((result, cur_result), dim=0)\n",
    "\n",
    "\n",
    "    ## basically this chunk collects the results\n",
    "    result = result.cpu()\n",
    "\n",
    "    # Sort indice and get HR_NDCG_topk\n",
    "    HRs, NDCGs = [], []\n",
    "    for k in klist:\n",
    "        ndcg, hr = 0, 0\n",
    "        #for all user\n",
    "        for i in range(user_emb.shape[0]):\n",
    "            #set helps to identify unique members in a list\n",
    "            test = set(test_user_list[i])\n",
    "            #top k item from prediction list\n",
    "            pred = set(result[i, :k].numpy().tolist())\n",
    "            #if topk lies on both test and pred list\n",
    "            val = len(test & pred)\n",
    "            #hit ratio = %item hit\n",
    "            hr += val / max([len(test), 1])\n",
    "            #convert pred back to list\n",
    "            pred = list(pred)\n",
    "            if test_user_list[i] == []:\n",
    "                continue\n",
    "            else:\n",
    "                x = int(test_user_list[i][0])\n",
    "                ## check if x is in the prediction where x = 1st member of user list\n",
    "                if pred.count(x) != 0:\n",
    "                    position = pred.index(x)\n",
    "                    ndcg += math.log(2) / math.log(position + 2) if position < k else 0\n",
    "                else:\n",
    "                    ndcg += 0                \n",
    "#                 for x in test_user_list[i]:\n",
    "#                     x = int(x)\n",
    "#                     ## check if x is in the prediction where x = 1st member of user list\n",
    "#                     if pred.count(x) != 0:\n",
    "#                         position = pred.index(x)\n",
    "#                         ndcg += math.log(2) / math.log(position + 2) if position < k else 0\n",
    "#                     else:\n",
    "#                         ndcg += 0\n",
    "        NDCGs.append(ndcg / user_emb.shape[0])\n",
    "        HRs.append(hr / user_emb.shape[0])\n",
    "        NDCGs.append(ndcg / user_emb.shape[0])\n",
    "    return HRs, NDCGs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f204e348",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('preprocessed/ml-1m-3.pickle', 'rb') as f:\n",
    "    dataset = pickle.load(f)\n",
    "    user_size, item_size = dataset['user_size'], dataset['item_size']\n",
    "    train_user_list, test_user_list = dataset['train_user_list'], dataset['test_user_list']\n",
    "    train_pair = dataset['train_pair']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2e7ffdb1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create dataset, model, optimizer\n",
    "dataset = GetTriplePair(item_size, train_user_list, train_pair, True, 1000)\n",
    "\n",
    "# load batch of 512 item triplets\n",
    "loader = DataLoader(dataset, batch_size=512)\n",
    "model = MF(user_size, item_size, 64, 0, 1, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d655d783",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time 0:00:00\n",
      "BPR-MF Epoch [20/1000]\n",
      "loss: 354.9457\n",
      "HR@50: 0.0757, HR@100: 0.1491, NDCG@50: 0.0222, NDCG@100: 0.0222\n",
      "BPR-MF Epoch [40/1000]\n",
      "loss: 355.0152\n",
      "HR@50: 0.0760, HR@100: 0.1485, NDCG@50: 0.0224, NDCG@100: 0.0224\n",
      "BPR-MF Epoch [60/1000]\n",
      "loss: 354.6689\n",
      "HR@50: 0.0755, HR@100: 0.1497, NDCG@50: 0.0219, NDCG@100: 0.0219\n",
      "BPR-MF Epoch [80/1000]\n",
      "loss: 354.6721\n",
      "HR@50: 0.0743, HR@100: 0.1527, NDCG@50: 0.0213, NDCG@100: 0.0213\n",
      "BPR-MF Epoch [100/1000]\n",
      "loss: 354.6452\n",
      "HR@50: 0.0814, HR@100: 0.1614, NDCG@50: 0.0238, NDCG@100: 0.0238\n",
      "time 0:00:12\n",
      "BPR-MF Epoch [120/1000]\n",
      "loss: 354.5093\n",
      "HR@50: 0.0924, HR@100: 0.1793, NDCG@50: 0.0276, NDCG@100: 0.0276\n",
      "BPR-MF Epoch [140/1000]\n",
      "loss: 353.9268\n",
      "HR@50: 0.1147, HR@100: 0.2121, NDCG@50: 0.0352, NDCG@100: 0.0352\n",
      "BPR-MF Epoch [160/1000]\n",
      "loss: 352.2237\n",
      "HR@50: 0.1491, HR@100: 0.2606, NDCG@50: 0.0456, NDCG@100: 0.0456\n",
      "BPR-MF Epoch [180/1000]\n",
      "loss: 349.9401\n",
      "HR@50: 0.1939, HR@100: 0.3187, NDCG@50: 0.0627, NDCG@100: 0.0627\n",
      "BPR-MF Epoch [200/1000]\n",
      "loss: 346.0452\n",
      "HR@50: 0.2385, HR@100: 0.3859, NDCG@50: 0.0757, NDCG@100: 0.0757\n",
      "time 0:00:24\n",
      "BPR-MF Epoch [220/1000]\n",
      "loss: 341.0044\n",
      "HR@50: 0.2821, HR@100: 0.4406, NDCG@50: 0.0894, NDCG@100: 0.0894\n",
      "BPR-MF Epoch [240/1000]\n",
      "loss: 332.6276\n",
      "HR@50: 0.3172, HR@100: 0.4804, NDCG@50: 0.1010, NDCG@100: 0.1010\n",
      "BPR-MF Epoch [260/1000]\n",
      "loss: 326.6565\n",
      "HR@50: 0.3431, HR@100: 0.5082, NDCG@50: 0.1084, NDCG@100: 0.1084\n",
      "BPR-MF Epoch [280/1000]\n",
      "loss: 315.8661\n",
      "HR@50: 0.3629, HR@100: 0.5243, NDCG@50: 0.1150, NDCG@100: 0.1150\n",
      "BPR-MF Epoch [300/1000]\n",
      "loss: 295.9922\n",
      "HR@50: 0.3770, HR@100: 0.5398, NDCG@50: 0.1195, NDCG@100: 0.1195\n",
      "time 0:00:35\n",
      "BPR-MF Epoch [320/1000]\n",
      "loss: 278.6262\n",
      "HR@50: 0.3877, HR@100: 0.5474, NDCG@50: 0.1225, NDCG@100: 0.1225\n",
      "BPR-MF Epoch [340/1000]\n",
      "loss: 262.9536\n",
      "HR@50: 0.3966, HR@100: 0.5541, NDCG@50: 0.1249, NDCG@100: 0.1249\n",
      "BPR-MF Epoch [360/1000]\n",
      "loss: 244.9704\n",
      "HR@50: 0.4018, HR@100: 0.5592, NDCG@50: 0.1271, NDCG@100: 0.1271\n",
      "BPR-MF Epoch [380/1000]\n",
      "loss: 245.0157\n",
      "HR@50: 0.4069, HR@100: 0.5620, NDCG@50: 0.1286, NDCG@100: 0.1286\n",
      "BPR-MF Epoch [400/1000]\n",
      "loss: 230.9523\n",
      "HR@50: 0.4113, HR@100: 0.5639, NDCG@50: 0.1307, NDCG@100: 0.1307\n",
      "time 0:00:47\n",
      "BPR-MF Epoch [420/1000]\n",
      "loss: 219.8795\n",
      "HR@50: 0.4146, HR@100: 0.5657, NDCG@50: 0.1319, NDCG@100: 0.1319\n",
      "BPR-MF Epoch [440/1000]\n",
      "loss: 203.9865\n",
      "HR@50: 0.4175, HR@100: 0.5679, NDCG@50: 0.1326, NDCG@100: 0.1326\n",
      "BPR-MF Epoch [460/1000]\n",
      "loss: 196.5296\n",
      "HR@50: 0.4206, HR@100: 0.5682, NDCG@50: 0.1344, NDCG@100: 0.1344\n",
      "BPR-MF Epoch [480/1000]\n",
      "loss: 194.8434\n",
      "HR@50: 0.4236, HR@100: 0.5679, NDCG@50: 0.1356, NDCG@100: 0.1356\n",
      "BPR-MF Epoch [500/1000]\n",
      "loss: 174.4084\n",
      "HR@50: 0.4260, HR@100: 0.5692, NDCG@50: 0.1368, NDCG@100: 0.1368\n",
      "time 0:00:57\n",
      "BPR-MF Epoch [520/1000]\n",
      "loss: 183.4557\n",
      "HR@50: 0.4295, HR@100: 0.5715, NDCG@50: 0.1376, NDCG@100: 0.1376\n",
      "BPR-MF Epoch [540/1000]\n",
      "loss: 197.7103\n",
      "HR@50: 0.4322, HR@100: 0.5742, NDCG@50: 0.1390, NDCG@100: 0.1390\n",
      "BPR-MF Epoch [560/1000]\n",
      "loss: 184.7457\n",
      "HR@50: 0.4337, HR@100: 0.5765, NDCG@50: 0.1397, NDCG@100: 0.1397\n",
      "BPR-MF Epoch [580/1000]\n",
      "loss: 188.0937\n",
      "HR@50: 0.4344, HR@100: 0.5774, NDCG@50: 0.1403, NDCG@100: 0.1403\n",
      "BPR-MF Epoch [600/1000]\n",
      "loss: 153.7436\n",
      "HR@50: 0.4366, HR@100: 0.5790, NDCG@50: 0.1407, NDCG@100: 0.1407\n",
      "time 0:01:08\n",
      "BPR-MF Epoch [620/1000]\n",
      "loss: 197.8172\n",
      "HR@50: 0.4374, HR@100: 0.5804, NDCG@50: 0.1409, NDCG@100: 0.1409\n",
      "BPR-MF Epoch [640/1000]\n",
      "loss: 203.3823\n",
      "HR@50: 0.4386, HR@100: 0.5826, NDCG@50: 0.1415, NDCG@100: 0.1415\n",
      "BPR-MF Epoch [660/1000]\n",
      "loss: 158.5993\n",
      "HR@50: 0.4410, HR@100: 0.5833, NDCG@50: 0.1420, NDCG@100: 0.1420\n",
      "BPR-MF Epoch [680/1000]\n",
      "loss: 172.8505\n",
      "HR@50: 0.4427, HR@100: 0.5848, NDCG@50: 0.1427, NDCG@100: 0.1427\n",
      "BPR-MF Epoch [700/1000]\n",
      "loss: 190.6320\n",
      "HR@50: 0.4450, HR@100: 0.5863, NDCG@50: 0.1435, NDCG@100: 0.1435\n",
      "BPR-MF Epoch [720/1000]\n",
      "loss: 207.2188\n",
      "HR@50: 0.4454, HR@100: 0.5889, NDCG@50: 0.1442, NDCG@100: 0.1442\n",
      "BPR-MF Epoch [740/1000]\n",
      "loss: 168.0965\n",
      "HR@50: 0.4461, HR@100: 0.5911, NDCG@50: 0.1445, NDCG@100: 0.1445\n",
      "BPR-MF Epoch [760/1000]\n",
      "loss: 172.2678\n",
      "HR@50: 0.4477, HR@100: 0.5930, NDCG@50: 0.1449, NDCG@100: 0.1449\n",
      "BPR-MF Epoch [780/1000]\n",
      "loss: 165.0264\n",
      "HR@50: 0.4484, HR@100: 0.5956, NDCG@50: 0.1449, NDCG@100: 0.1449\n",
      "BPR-MF Epoch [800/1000]\n",
      "loss: 157.0392\n",
      "HR@50: 0.4494, HR@100: 0.5971, NDCG@50: 0.1455, NDCG@100: 0.1455\n",
      "time 0:01:30\n",
      "BPR-MF Epoch [820/1000]\n",
      "loss: 174.6257\n",
      "HR@50: 0.4513, HR@100: 0.5977, NDCG@50: 0.1463, NDCG@100: 0.1463\n",
      "BPR-MF Epoch [840/1000]\n",
      "loss: 151.9054\n",
      "HR@50: 0.4538, HR@100: 0.5986, NDCG@50: 0.1469, NDCG@100: 0.1469\n",
      "BPR-MF Epoch [860/1000]\n",
      "loss: 192.1342\n",
      "HR@50: 0.4542, HR@100: 0.6005, NDCG@50: 0.1470, NDCG@100: 0.1470\n",
      "BPR-MF Epoch [880/1000]\n",
      "loss: 168.3932\n",
      "HR@50: 0.4566, HR@100: 0.6024, NDCG@50: 0.1474, NDCG@100: 0.1474\n",
      "BPR-MF Epoch [900/1000]\n",
      "loss: 179.3589\n",
      "HR@50: 0.4595, HR@100: 0.6036, NDCG@50: 0.1483, NDCG@100: 0.1483\n",
      "BPR-MF Epoch [920/1000]\n",
      "loss: 144.7661\n",
      "HR@50: 0.4608, HR@100: 0.6057, NDCG@50: 0.1493, NDCG@100: 0.1493\n",
      "BPR-MF Epoch [940/1000]\n",
      "loss: 164.5104\n",
      "HR@50: 0.4633, HR@100: 0.6060, NDCG@50: 0.1499, NDCG@100: 0.1499\n",
      "BPR-MF Epoch [960/1000]\n",
      "loss: 157.7827\n",
      "HR@50: 0.4661, HR@100: 0.6088, NDCG@50: 0.1508, NDCG@100: 0.1508\n",
      "BPR-MF Epoch [980/1000]\n",
      "loss: 157.3350\n",
      "HR@50: 0.4681, HR@100: 0.6093, NDCG@50: 0.1519, NDCG@100: 0.1519\n",
      "BPR-MF Epoch [1000/1000]\n",
      "loss: 155.6479\n",
      "HR@50: 0.4710, HR@100: 0.6108, NDCG@50: 0.1527, NDCG@100: 0.1527\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=0.00025)\n",
    "\n",
    "# Training\n",
    "start_time = time.time()\n",
    "eval_best_loss = float('inf')\n",
    "\n",
    "##zero_grad: zeroes the grad attribute of all the parameters passed to the optimizer upon construction\n",
    "optimizer.zero_grad()\n",
    "epoch = 0\n",
    "HR_history = []\n",
    "NDCG_history = []\n",
    "# result_history = []\n",
    "#loader has batch size of 512. In each batch there are 3 tensors of u i j accordingly\n",
    "for u, i, j in loader:\n",
    "    if epoch in range(1000):\n",
    "        loss = model(u, i, j, epoch)\n",
    "\n",
    "        ##  updates the value of those parameters according to the optimization strategy implemented by the specific optimizer.\n",
    "        optimizer.step()\n",
    "        HR_list, NDCG_list = evaluate_k(model.W.detach(),\n",
    "                                        model.H.detach(),\n",
    "                                        train_user_list,\n",
    "                                        test_user_list,\n",
    "                                        klist=[50, 100])\n",
    "        if epoch % 20 == (20- 1):\n",
    "            if epoch in range(1000):\n",
    "                print('BPR-MF Epoch [{}/{}]'.format(epoch + 1, 1000))\n",
    "            print('loss: %.4f' % loss)\n",
    "            print('HR@50: %.4f, HR@100: %.4f, NDCG@50: %.4f, NDCG@100: %.4f' % (\n",
    "                HR_list[0], HR_list[1], NDCG_list[0], NDCG_list[1]))\n",
    "        HR_history.append(HR_list[1])\n",
    "        NDCG_history.append(NDCG_list[1])\n",
    "        if epoch % 100 == 0:\n",
    "            if loss < eval_best_loss:\n",
    "                eval_best_loss = loss\n",
    "                dirname = os.path.dirname(os.path.abspath('output/bpr_manual'))\n",
    "                os.makedirs(dirname, exist_ok=True)\n",
    "                torch.save(model.state_dict(), 'output/bpr_manual')\n",
    "                time_dif = get_time_dif(start_time)\n",
    "                print(\"time\", time_dif)\n",
    "        epoch += 1\n",
    "    else:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b039d9",
   "metadata": {},
   "source": [
    "for ml-1m-2 data set with continue , epoch 1000 has\n",
    "loss: 160.6638\n",
    "HR@50: 0.3218, HR@100: 0.4558, NDCG@50: 0.0939, NDCG@100: 0.0939\n",
    "\n",
    "\n",
    "\n",
    "for ml-1m-6 data set with continue , epoch 1000 has\n",
    "loss: 160.6638\n",
    "HR@50: 0.3218, HR@100: 0.4558, NDCG@50: 0.0939, NDCG@100: 0.0939\n",
    "\n",
    "for ml-1m-6 data set with continue + for loop for X in test user set , epoch 1000 has\n",
    "loss: 196.0934\n",
    "HR@50: 0.3266, HR@100: 0.4649, NDCG@50: 0.7004, NDCG@100: 0.7004\n",
    "\n",
    "\n",
    "ml-1m normal dataset\n",
    "loss: 150.8645\n",
    "HR@50: 0.1437, HR@100: 0.2245, NDCG@50: 0.0365, NDCG@100: 0.0365\n",
    "\n",
    "test ml-1m normal dataset with for loop for X in test user set\n",
    "loss: 181.7949\n",
    "HR@50: 0.1480, HR@100: 0.2252, NDCG@50: 0.0373, NDCG@100: 0.0373"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "29d7f1e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig_HR = plt.figure(edgecolor='blue')\n",
    "ax1 = fig_HR.add_subplot(111)\n",
    "plt.ylabel('HR@100')\n",
    "plt.xlabel('Epoch')\n",
    "plt.title('ML-1M')\n",
    "ax1.plot(range(len(HR_history)), HR_history, c=np.array([255, 71, 90]) / 255.)\n",
    "plt.show()\n",
    "fig_P = plt.figure(edgecolor='blue')\n",
    "ax1 = fig_P.add_subplot(111)\n",
    "plt.ylabel('NDCG@100')\n",
    "plt.xlabel('Epoch')\n",
    "plt.title('ML-1M')\n",
    "ax1.plot(range(len(NDCG_history)), NDCG_history, c=np.array([255, 71, 90]) / 255.)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "886dcf79",
   "metadata": {},
   "source": [
    "Running a saving output of BPR model\n",
    "\n",
    "Next up = train adversary based on fixed parameters of BPR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b6ae75b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_pickle(r'ml1m-2/training_df.pkl')\n",
    "vali_df = pd.read_pickle(r'ml1m-2/valiing_df.pkl')\n",
    "key_genre = pd.read_pickle(r'ml1m-2/key_genre.pkl')  \n",
    "item_idd_genre_list = pd.read_pickle(r'ml1m-2/item_idd_genre_list.pkl')   \n",
    "genre_item_vector = pd.read_pickle(r'ml1m-2/genre_item_vector.pkl')    \n",
    "genre_count = pd.read_pickle(r'ml1m-2/genre_count.pkl')      \n",
    "user_genre_count = pd.read_pickle(r'ml1m-2/user_genre_count.pkl') \n",
    "\n",
    "num_item = len(train_df['item_id'].unique())\n",
    "num_user = len(train_df['user_id'].unique())\n",
    "num_genre = len(key_genre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a9335ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Horror': 330, 'Sci-Fi': 271}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "genre_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "364be573",
   "metadata": {},
   "outputs": [],
   "source": [
    "item_genre_list = []\n",
    "for u in range(num_item):\n",
    "    gl = item_idd_genre_list[u]\n",
    "    tmp = []\n",
    "    for g in gl:\n",
    "        if g in key_genre:\n",
    "            tmp.append(g)\n",
    "    item_genre_list.append(tmp)\n",
    "\n",
    "item_genre = np.zeros((num_item, num_genre))\n",
    "for i in range(num_item):\n",
    "    gl = item_genre_list[i]\n",
    "    for k in range(num_genre):\n",
    "        if key_genre[k] in gl:\n",
    "            item_genre[i, k] = 1.0\n",
    "\n",
    "genre_count_mean_reciprocal = []\n",
    "\n",
    "##there are six key_genre --> in the training dataset, count the number of movies for each genre\n",
    "#genre_count = dictionary with number of movies for each keygrenre\n",
    "for k in key_genre:\n",
    "    genre_count_mean_reciprocal.append(1.0 / genre_count[k])\n",
    "genre_count_mean_reciprocal = (np.array(genre_count_mean_reciprocal)).reshape((num_genre, 1))\n",
    "genre_error_weight = np.dot(item_genre, genre_count_mean_reciprocal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2326714e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00369004],\n",
       "       [0.0030303 ]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "genre_count_mean_reciprocal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "55f5af0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([543, 2])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item_genre = torch.from_numpy(item_genre).type(torch.float)\n",
    "item_genre.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2000011e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5562, 64]), torch.Size([543, 64]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load the results of BPR\n",
    "model1 = (torch.load('output/bpr_manual'))\n",
    "list(model1.items())[0][1].size(),list(model1.items())[1][1].size()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0ed68cc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2953,  0.2628,  0.4346,  ...,  0.2788, -0.3367, -0.2446],\n",
       "        [ 0.2715,  0.3300,  0.5074,  ...,  0.3383, -0.2073, -0.3275],\n",
       "        [ 0.3154,  0.3356,  0.3136,  ...,  0.4675, -0.2804, -0.4066],\n",
       "        ...,\n",
       "        [-0.2611, -0.2157, -0.3304,  ..., -0.2819,  0.4425,  0.3154],\n",
       "        [-0.3096, -0.3436, -0.3519,  ..., -0.2690,  0.2925,  0.3288],\n",
       "        [-0.3899,  0.2384, -0.3255,  ..., -0.3058,  0.2328,  0.3160]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model1.items())[1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0c24762f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5562, 543])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Rec = np.matmul(list(model1.items())[0][1], list(model1.items())[1][1].T)\n",
    "Rec.size()\n",
    "# Rec[1,:].size()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "6d8d492c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "####################################################################################################\n",
      "# System-level Recall:\n",
      "# \t\t\tRecall@1\tRecall@5\tRecall@10\tRecall@15\n",
      "# Sci-Fi\t\t0.01387\t\t0.07008\t\t0.13872\t\t0.20853\n",
      "# Horror\t\t0.00013\t\t0.00027\t\t0.00067\t\t0.00160\n",
      "# relative std\t\t0.98093\t\t0.99241\t\t0.99042\t\t0.98475\n",
      "####################################################################################################\n",
      "# User-level Recall:\n",
      "# \t\t\tRecall@1\tRecall@5\tRecall@10\tRecall@15\n",
      "# Sci-Fi\t\t0.01711\t\t0.08543\t\t0.17070\t\t0.25925\n",
      "# Horror\t\t0.00019\t\t0.00038\t\t0.00099\t\t0.00234\n",
      "# relative std\t\t0.97804\t\t0.99115\t\t0.98849\t\t0.98214\n",
      "####################################################################################################\n",
      "# System-level top ranking probability:\n",
      "# \t\t\t@1\t\t@5\t\t@10\t\t@15\n",
      "# Sci-Fi\t\t0.00357\t\t0.00752\t\t0.00752\t\t0.00752\n",
      "# Horror\t\t0.00044\t\t0.00047\t\t0.00047\t\t0.00047\n",
      "# relative std\t\t0.78056\t\t0.88257\t\t0.88257\t\t0.88257\n",
      "####################################################################################################\n",
      "# User-level top ranking probability:\n",
      "# \t\t\t@1\t\t@5\t\t@10\t\t@15\n",
      "# Sci-Fi\t\t0.00361\t\t0.00761\t\t0.00761\t\t0.00761\n",
      "# Horror\t\t0.00045\t\t0.00048\t\t0.00048\t\t0.00048\n",
      "# relative std\t\t0.77751\t\t0.88108\t\t0.88108\t\t0.88108\n",
      "####################################################################################################\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([0.78055942, 0.88256852, 0.88256852, 0.88256852]),\n",
       " array([0.98093382, 0.99240851, 0.99042126, 0.98475139]))"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import utility\n",
    "utility.ranking_analysis(Rec, vali_df, train_df, key_genre,\n",
    "                                                      item_genre_list, user_genre_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "90785b71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(434, 109, 434, 109)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(Rec.T, \n",
    "                                                    item_genre, \n",
    "                                                    test_size=0.2, # 20% test, 80% train\n",
    "                                                    random_state=181) # make the random split reproducible\n",
    "\n",
    "len(X_train), len(X_test), len(y_train), len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "494ea526",
   "metadata": {},
   "outputs": [],
   "source": [
    "#adv, linear-relu, linear-sigmoid last layer, numlayer = 4, 512,256,128,64 hidden units \n",
    "\n",
    "adversary = nn.Sequential(\n",
    "    nn.Linear(list(model1.items())[0][1].size()[0], 512),\n",
    "#     nn.ReLU(),\n",
    "    nn.Linear(512, 256),\n",
    "#     nn.ReLU(),\n",
    "    nn.Linear(256, 128),\n",
    "#     nn.ReLU(),\n",
    "    nn.Linear(128, 2),\n",
    "    nn.Sigmoid()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8bf3ee5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a loss function\n",
    "# loss_fn = nn.BCELoss() # BCELoss = no sigmoid built-in\n",
    "loss_fn = nn.BCEWithLogitsLoss() # BCEWithLogitsLoss = sigmoid built-in\n",
    "\n",
    "# Create an optimizer\n",
    "optimizer = torch.optim.Adam(params=adversary.parameters(), \n",
    "                            lr=0.000005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7d1f7bd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "896d5686",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate accuracy (a classification metric)\n",
    "def accuracy_fn(y_true, y_pred):\n",
    "    correct = torch.eq(y_true, y_pred).sum().item() # torch.eq() calculates where two tensors are equal\n",
    "    acc = (correct / (y_true.size()[0]*y_true.size()[1])) * 100 \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "28bda2f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Loss: 0.77921, Accuracy: 39.98% | Test loss: 0.78000, Test acc: 41.74%\n",
      "Epoch: 10 | Loss: 0.76126, Accuracy: 44.82% | Test loss: 0.77477, Test acc: 44.04%\n",
      "Epoch: 20 | Loss: 0.75053, Accuracy: 47.93% | Test loss: 0.77157, Test acc: 45.41%\n",
      "Epoch: 30 | Loss: 0.74132, Accuracy: 50.35% | Test loss: 0.76744, Test acc: 46.79%\n",
      "Epoch: 40 | Loss: 0.73305, Accuracy: 52.65% | Test loss: 0.75986, Test acc: 49.08%\n",
      "Epoch: 50 | Loss: 0.71817, Accuracy: 55.07% | Test loss: 0.73932, Test acc: 52.75%\n",
      "Epoch: 60 | Loss: 0.70437, Accuracy: 57.60% | Test loss: 0.72047, Test acc: 55.96%\n",
      "Epoch: 70 | Loss: 0.68631, Accuracy: 59.45% | Test loss: 0.71377, Test acc: 55.96%\n",
      "Epoch: 80 | Loss: 0.68162, Accuracy: 60.02% | Test loss: 0.70245, Test acc: 57.80%\n",
      "Epoch: 90 | Loss: 0.68075, Accuracy: 60.25% | Test loss: 0.70243, Test acc: 57.80%\n",
      "Epoch: 100 | Loss: 0.68075, Accuracy: 60.25% | Test loss: 0.70241, Test acc: 57.80%\n",
      "Epoch: 110 | Loss: 0.67812, Accuracy: 60.94% | Test loss: 0.69893, Test acc: 58.72%\n",
      "Epoch: 120 | Loss: 0.67545, Accuracy: 61.87% | Test loss: 0.70225, Test acc: 58.72%\n",
      "Epoch: 130 | Loss: 0.66764, Accuracy: 63.94% | Test loss: 0.69527, Test acc: 60.55%\n",
      "Epoch: 140 | Loss: 0.65955, Accuracy: 66.13% | Test loss: 0.69768, Test acc: 60.55%\n",
      "Epoch: 150 | Loss: 0.65196, Accuracy: 68.09% | Test loss: 0.68527, Test acc: 63.76%\n",
      "Epoch: 160 | Loss: 0.65101, Accuracy: 69.12% | Test loss: 0.68684, Test acc: 64.22%\n",
      "Epoch: 170 | Loss: 0.65223, Accuracy: 69.35% | Test loss: 0.67815, Test acc: 66.51%\n",
      "Epoch: 180 | Loss: 0.64678, Accuracy: 70.51% | Test loss: 0.67992, Test acc: 66.06%\n",
      "Epoch: 190 | Loss: 0.63633, Accuracy: 72.24% | Test loss: 0.66569, Test acc: 68.35%\n",
      "Epoch: 200 | Loss: 0.61466, Accuracy: 75.12% | Test loss: 0.65273, Test acc: 69.72%\n",
      "Epoch: 210 | Loss: 0.60053, Accuracy: 77.07% | Test loss: 0.64700, Test acc: 70.18%\n",
      "Epoch: 220 | Loss: 0.59413, Accuracy: 77.88% | Test loss: 0.63021, Test acc: 72.48%\n",
      "Epoch: 230 | Loss: 0.58600, Accuracy: 79.26% | Test loss: 0.61706, Test acc: 74.77%\n",
      "Epoch: 240 | Loss: 0.57862, Accuracy: 80.65% | Test loss: 0.61991, Test acc: 74.31%\n",
      "Epoch: 250 | Loss: 0.57321, Accuracy: 81.57% | Test loss: 0.61140, Test acc: 75.69%\n",
      "Epoch: 260 | Loss: 0.57250, Accuracy: 81.68% | Test loss: 0.61137, Test acc: 75.69%\n",
      "Epoch: 270 | Loss: 0.57217, Accuracy: 81.68% | Test loss: 0.61130, Test acc: 75.69%\n",
      "Epoch: 280 | Loss: 0.56663, Accuracy: 82.49% | Test loss: 0.59990, Test acc: 77.52%\n",
      "Epoch: 290 | Loss: 0.56337, Accuracy: 83.06% | Test loss: 0.59679, Test acc: 77.98%\n",
      "Epoch: 300 | Loss: 0.56249, Accuracy: 83.29% | Test loss: 0.59626, Test acc: 77.98%\n",
      "Epoch: 310 | Loss: 0.56249, Accuracy: 83.29% | Test loss: 0.59594, Test acc: 77.98%\n",
      "Epoch: 320 | Loss: 0.56249, Accuracy: 83.29% | Test loss: 0.59409, Test acc: 78.44%\n",
      "Epoch: 330 | Loss: 0.56249, Accuracy: 83.29% | Test loss: 0.59405, Test acc: 78.44%\n",
      "Epoch: 340 | Loss: 0.56249, Accuracy: 83.29% | Test loss: 0.59403, Test acc: 78.44%\n",
      "Epoch: 350 | Loss: 0.56249, Accuracy: 83.29% | Test loss: 0.59403, Test acc: 78.44%\n",
      "Epoch: 360 | Loss: 0.56249, Accuracy: 83.29% | Test loss: 0.59403, Test acc: 78.44%\n",
      "Epoch: 370 | Loss: 0.56249, Accuracy: 83.29% | Test loss: 0.59403, Test acc: 78.44%\n",
      "Epoch: 380 | Loss: 0.56249, Accuracy: 83.29% | Test loss: 0.59403, Test acc: 78.44%\n",
      "Epoch: 390 | Loss: 0.56249, Accuracy: 83.29% | Test loss: 0.59403, Test acc: 78.44%\n",
      "Epoch: 400 | Loss: 0.56206, Accuracy: 83.41% | Test loss: 0.59408, Test acc: 78.44%\n",
      "Epoch: 410 | Loss: 0.56206, Accuracy: 83.41% | Test loss: 0.59410, Test acc: 78.44%\n",
      "Epoch: 420 | Loss: 0.56217, Accuracy: 83.53% | Test loss: 0.60087, Test acc: 77.52%\n",
      "Epoch: 430 | Loss: 0.56086, Accuracy: 83.87% | Test loss: 0.59926, Test acc: 78.44%\n",
      "Epoch: 440 | Loss: 0.56086, Accuracy: 83.87% | Test loss: 0.59807, Test acc: 78.90%\n",
      "Epoch: 450 | Loss: 0.56042, Accuracy: 83.99% | Test loss: 0.59807, Test acc: 78.90%\n",
      "Epoch: 460 | Loss: 0.56042, Accuracy: 83.99% | Test loss: 0.59807, Test acc: 78.90%\n",
      "Epoch: 470 | Loss: 0.56042, Accuracy: 83.99% | Test loss: 0.59807, Test acc: 78.90%\n",
      "Epoch: 480 | Loss: 0.56042, Accuracy: 83.99% | Test loss: 0.59807, Test acc: 78.90%\n",
      "Epoch: 490 | Loss: 0.56042, Accuracy: 83.99% | Test loss: 0.59807, Test acc: 78.90%\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(18)\n",
    "\n",
    "# Set the number of epochs\n",
    "epochs = 500\n",
    "\n",
    "# Put data to target device\n",
    "X_train, y_train = X_train.to(device), y_train.to(device)\n",
    "X_test, y_test = X_test.to(device), y_test.to(device)\n",
    "\n",
    "# Build training and evaluation loop\n",
    "for epoch in range(epochs):\n",
    "    ### Training\n",
    "    adversary.train()\n",
    "\n",
    "    # 1. Forward pass (model outputs raw logits)\n",
    "    y_logits = adversary(X_train).squeeze() # squeeze to remove extra `1` dimensions, this won't work unless model and data are on same device \n",
    "    y_pred = torch.round((y_logits)) # turn logits -> pred probs -> pred labls\n",
    "  \n",
    "    # 2. Calculate loss/accuracy\n",
    "    # loss = loss_fn(torch.sigmoid(y_logits), # Using nn.BCELoss you need torch.sigmoid()\n",
    "    #                y_train) \n",
    "    loss = loss_fn(y_logits, # Using nn.BCEWithLogitsLoss works with raw logits\n",
    "                   y_train) \n",
    "    acc = accuracy_fn(y_true=y_train, \n",
    "                      y_pred=y_pred) \n",
    "\n",
    "    # 3. Optimizer zero grad\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # 4. Loss backwards\n",
    "    loss.backward()\n",
    "\n",
    "    # 5. Optimizer step\n",
    "    optimizer.step()\n",
    "\n",
    "    ### Testing\n",
    "    adversary.eval()\n",
    "    with torch.inference_mode():\n",
    "        # 1. Forward pass\n",
    "        test_logits = adversary(X_test).squeeze() \n",
    "        test_pred = torch.round((test_logits))\n",
    "        # 2. Caculate loss/accuracy\n",
    "        test_loss = loss_fn(test_logits,\n",
    "                            y_test)\n",
    "        test_acc = accuracy_fn(y_true=y_test,\n",
    "                               y_pred=test_pred)\n",
    "\n",
    "    # Print out what's happening every 10 epochs\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch: {epoch} | Loss: {loss:.5f}, Accuracy: {acc:.2f}% | Test loss: {test_loss:.5f}, Test acc: {test_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97014932",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # prepare model and training parameters\n",
    "# n_epochs = 500\n",
    "# batch_size = 50\n",
    "# batches_per_epoch = len(X_train) // batch_size\n",
    " \n",
    "# best_acc = - np.inf   # init to negative infinity\n",
    "# best_weights = None\n",
    "# train_loss_hist = []\n",
    "# train_acc_hist = []\n",
    "# test_loss_hist = []\n",
    "# test_acc_hist = []\n",
    " \n",
    "# # training loop\n",
    "# for epoch in range(n_epochs):\n",
    "#     epoch_loss = []\n",
    "#     epoch_acc = []\n",
    "#     # set model in training mode and run through each batch\n",
    "#     adversary.train()\n",
    "#     with tqdm.trange(batches_per_epoch, unit=\"batch\", mininterval=0) as bar:\n",
    "#         bar.set_description(f\"Epoch {epoch}\")\n",
    "#         for i in bar:\n",
    "#             # take a batch\n",
    "#             start = i * batch_size\n",
    "#             X_batch = X_train[start:start+batch_size]\n",
    "#             y_batch = y_train[start:start+batch_size]\n",
    "#             # forward pass\n",
    "#             y_pred = adversary(X_batch)\n",
    "#             loss = loss_fn(y_pred, y_batch)\n",
    "#             # backward pass\n",
    "#             optimizer.zero_grad()\n",
    "#             loss.backward()\n",
    "#             # update weights\n",
    "#             optimizer.step()\n",
    "#             # compute and store metrics\n",
    "#             acc = accuracy_fn(y_true=y_batch, \n",
    "#                       y_pred=y_pred)\n",
    "#             epoch_loss.append(float(loss))\n",
    "#             epoch_acc.append(float(acc))\n",
    "#             bar.set_postfix(\n",
    "#                 loss=float(loss),\n",
    "#                 acc=float(acc)\n",
    "#             )\n",
    "#     # set model in evaluation mode and run through the test set\n",
    "#     adversary.eval()\n",
    "#     y_pred = adversary(X_test)\n",
    "#     ce = loss_fn(y_pred, y_test)\n",
    "#     acc = accuracy_fn(y_true=y_test, \n",
    "#                       y_pred=y_pred)\n",
    "#     ce = float(ce)\n",
    "#     acc = float(acc)\n",
    "#     train_loss_hist.append(np.mean(epoch_loss))\n",
    "#     train_acc_hist.append(np.mean(epoch_acc))\n",
    "#     test_loss_hist.append(ce)\n",
    "#     test_acc_hist.append(acc)\n",
    "#     if acc > best_acc:\n",
    "#         best_acc = acc\n",
    "#         best_weights = copy.deepcopy(adversary.state_dict())\n",
    "#     print(f\"Epoch {epoch} validation: Cross-entropy={ce:.2f}, Accuracy={acc:.1f}%\")\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4a737340",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'adversary' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4850/2717534567.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Restore best model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0madversary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Plot the loss and accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loss_hist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'adversary' is not defined"
     ]
    }
   ],
   "source": [
    " \n",
    "# Restore best model\n",
    "adversary.load_state_dict(best_weights)\n",
    " \n",
    "# Plot the loss and accuracy\n",
    "plt.plot(train_loss_hist, label=\"train\")\n",
    "plt.plot(test_loss_hist, label=\"test\")\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"cross entropy\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    " \n",
    "plt.plot(train_acc_hist, label=\"train\")\n",
    "plt.plot(test_acc_hist, label=\"test\")\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f758b8a9",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'adversary' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4850/589744199.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdirname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'output/adversary'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexist_ok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madversary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'output/adversary'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'adversary' is not defined"
     ]
    }
   ],
   "source": [
    "dirname = os.path.dirname(os.path.abspath('output/adversary'))\n",
    "os.makedirs(dirname, exist_ok=True)\n",
    "torch.save(adversary, 'output/adversary')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c04f7d0",
   "metadata": {},
   "source": [
    "Take it as it is for now, now save the parameters trained for BPR and adversary\n",
    "\n",
    "Then train for an universal perturbation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "00f95259",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Load\n",
    "adversary = torch.load('output/adversary')\n",
    "# model(X_train)\n",
    "uniform_dist = torch.Tensor([0.5, 0.5])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "70bd1698",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is for indirectly optimize the pertubation\n",
    "\n",
    "criteria = torch.nn.MSELoss()\n",
    "transform_func = torch.nn.Linear(64, 64)#.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "319e9fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class fairness_reprogramming(nn.Module):\n",
    "    def __init__(self, user_emb, item_emb, Rec, reg):\n",
    "        super().__init__()\n",
    "        ##init the embedding for U and I\n",
    "        self.user_emb = user_emb\n",
    "        self.item_emb = item_emb  \n",
    "        self.reg = reg\n",
    "        self.perturb = nn.Parameter(torch.empty(1, 64))  # User embedding\n",
    "        nn.init.xavier_normal_(self.perturb.data)\n",
    "\n",
    "        \n",
    "## forward cal\n",
    "    def forward(self, u, i, j, epoch):\n",
    "        \"\"\"Return loss value.\n",
    "        \n",
    "        Args:\n",
    "            u(torch.LongTensor): tensor stored user indexes. [batch_size,]\n",
    "            i(torch.LongTensor): tensor stored item indexes which is prefered by user. [batch_size,]\n",
    "            j(torch.LongTensor): tensor stored item indexes which is not prefered by user. [batch_size,]\n",
    "            epoch\n",
    "\n",
    "        Returns:\n",
    "            torch.FloatTensor\n",
    "        \"\"\"\n",
    "        ##u,i,j respectively, each is a vector of dim embedding (default = 64)\n",
    "#         u = self.user_emb[u, :]\n",
    "#         i = self.item_emb[i, :]\n",
    "#         j = self.item_emb[j, :]\n",
    "\n",
    "        ## Enables this Tensor to have their grad populated during backward(), convert any non-leaf tensor into a leaf tensor,\n",
    "        ##https://stackoverflow.com/questions/73698041/how-retain-grad-in-pytorch-works-i-found-its-position-changes-the-grad-result\n",
    "#         self.perturb.retain_grad()\n",
    "\n",
    "#       transform perturbation\n",
    "        perturb = transform_func(self.perturb)\n",
    "    \n",
    "        transformation_loss = criteria(self.perturb,perturb)\n",
    "        ## mf, dot product of user with pos/neg item\n",
    "        x_ui = torch.mul(self.user_emb[u, :] , self.item_emb[i, :] + perturb).sum(dim=1)\n",
    "        x_uj = torch.mul(self.user_emb[u, :] , self.item_emb[j, :] + perturb).sum(dim=1)\n",
    "        \n",
    "\n",
    "        #extract prediction for item and genres \n",
    "        \n",
    "        \n",
    "        ## Fix here, adversary needs to predict the recommendation for embedding + perturbation\n",
    "        \n",
    "        #torch.matmul(list(model1.items())[0][1], list(model1.items())[1][1][i,:].T)\n",
    "        #torch.matmul(self.user_emb,(self.item_emb[i, :] + perturb).T).T[i,:]\n",
    "#         i_feature = torch.matmul(self.user_emb,(self.item_emb[i, :] + perturb).T).T[i,:]\n",
    "#         j_feature = torch.matmul(self.user_emb,(self.item_emb[j, :] + perturb).T).T[j,:]\n",
    "        i_feature = Rec.T[i,:]\n",
    "        j_feature = Rec.T[j,:]\n",
    "        \n",
    "#         i_genre = item_genre[i,:]\n",
    "#         j_genre = item_genre[j,:]\n",
    "        \n",
    "        \n",
    "        i.prob = adversary(i_feature).mean(axis = 0)\n",
    "        j.prob = adversary(j_feature).mean(axis = 0)\n",
    "        #similar to clip value, find diff between ui and uj\n",
    "        x_uij =torch.clamp(x_ui - x_uj,min=-80.0,max=1e8)\n",
    "        #logsigmoid this is equivalent to equation 1 in the paper (classic loss of bpr)\n",
    "        log_prob = F.logsigmoid(x_uij).sum()\n",
    "        # regularization = lambda * l2 norm of u, i, j\n",
    "        regularization = self.reg * (self.user_emb[u, :].norm(dim=1).pow(2).sum() + self.item_emb[i, :].norm(dim=1).pow(2).sum() + self.item_emb[j, :].norm(dim=1).pow(2).sum())\n",
    "\n",
    "        ## original bpr loss,\n",
    "        loss_bpr = -log_prob + regularization\n",
    "#         loss_bpr.backward(retain_graph=True)\n",
    "        \n",
    "        \n",
    "        loss_adv = F.kl_div(uniform_dist.log(), i.prob, None, None, 'sum') + F.kl_div(uniform_dist.log(), j.prob , None, None, 'sum')\n",
    "        total_loss = transformation_loss + loss_bpr + loss_adv\n",
    "\n",
    "        total_loss.backward()\n",
    "        \n",
    "        \n",
    "        return total_loss, perturb\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c95449f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4a9ed903",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('preprocessed/ml-1m-3.pickle', 'rb') as f:\n",
    "    dataset = pickle.load(f)\n",
    "    user_size, item_size = dataset['user_size'], dataset['item_size']\n",
    "    train_user_list, test_user_list = dataset['train_user_list'], dataset['test_user_list']\n",
    "    train_pair = dataset['train_pair']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3d1f537f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset, model, optimizer\n",
    "dataset = GetTriplePair(item_size, train_user_list, train_pair, True, 200)\n",
    "#list(model1.items())[0][1], list(model1.items())[1][1]\n",
    "# load batch of 512 item triplets\n",
    "loader = DataLoader(dataset, batch_size=512)\n",
    "model = fairness_reprogramming(list(model1.items())[0][1], list(model1.items())[1][1], Rec, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f82a9fa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "perturb tensor([[-0.1057, -0.0426, -0.1498, -0.2298, -0.2873, -0.1394,  0.2082, -0.1290,\n",
      "         -0.2557, -0.2041,  0.1011, -0.2323,  0.0182,  0.0304, -0.0793,  0.1344,\n",
      "         -0.1661, -0.0277, -0.0182, -0.0847,  0.0871, -0.3417,  0.0399, -0.0030,\n",
      "          0.0118,  0.2464,  0.2156,  0.0834,  0.2384,  0.0470, -0.2770, -0.0834,\n",
      "          0.0678, -0.1201,  0.2447, -0.2136,  0.0393, -0.3610,  0.1519,  0.0332,\n",
      "         -0.1477,  0.1458,  0.0785,  0.2009,  0.0619, -0.1453, -0.1305,  0.1267,\n",
      "         -0.0933,  0.0496, -0.0666, -0.0993, -0.0904, -0.1346,  0.3599,  0.3150,\n",
      "         -0.1978, -0.0393, -0.1128, -0.2536, -0.0702, -0.3339,  0.0850, -0.0178]]) None\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print (name, param.data, param.data.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "df76aaed",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 542 is out of bounds for dimension 0 with size 512",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [29], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m u, i, j \u001b[38;5;129;01min\u001b[39;00m loader:\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m200\u001b[39m):\n\u001b[0;32m---> 17\u001b[0m         loss,perturb \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mu\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m         \u001b[38;5;66;03m##  updates the value of those parameters according to the optimization strategy implemented by the specific optimizer.\u001b[39;00m\n\u001b[1;32m     20\u001b[0m         optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn [25], line 51\u001b[0m, in \u001b[0;36mfairness_reprogramming.forward\u001b[0;34m(self, u, i, j, epoch)\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[38;5;66;03m#extract prediction for item and genres \u001b[39;00m\n\u001b[1;32m     44\u001b[0m         \n\u001b[1;32m     45\u001b[0m         \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[38;5;66;03m#torch.matmul(list(model1.items())[0][1], list(model1.items())[1][1][i,:].T)\u001b[39;00m\n\u001b[1;32m     49\u001b[0m         \u001b[38;5;66;03m#torch.matmul(self.user_emb,(self.item_emb[i, :] + perturb).T).T[i,:]\u001b[39;00m\n\u001b[1;32m     50\u001b[0m         i_feature \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmatmul(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muser_emb,(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitem_emb[i, :] \u001b[38;5;241m+\u001b[39m perturb)\u001b[38;5;241m.\u001b[39mT)\u001b[38;5;241m.\u001b[39mT[i,:]\n\u001b[0;32m---> 51\u001b[0m         j_feature \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muser_emb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem_emb\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mperturb\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m#         i_genre = item_genre[i,:]\u001b[39;00m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m#         j_genre = item_genre[j,:]\u001b[39;00m\n\u001b[1;32m     58\u001b[0m         i\u001b[38;5;241m.\u001b[39mprob \u001b[38;5;241m=\u001b[39m adversary(i_feature)\u001b[38;5;241m.\u001b[39mmean(axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[0;31mIndexError\u001b[0m: index 542 is out of bounds for dimension 0 with size 512"
     ]
    }
   ],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=0.000025)\n",
    "\n",
    "# Training\n",
    "start_time = time.time()\n",
    "eval_best_loss = float('inf')\n",
    "\n",
    "##zero_grad: zeroes the grad attribute of all the parameters passed to the optimizer upon construction\n",
    "optimizer.zero_grad()\n",
    "epoch = 0\n",
    "HR_history = []\n",
    "NDCG_history = []\n",
    "perturb_list = []\n",
    "# result_history = []\n",
    "#loader has batch size of 512. In each batch there are 3 tensors of u i j accordingly\n",
    "for u, i, j in loader:\n",
    "    if epoch in range(200):\n",
    "        loss,perturb = model(u, i, j, epoch)\n",
    "\n",
    "        ##  updates the value of those parameters according to the optimization strategy implemented by the specific optimizer.\n",
    "        optimizer.step()\n",
    "        HR_list, NDCG_list = evaluate_k(list(model1.items())[0][1] + perturb,\n",
    "                                        list(model1.items())[1][1] + perturb,\n",
    "                                        train_user_list,\n",
    "                                        test_user_list,\n",
    "                                        klist=[50, 100])\n",
    "        if epoch % 20 == (20- 1):\n",
    "            if epoch in range(1000):\n",
    "                print('BPR-MF Epoch [{}/{}]'.format(epoch + 1, 1000))\n",
    "            print('loss: %.4f' % loss)\n",
    "            print('HR@50: %.4f, HR@100: %.4f, NDCG@50: %.4f, NDCG@100: %.4f' % (\n",
    "                HR_list[0], HR_list[1], NDCG_list[0], NDCG_list[1]))\n",
    "        perturb_list.append(perturb)\n",
    "        HR_history.append(HR_list[1])\n",
    "        NDCG_history.append(NDCG_list[1])\n",
    "        if epoch % 100 == 0:\n",
    "            if loss < eval_best_loss:\n",
    "                eval_best_loss = loss\n",
    "                dirname = os.path.dirname(os.path.abspath('output/perturbation'))\n",
    "                os.makedirs(dirname, exist_ok=True)\n",
    "                torch.save(model.state_dict(), 'output/perturbation')\n",
    "                time_dif = get_time_dif(start_time)\n",
    "                print(\"time\", time_dif)\n",
    "        epoch += 1\n",
    "    else:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b31e3f",
   "metadata": {},
   "source": [
    "https://stackoverflow.com/questions/66572604/optimize-input-instead-of-network-in-pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "614bc549",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 64])"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load the results of BPR\n",
    "pertubation = (torch.load('output/perturbation'))\n",
    "list(pertubation.items())[0][1].size()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "caa01e15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5562, 543])"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load the results of BPR\n",
    "model1 = (torch.load('output/bpr_manual'))\n",
    "list(model1.items())[0][1].size(),list(model1.items())[1][1].size()\n",
    "\n",
    "\n",
    "\n",
    "Rec = np.matmul(list(model1.items())[0][1]+list(pertubation.items())[0][1], (list(model1.items())[1][1]+list(pertubation.items())[0][1]).T)\n",
    "Rec.size()\n",
    "# Rec[1,:].size()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "5d4e7ace",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vuhoang181/Documents/code_base/APR-PyTorch/utility.py:291: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  top15 = (np.array([top15_item_idx_no_train, u_pred[top15_item_idx_no_train]])).T\n",
      "/home/vuhoang181/Documents/code_base/APR-PyTorch/utility.py:291: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  top15 = (np.array([top15_item_idx_no_train, u_pred[top15_item_idx_no_train]])).T\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "####################################################################################################\n",
      "# System-level Recall:\n",
      "# \t\t\tRecall@1\tRecall@5\tRecall@10\tRecall@15\n",
      "# Sci-Fi\t\t0.00743\t\t0.04135\t\t0.08665\t\t0.13359\n",
      "# Horror\t\t0.00027\t\t0.00214\t\t0.00547\t\t0.00828\n",
      "# relative std\t\t0.93061\t\t0.90176\t\t0.88116\t\t0.88330\n",
      "####################################################################################################\n",
      "# User-level Recall:\n",
      "# \t\t\tRecall@1\tRecall@5\tRecall@10\tRecall@15\n",
      "# Sci-Fi\t\t0.00967\t\t0.05499\t\t0.11560\t\t0.17443\n",
      "# Horror\t\t0.00051\t\t0.00464\t\t0.01163\t\t0.01687\n",
      "# relative std\t\t0.90049\t\t0.84452\t\t0.81715\t\t0.82363\n",
      "####################################################################################################\n",
      "# System-level top ranking probability:\n",
      "# \t\t\t@1\t\t@5\t\t@10\t\t@15\n",
      "# Sci-Fi\t\t0.00370\t\t0.00764\t\t0.00764\t\t0.00764\n",
      "# Horror\t\t0.00035\t\t0.00041\t\t0.00041\t\t0.00041\n",
      "# relative std\t\t0.82604\t\t0.89932\t\t0.89932\t\t0.89932\n",
      "####################################################################################################\n",
      "# User-level top ranking probability:\n",
      "# \t\t\t@1\t\t@5\t\t@10\t\t@15\n",
      "# Sci-Fi\t\t0.00374\t\t0.00773\t\t0.00773\t\t0.00773\n",
      "# Horror\t\t0.00036\t\t0.00042\t\t0.00042\t\t0.00042\n",
      "# relative std\t\t0.82259\t\t0.89772\t\t0.89772\t\t0.89772\n",
      "####################################################################################################\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([0.82603744, 0.89931872, 0.89931872, 0.89931872]),\n",
       " array([0.93060609, 0.90175728, 0.88115807, 0.88330188]))"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import utility\n",
    "utility.ranking_analysis(Rec, vali_df, train_df, key_genre,\n",
    "                                                      item_genre_list, user_genre_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6adf9362",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vuhoang/Documents/code_base/APR-PyTorch/utility.py:291: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  top15 = (np.array([top15_item_idx_no_train, u_pred[top15_item_idx_no_train]])).T\n",
      "/Users/vuhoang/Documents/code_base/APR-PyTorch/utility.py:291: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  top15 = (np.array([top15_item_idx_no_train, u_pred[top15_item_idx_no_train]])).T\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "####################################################################################################\n",
      "# System-level Recall:\n",
      "# \t\t\tRecall@1\tRecall@5\tRecall@10\tRecall@15\n",
      "# Sci-Fi\t\t0.01387\t\t0.07008\t\t0.13872\t\t0.20853\n",
      "# Horror\t\t0.00013\t\t0.00027\t\t0.00067\t\t0.00160\n",
      "# relative std\t\t0.98093\t\t0.99241\t\t0.99042\t\t0.98475\n",
      "####################################################################################################\n",
      "# User-level Recall:\n",
      "# \t\t\tRecall@1\tRecall@5\tRecall@10\tRecall@15\n",
      "# Sci-Fi\t\t0.01711\t\t0.08543\t\t0.17070\t\t0.25925\n",
      "# Horror\t\t0.00019\t\t0.00038\t\t0.00099\t\t0.00234\n",
      "# relative std\t\t0.97804\t\t0.99115\t\t0.98849\t\t0.98214\n",
      "####################################################################################################\n",
      "# System-level top ranking probability:\n",
      "# \t\t\t@1\t\t@5\t\t@10\t\t@15\n",
      "# Sci-Fi\t\t0.00357\t\t0.00752\t\t0.00752\t\t0.00752\n",
      "# Horror\t\t0.00044\t\t0.00047\t\t0.00047\t\t0.00047\n",
      "# relative std\t\t0.78056\t\t0.88257\t\t0.88257\t\t0.88257\n",
      "####################################################################################################\n",
      "# User-level top ranking probability:\n",
      "# \t\t\t@1\t\t@5\t\t@10\t\t@15\n",
      "# Sci-Fi\t\t0.00361\t\t0.00761\t\t0.00761\t\t0.00761\n",
      "# Horror\t\t0.00045\t\t0.00048\t\t0.00048\t\t0.00048\n",
      "# relative std\t\t0.77751\t\t0.88108\t\t0.88108\t\t0.88108\n",
      "####################################################################################################\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([0.78055942, 0.88256852, 0.88256852, 0.88256852]),\n",
       " array([0.98093382, 0.99240851, 0.99042126, 0.98475139]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#before training\n",
    "import utility\n",
    "utility.ranking_analysis(Rec, vali_df, train_df, key_genre,\n",
    "                                                      item_genre_list, user_genre_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "def343e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.9876, 3.1480, 2.5682,  ..., 1.8456, 2.9970, 1.9240])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.matmul(list(model1.items())[0][1], list(model1.items())[1][1][43,:].T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3eb4c74",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
