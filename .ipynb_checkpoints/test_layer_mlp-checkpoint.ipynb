{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e61bc341",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.regularizers import l1,l2\n",
    "from keras.models import Sequential,Model\n",
    "from keras.layers import Embedding,Input,Dense,Flatten,Lambda, Multiply, Concatenate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae387058",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(num_users,num_items,mf_dim=10,layers=[10],reg_layers=[0,0,0,0],reg_mf=0):\n",
    "    num_layer=len(layers)\n",
    "\n",
    "    user_input=Input(shape=(1,),dtype='int32')\n",
    "    item_input_pos=Input(shape=(1,),dtype='int32')\n",
    "    item_input_neg = Input(shape=(1,), dtype='int32')\n",
    "\n",
    "    ## in this context each point is projected to have 10 dimensional coordinates?\n",
    "    MF_embedding_user=Embedding(input_dim=num_users,output_dim=mf_dim,embeddings_initializer='random_normal',\n",
    "                                name='mf_user_embedding',embeddings_regularizer=l2(reg_mf),input_length=1)\n",
    "    MF_embedding_item = Embedding(input_dim=num_items, output_dim=mf_dim, embeddings_initializer='random_normal',\n",
    "                                  name='mf_item_embedding',embeddings_regularizer=l2(reg_mf), input_length=1)\n",
    "    MLP_embedding_user=Embedding(input_dim=num_users,output_dim=layers[0],embeddings_initializer='random_normal',\n",
    "                                 name='mlp_user_embedding', embeddings_regularizer=l2(reg_mf),input_length=1)\n",
    "    MLP_embedding_item = Embedding(input_dim=num_items, output_dim=layers[0], embeddings_initializer='random_normal',\n",
    "                                   name='mlp_item_embedding',embeddings_regularizer=l2(reg_mf), input_length=1)\n",
    "\n",
    "    mf_user_latent=Flatten()(MF_embedding_user(user_input))\n",
    "    mf_item_latent_pos=Flatten()(MF_embedding_item(item_input_pos))\n",
    "    mf_item_latent_neg = Flatten()(MF_embedding_item(item_input_neg))\n",
    "\n",
    "    ## merge = deprecated use keras.layers.Concatenate(axis=-1) instead\n",
    "    prefer_pos = merge([mf_user_latent, mf_item_latent_pos], mode='mul')\n",
    "    prefer_neg = merge([mf_user_latent, mf_item_latent_neg], mode='mul')\n",
    "    ## convert negative layer to negative, lambda layers should be re-written as subclass layer if too complex (eg: multiply by scale?)\n",
    "    prefer_neg = Lambda(lambda x: -x)(prefer_neg)\n",
    "    ## basically just merge 2 layer\n",
    "    mf_vector = merge([prefer_pos, prefer_neg], mode='concat')\n",
    "\n",
    "    ## flat matrix to an array\n",
    "    mlp_user_latent=Flatten()(MLP_embedding_user(user_input))\n",
    "    mlp_item_latent_pos=Flatten()(MLP_embedding_item(item_input_pos))\n",
    "    mlp_item_latent_neg=Flatten()(MLP_embedding_item(item_input_neg))\n",
    "    mlp_item_latent_neg=Lambda(lambda x:-x)(mlp_item_latent_neg)\n",
    "    mlp_vector=merge([mlp_user_latent,mlp_item_latent_pos,mlp_item_latent_neg],mode='concat')\n",
    "    for idx in range(1,num_layer):\n",
    "        #set up hidden layer of the network, why tanh and have to regularize?? L1 consider weight, l2 consider square of weight\n",
    "        layer=Dense(layers[idx],kernel_regularizer=l2(0.0000),activation='tanh',name=\"layer%d\" %idx)\n",
    "        mlp_vector=layer(mlp_vector)\n",
    "\n",
    "    ## concatenation of NBPR layer and DNCR layer\n",
    "    predict_vector=merge([mf_vector,mlp_vector],mode='concat')\n",
    "\n",
    "\n",
    "    #set up prediction --> final layer, sigmoid activate to give binary output\n",
    "    prediction=Dense(1,activation='sigmoid',kernel_initializer='lecun_uniform',name='prediction')(predict_vector)\n",
    "    model=Model(inputs=[user_input,item_input_pos,item_input_neg],outputs=prediction)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "73accab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input=Input(shape=(1,),dtype='int32')\n",
    "item_input_pos=Input(shape=(1,),dtype='int32')\n",
    "item_input_neg = Input(shape=(1,), dtype='int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "19e8e28e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mf_dim=10\n",
    "layers=[10]\n",
    "num_layer=len(layers)\n",
    "reg_layers=[0,0,0,0]\n",
    "reg_mf=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9da2ee1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "MF_embedding_user=Embedding(input_dim=2000,output_dim=mf_dim,embeddings_initializer='random_normal',\n",
    "                                name='mf_user_embedding',embeddings_regularizer=l2(reg_mf),input_length=1)\n",
    "MF_embedding_item = Embedding(input_dim=50000, output_dim=mf_dim, embeddings_initializer='random_normal',\n",
    "                                  name='mf_item_embedding',embeddings_regularizer=l2(reg_mf), input_length=1)\n",
    "MLP_embedding_user=Embedding(input_dim=2000,output_dim=layers[0],embeddings_initializer='random_normal',\n",
    "                                 name='mlp_user_embedding', embeddings_regularizer=l2(reg_mf),input_length=1)\n",
    "MLP_embedding_item = Embedding(input_dim=50000, output_dim=layers[0], embeddings_initializer='random_normal',\n",
    "                                   name='mlp_item_embedding',embeddings_regularizer=l2(reg_mf), input_length=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "09788803",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-14 23:27:03.991416: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "mf_user_latent=Flatten()(MF_embedding_user(user_input))\n",
    "mf_item_latent_pos=Flatten()(MF_embedding_item(item_input_pos))\n",
    "mf_item_latent_neg = Flatten()(MF_embedding_item(item_input_neg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0a3d5a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "## merge = deprecated use keras.layers.Concatenate(axis=-1) instead\n",
    "prefer_pos = Multiply()([mf_user_latent, mf_item_latent_pos])\n",
    "prefer_neg = Multiply()([mf_user_latent, mf_item_latent_neg])\n",
    "## convert negative layer to negative, lambda layers should be re-written as subclass layer if too complex (eg: multiply by scale?)\n",
    "prefer_neg = Lambda(lambda x: -x)(prefer_neg)\n",
    "## basically just merge 2 layer\n",
    "mf_vector = Concatenate()([prefer_pos, prefer_neg])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c3f50bd0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'num_layer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [24], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m mlp_item_latent_neg\u001b[38;5;241m=\u001b[39mLambda(\u001b[38;5;28;01mlambda\u001b[39;00m x:\u001b[38;5;241m-\u001b[39mx)(mlp_item_latent_neg)\n\u001b[1;32m      5\u001b[0m mlp_vector\u001b[38;5;241m=\u001b[39mConcatenate()([mlp_user_latent,mlp_item_latent_pos,mlp_item_latent_neg])\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m,\u001b[43mnum_layer\u001b[49m):\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;66;03m#set up hidden layer of the network, why tanh and have to regularize?? L1 consider weight, l2 consider square of weight\u001b[39;00m\n\u001b[1;32m      8\u001b[0m     layer\u001b[38;5;241m=\u001b[39mDense(layers[idx],kernel_regularizer\u001b[38;5;241m=\u001b[39ml2(\u001b[38;5;241m0.0000\u001b[39m),activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtanh\u001b[39m\u001b[38;5;124m'\u001b[39m,name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlayer\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39midx)\n\u001b[1;32m      9\u001b[0m     mlp_vector\u001b[38;5;241m=\u001b[39mlayer(mlp_vector)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'num_layer' is not defined"
     ]
    }
   ],
   "source": [
    "mlp_user_latent=Flatten()(MLP_embedding_user(user_input))\n",
    "mlp_item_latent_pos=Flatten()(MLP_embedding_item(item_input_pos))\n",
    "mlp_item_latent_neg=Flatten()(MLP_embedding_item(item_input_neg))\n",
    "mlp_item_latent_neg=Lambda(lambda x:-x)(mlp_item_latent_neg)\n",
    "mlp_vector=Concatenate()([mlp_user_latent,mlp_item_latent_pos,mlp_item_latent_neg])\n",
    "for idx in range(1,num_layer):\n",
    "    #set up hidden layer of the network, why tanh and have to regularize?? L1 consider weight, l2 consider square of weight\n",
    "    layer=Dense(layers[idx],kernel_regularizer=l2(0.0000),activation='tanh',name=\"layer%d\" %idx)\n",
    "    mlp_vector=layer(mlp_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8fa9df99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse as sp\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b54e3ef1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2000x50000 sparse matrix of type '<class 'numpy.float32'>'\n",
       "\twith 0 stored elements in Dictionary Of Keys format>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat = sp.dok_matrix((2000, 50000), dtype=np.float32)\n",
    "mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1547f45c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 50000)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "88d5f97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layer=len(layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bba1acd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a195cfcd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
