{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6456030",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-15 16:52:07.158290: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-15 16:52:07.258079: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "from keras.regularizers import l1,l2\n",
    "from keras.models import Sequential,Model\n",
    "from keras.layers import Embedding,Input,Dense,Flatten,Lambda, Multiply, Concatenate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c393bbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('W',\n",
       "              tensor([[ 0.1027, -0.2571,  0.2280,  ..., -0.3319,  0.1107, -0.0547],\n",
       "                      [-0.2378,  0.1023, -0.2476,  ..., -0.1183,  0.2373,  0.3497],\n",
       "                      [ 0.1866, -0.2054, -0.2772,  ..., -0.2186,  0.2774,  0.2513],\n",
       "                      ...,\n",
       "                      [ 0.1794, -0.2494,  0.2269,  ..., -0.2170,  0.0641,  0.0161],\n",
       "                      [ 0.2467, -0.1777,  0.1404,  ..., -0.2994, -0.1175, -0.0339],\n",
       "                      [-0.1238,  0.1703, -0.1914,  ...,  0.0497,  0.2560, -0.1212]])),\n",
       "             ('H',\n",
       "              tensor([[-0.1210, -0.0994, -0.2235,  ..., -0.2647, -0.2383, -0.1061],\n",
       "                      [-0.1862,  0.1676, -0.3127,  ..., -0.2307, -0.2465, -0.3151],\n",
       "                      [-0.2041,  0.2428, -0.3161,  ..., -0.2608, -0.1408, -0.2574],\n",
       "                      ...,\n",
       "                      [-0.2339, -0.0100,  0.1009,  ..., -0.0604, -0.1932, -0.1700],\n",
       "                      [ 0.0182,  0.1940, -0.2179,  ..., -0.1539, -0.0744, -0.2069],\n",
       "                      [-0.1560,  0.2314,  0.2187,  ...,  0.2337, -0.1828, -0.1758]]))])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.load('/home/vuhoang181/Documents/code_base/APR-PyTorch/output/bpr.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79641089",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(num_users,num_items,mf_dim=10,layers=[10],reg_layers=[0,0,0,0],reg_mf=0):\n",
    "    num_layer=len(layers)\n",
    "\n",
    "    user_input=Input(shape=(1,),dtype='int32')\n",
    "    item_input_pos=Input(shape=(1,),dtype='int32')\n",
    "    item_input_neg = Input(shape=(1,), dtype='int32')\n",
    "\n",
    "    ## in this context each point is projected to have 10 dimensional coordinates?\n",
    "    MF_embedding_user=Embedding(input_dim=num_users,output_dim=mf_dim,embeddings_initializer='random_normal',\n",
    "                                name='mf_user_embedding',embeddings_regularizer=l2(reg_mf),input_length=1)\n",
    "    MF_embedding_item = Embedding(input_dim=num_items, output_dim=mf_dim, embeddings_initializer='random_normal',\n",
    "                                  name='mf_item_embedding',embeddings_regularizer=l2(reg_mf), input_length=1)\n",
    "    MLP_embedding_user=Embedding(input_dim=num_users,output_dim=layers[0],embeddings_initializer='random_normal',\n",
    "                                 name='mlp_user_embedding', embeddings_regularizer=l2(reg_mf),input_length=1)\n",
    "    MLP_embedding_item = Embedding(input_dim=num_items, output_dim=layers[0], embeddings_initializer='random_normal',\n",
    "                                   name='mlp_item_embedding',embeddings_regularizer=l2(reg_mf), input_length=1)\n",
    "\n",
    "    mf_user_latent=Flatten()(MF_embedding_user(user_input))\n",
    "    mf_item_latent_pos=Flatten()(MF_embedding_item(item_input_pos))\n",
    "    mf_item_latent_neg = Flatten()(MF_embedding_item(item_input_neg))\n",
    "\n",
    "    ## merge = deprecated use keras.layers.Concatenate(axis=-1) instead\n",
    "    prefer_pos = merge([mf_user_latent, mf_item_latent_pos], mode='mul')\n",
    "    prefer_neg = merge([mf_user_latent, mf_item_latent_neg], mode='mul')\n",
    "    ## convert negative layer to negative, lambda layers should be re-written as subclass layer if too complex (eg: multiply by scale?)\n",
    "    prefer_neg = Lambda(lambda x: -x)(prefer_neg)\n",
    "    ## basically just merge 2 layer\n",
    "    mf_vector = merge([prefer_pos, prefer_neg], mode='concat')\n",
    "\n",
    "    ## flat matrix to an array\n",
    "    mlp_user_latent=Flatten()(MLP_embedding_user(user_input))\n",
    "    mlp_item_latent_pos=Flatten()(MLP_embedding_item(item_input_pos))\n",
    "    mlp_item_latent_neg=Flatten()(MLP_embedding_item(item_input_neg))\n",
    "    mlp_item_latent_neg=Lambda(lambda x:-x)(mlp_item_latent_neg)\n",
    "    mlp_vector=merge([mlp_user_latent,mlp_item_latent_pos,mlp_item_latent_neg],mode='concat')\n",
    "    for idx in range(1,num_layer):\n",
    "        #set up hidden layer of the network, why tanh and have to regularize?? L1 consider weight, l2 consider square of weight\n",
    "        layer=Dense(layers[idx],kernel_regularizer=l2(0.0000),activation='tanh',name=\"layer%d\" %idx)\n",
    "        mlp_vector=layer(mlp_vector)\n",
    "\n",
    "    ## concatenation of NBPR layer and DNCR layer\n",
    "    predict_vector=merge([mf_vector,mlp_vector],mode='concat')\n",
    "\n",
    "\n",
    "    #set up prediction --> final layer, sigmoid activate to give binary output\n",
    "    prediction=Dense(1,activation='sigmoid',kernel_initializer='lecun_uniform',name='prediction')(predict_vector)\n",
    "    model=Model(inputs=[user_input,item_input_pos,item_input_neg],outputs=prediction)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "efe4fdc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input=Input(shape=(1,),dtype='int32')\n",
    "item_input_pos=Input(shape=(1,),dtype='int32')\n",
    "item_input_neg = Input(shape=(1,), dtype='int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cdc327c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "mf_dim=10\n",
    "layers=[10]\n",
    "num_layer=len(layers)\n",
    "reg_layers=[0,0,0,0]\n",
    "reg_mf=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2e40dcb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "MF_embedding_user=Embedding(input_dim=2000,output_dim=mf_dim,embeddings_initializer='random_normal',\n",
    "                                name='mf_user_embedding',embeddings_regularizer=l2(reg_mf),input_length=1)\n",
    "MF_embedding_item = Embedding(input_dim=50000, output_dim=mf_dim, embeddings_initializer='random_normal',\n",
    "                                  name='mf_item_embedding',embeddings_regularizer=l2(reg_mf), input_length=1)\n",
    "MLP_embedding_user=Embedding(input_dim=2000,output_dim=layers[0],embeddings_initializer='random_normal',\n",
    "                                 name='mlp_user_embedding', embeddings_regularizer=l2(reg_mf),input_length=1)\n",
    "MLP_embedding_item = Embedding(input_dim=50000, output_dim=layers[0], embeddings_initializer='random_normal',\n",
    "                                   name='mlp_item_embedding',embeddings_regularizer=l2(reg_mf), input_length=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "be3b68ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-14 23:27:03.991416: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "mf_user_latent=Flatten()(MF_embedding_user(user_input))\n",
    "mf_item_latent_pos=Flatten()(MF_embedding_item(item_input_pos))\n",
    "mf_item_latent_neg = Flatten()(MF_embedding_item(item_input_neg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4fdbf484",
   "metadata": {},
   "outputs": [],
   "source": [
    "## merge = deprecated use keras.layers.Concatenate(axis=-1) instead\n",
    "prefer_pos = Multiply()([mf_user_latent, mf_item_latent_pos])\n",
    "prefer_neg = Multiply()([mf_user_latent, mf_item_latent_neg])\n",
    "## convert negative layer to negative, lambda layers should be re-written as subclass layer if too complex (eg: multiply by scale?)\n",
    "prefer_neg = Lambda(lambda x: -x)(prefer_neg)\n",
    "## basically just merge 2 layer\n",
    "mf_vector = Concatenate()([prefer_pos, prefer_neg])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b20f4e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_user_latent=Flatten()(MLP_embedding_user(user_input))\n",
    "mlp_item_latent_pos=Flatten()(MLP_embedding_item(item_input_pos))\n",
    "mlp_item_latent_neg=Flatten()(MLP_embedding_item(item_input_neg))\n",
    "mlp_item_latent_neg=Lambda(lambda x:-x)(mlp_item_latent_neg)\n",
    "mlp_vector=Concatenate()([mlp_user_latent,mlp_item_latent_pos,mlp_item_latent_neg])\n",
    "for idx in range(1,num_layer):\n",
    "    #set up hidden layer of the network, why tanh and have to regularize?? L1 consider weight, l2 consider square of weight\n",
    "    layer=Dense(layers[idx],kernel_regularizer=l2(0.0000),activation='tanh',name=\"layer%d\" %idx)\n",
    "    mlp_vector=layer(mlp_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2f9ce829",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_vector=Concatenate()([mf_vector,mlp_vector])\n",
    "\n",
    "\n",
    "#set up prediction --> final layer, sigmoid activate to give binary output\n",
    "prediction=Dense(3,kernel_initializer='lecun_uniform',name='prediction')(predict_vector)\n",
    "model=Model(inputs=[user_input,item_input_pos,item_input_neg],outputs=prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9886b91e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KerasTensor: shape=(None, 3) dtype=float32 (created by layer 'prediction')>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fe985b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer=Dense(1,kernel_regularizer=l2(0.0000),activation='tanh')\n",
    "mlp_vector=layer(mf_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "62650712",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KerasTensor: shape=(None, 1) dtype=float32 (created by layer 'dense')>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9406f5de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6d221ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layer=len(layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c08cc799",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "134400d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0071, -0.0003, -0.0036,  ..., -0.0109, -0.0021, -0.0093],\n",
       "        [-0.0168,  0.0075,  0.0166,  ...,  0.0210,  0.0088,  0.0050],\n",
       "        [ 0.0069,  0.0208, -0.0190,  ...,  0.0083,  0.0040, -0.0226],\n",
       "        ...,\n",
       "        [ 0.0188, -0.0042,  0.0058,  ..., -0.0114,  0.0023,  0.0108],\n",
       "        [ 0.0068, -0.0273,  0.0137,  ..., -0.0128,  0.0244,  0.0166],\n",
       "        [ 0.0157, -0.0429, -0.0156,  ..., -0.0312, -0.0007,  0.0045]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W = nn.Parameter(torch.empty(2000, 64))  # User embedding\n",
    "H = nn.Parameter(torch.empty(10000, 64))  # Item embedding\n",
    "nn.init.xavier_normal_(W.data)\n",
    "nn.init.xavier_normal_(H.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e9525373",
   "metadata": {},
   "outputs": [],
   "source": [
    "u=W[1,:]\n",
    "i=H[1,:]\n",
    "j=H[2,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "25dfa048",
   "metadata": {},
   "outputs": [],
   "source": [
    "u.retain_grad()\n",
    "i.retain_grad()\n",
    "j.retain_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7e5d7dfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.0072, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mul(u, i).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dd08aba2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0308,  0.0214, -0.0337, -0.0007,  0.0668,  0.0039,  0.0473, -0.0047,\n",
       "        -0.0129, -0.0313, -0.0083,  0.0202,  0.0343, -0.0094, -0.0246,  0.0225,\n",
       "        -0.0211,  0.0477, -0.0036, -0.0222, -0.0022, -0.0409, -0.0026, -0.0114,\n",
       "         0.0423, -0.0686,  0.0195,  0.0083, -0.0347,  0.0004, -0.0054,  0.0049,\n",
       "        -0.0268, -0.0576, -0.0233, -0.0235, -0.0006,  0.0016, -0.0176,  0.0523,\n",
       "        -0.0086,  0.0087,  0.0148,  0.0087,  0.0461, -0.0197,  0.0121, -0.0110,\n",
       "        -0.0299, -0.0086, -0.0063, -0.0063,  0.0055, -0.0220, -0.0304, -0.0149,\n",
       "        -0.0629,  0.0318,  0.0386, -0.0172, -0.0283, -0.0047, -0.0392, -0.0386],\n",
       "       grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "abc4a154",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a87a2b20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.6968, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.logsigmoid(torch.mul(u, i).sum()).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f942c5ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0308,  0.0214, -0.0337, -0.0007,  0.0668,  0.0039,  0.0473, -0.0047,\n",
       "        -0.0129, -0.0313, -0.0083,  0.0202,  0.0343, -0.0094, -0.0246,  0.0225,\n",
       "        -0.0211,  0.0477, -0.0036, -0.0222, -0.0022, -0.0409, -0.0026, -0.0114,\n",
       "         0.0423, -0.0686,  0.0195,  0.0083, -0.0347,  0.0004, -0.0054,  0.0049,\n",
       "        -0.0268, -0.0576, -0.0233, -0.0235, -0.0006,  0.0016, -0.0176,  0.0523,\n",
       "        -0.0086,  0.0087,  0.0148,  0.0087,  0.0461, -0.0197,  0.0121, -0.0110,\n",
       "        -0.0299, -0.0086, -0.0063, -0.0063,  0.0055, -0.0220, -0.0304, -0.0149,\n",
       "        -0.0629,  0.0318,  0.0386, -0.0172, -0.0283, -0.0047, -0.0392, -0.0386],\n",
       "       grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "49fc65b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_vector=torch.stack((u,i,j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1c8c9f5c",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'torch.Size' object has no attribute 'rank'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1161558/4081738527.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mkernel_regularizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0ml2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.0000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'tanh'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'final'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmlp_vector\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/keras/engine/input_spec.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[0;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[1;32m    246\u001b[0m                 )\n\u001b[1;32m    247\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin_ndim\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 248\u001b[0;31m             \u001b[0mndim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrank\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mndim\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mndim\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin_ndim\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m                 raise ValueError(\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'torch.Size' object has no attribute 'rank'"
     ]
    }
   ],
   "source": [
    "Dense(1,kernel_regularizer=l2(0.0000),activation='tanh',name='final')(mlp_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "1a241755",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = torch.nn.Linear(mlp_vector.shape[1], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d3cd2111",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "x = torch.tensor([[1.0, -1.0],\n",
    "                  [0.0,  1.0],\n",
    "                  [0.0,  0.0]])\n",
    "\n",
    "in_features = x.shape[1]  # = 2\n",
    "out_features = 2\n",
    "\n",
    "m = nn.Linear(in_features, out_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "4ae334f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0586],\n",
       "        [-0.0656],\n",
       "        [-0.0755]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m(mlp_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "15d43729",
   "metadata": {},
   "outputs": [],
   "source": [
    "m1 = nn.Linear(64, 32)\n",
    "m2 = nn.Linear(32,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "e23ed1c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0560],\n",
       "        [-0.0568],\n",
       "        [-0.0535]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m2(m1(mlp_vector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "c8ab21ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 1),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "884cd11d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1656],\n",
       "        [0.1633],\n",
       "        [0.1632]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_relu_stack(mlp_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "efc276a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1633], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_relu_stack(mlp_vector)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0b16cf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--data DATA] [--seed SEED] [--dim DIM]\n",
      "                             [--lr LR] [--reg REG] [--epochs EPOCHS]\n",
      "                             [--batch_size BATCH_SIZE] [--verbose VERBOSE]\n",
      "                             [--eval_every EVAL_EVERY] [--model MODEL]\n",
      "                             [--reg_adv REG_ADV] [--adv_epoch ADV_EPOCH]\n",
      "                             [--eps EPS]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f /Users/vuhoang/Library/Jupyter/runtime/kernel-86889cae-b42c-46b5-a26c-f7149c78243c.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vuhoang/miniconda3/lib/python3.9/site-packages/IPython/core/interactiveshell.py:3441: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import os\n",
    "import random\n",
    "import pickle\n",
    "import argparse\n",
    "from collections import deque\n",
    "import time\n",
    "from datetime import timedelta\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import IterableDataset, DataLoader, get_worker_info\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "## time the process\n",
    "def get_time_dif(start_time):\n",
    "    \"\"\"get the running time\"\"\"\n",
    "    end_time = time.time()\n",
    "    time_dif = end_time - start_time\n",
    "    return timedelta(seconds=int(round(time_dif)))\n",
    "\n",
    "\n",
    "## set up the u,i,j triplet for BPR framework\n",
    "class GetTriplePair(IterableDataset):\n",
    "    def __init__(self, item_size, user_list, pair, shuffle, num_epochs):\n",
    "        self.item_size = item_size\n",
    "        self.user_list = user_list\n",
    "        self.pair = pair\n",
    "        self.shuffle = shuffle\n",
    "        self.num_epochs = num_epochs\n",
    "\n",
    "    def __iter__(self):\n",
    "        self.example_size = self.num_epochs * len(self.pair)\n",
    "        self.example_index_queue = deque([])\n",
    "        self.seed = 0\n",
    "        self.start_list_index = None\n",
    "        self.num_workers = 1\n",
    "        self.index = 0\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        if self.index >= self.example_size:\n",
    "            raise StopIteration\n",
    "        # If `example_index_queue` is used up, replenish this list.\n",
    "        while len(self.example_index_queue) == 0:\n",
    "            index_list = list(range(len(self.pair)))\n",
    "            if self.shuffle:\n",
    "                random.Random(self.seed).shuffle(index_list)\n",
    "                self.seed += 1\n",
    "            if self.start_list_index is not None:\n",
    "                index_list = index_list[self.start_list_index::self.num_workers]\n",
    "\n",
    "                # Calculate next start index\n",
    "                self.start_list_index = (self.start_list_index + (self.num_workers - (len(self.pair) % self.num_workers))) % self.num_workers\n",
    "            self.example_index_queue.extend(index_list)\n",
    "        result = self._example(self.example_index_queue.popleft())\n",
    "        self.index += self.num_workers\n",
    "        return result\n",
    "\n",
    "    def _example(self, idx):\n",
    "        u = self.pair[idx][0]\n",
    "        i = self.pair[idx][1]\n",
    "        j = np.random.randint(self.item_size)\n",
    "        while j in self.user_list[u]:\n",
    "            j = np.random.randint(self.item_size)\n",
    "        return u, i, j\n",
    "\n",
    "## chunk to define matrix factorization part\n",
    "class MF(nn.Module):\n",
    "    def __init__(self, user_size, item_size, dim, reg, reg_adv, eps):\n",
    "        super().__init__()\n",
    "        ##init the embedding for U and I\n",
    "        self.W = nn.Parameter(torch.empty(user_size, dim))  # User embedding\n",
    "        self.H = nn.Parameter(torch.empty(item_size, dim))  # Item embedding\n",
    "        nn.init.xavier_normal_(self.W.data)\n",
    "        nn.init.xavier_normal_(self.H.data)\n",
    "        self.reg = reg\n",
    "        self.user_size = user_size\n",
    "        self.item_size = item_size\n",
    "        self.dim = dim\n",
    "        self.reg_adv = reg_adv\n",
    "        self.eps = eps\n",
    "        self.update_u = None\n",
    "        self.update_i = None\n",
    "        self.update_j = None\n",
    "\n",
    "## forward cal\n",
    "    def forward(self, u, i, j, epoch):\n",
    "        \"\"\"Return loss value.\n",
    "\n",
    "        Args:\n",
    "            u(torch.LongTensor): tensor stored user indexes. [batch_size,]\n",
    "            i(torch.LongTensor): tensor stored item indexes which is prefered by user. [batch_size,]\n",
    "            j(torch.LongTensor): tensor stored item indexes which is not prefered by user. [batch_size,]\n",
    "            epoch\n",
    "\n",
    "        Returns:\n",
    "            torch.FloatTensor\n",
    "        \"\"\"\n",
    "        ##u,i,j respectively, each is a vector of dim embedding (default = 64)\n",
    "        u = self.W[u, :]\n",
    "        i = self.H[i, :]\n",
    "        j = self.H[j, :]\n",
    "\n",
    "        ## Enables this Tensor to have their grad populated during backward(), convert any non-leaf tensor into a leaf tensor,\n",
    "        ##https://stackoverflow.com/questions/73698041/how-retain-grad-in-pytorch-works-i-found-its-position-changes-the-grad-result\n",
    "        u.retain_grad()\n",
    "        u_clone = u.data.clone()\n",
    "        i.retain_grad()\n",
    "        i_clone = i.data.clone()\n",
    "        j.retain_grad()\n",
    "        j_clone = j.data.clone()\n",
    "\n",
    "        ## mf, dot product of user with pos/neg item\n",
    "        x_ui = torch.mul(u, i).sum(dim=1)\n",
    "        x_uj = torch.mul(u, j).sum(dim=1)\n",
    "\n",
    "\n",
    "        #similar to clip value, find diff between ui and uj\n",
    "        x_uij =torch.clamp(x_ui - x_uj,min=-80.0,max=1e8)\n",
    "        #logsigmoid this is equivalent to equation 1 in the paper (classic loss of bpr)\n",
    "        log_prob = F.logsigmoid(x_uij).sum()\n",
    "        # regularization = lambda * l2 norm of u, i, j\n",
    "        regularization = self.reg * (u.norm(dim=1).pow(2).sum() + i.norm(dim=1).pow(2).sum() + j.norm(dim=1).pow(2).sum())\n",
    "\n",
    "        ## original bpr loss,\n",
    "        loss = -log_prob + regularization\n",
    "\n",
    "\n",
    "        ## add adv training after a certain number of epochs, here is the part which we add hypernet module\n",
    "        if epoch not in range(args.epochs, args.adv_epoch + args.epochs):\n",
    "            \"\"\"Normal training\"\"\"\n",
    "            loss.backward()\n",
    "            return loss\n",
    "\n",
    "        else:\n",
    "            \"\"\"Adversarial training:\n",
    "                    1.Backward to get grads\n",
    "                    2.Construct adversarial perturbation\n",
    "                    3.Add adversarial perturbation to embeddings\n",
    "                    4.Calculate APR loss\n",
    "            \"\"\"\n",
    "            # Backward to get grads\n",
    "            # this would be the part we change in defining delta, delta = HPN (phi)\n",
    "            loss.backward(retain_graph=True)\n",
    "            grad_u = u.grad\n",
    "            grad_i = i.grad\n",
    "            grad_j = j.grad\n",
    "\n",
    "            # Construct adversarial perturbation based on gradient of loss function, and normalize it with epsilon * norm\n",
    "            if grad_u is not None:\n",
    "                delta_u = nn.functional.normalize(grad_u, p=2, dim=1, eps=self.eps)\n",
    "            else:\n",
    "                delta_u = torch.rand(u.size()) ## why we have to do this if grad is none?\n",
    "            if grad_i is not None:\n",
    "                delta_i = nn.functional.normalize(grad_i, p=2, dim=1, eps=self.eps)\n",
    "            else:\n",
    "                delta_i = torch.rand(i.size())\n",
    "            if grad_j is not None:\n",
    "                delta_j = nn.functional.normalize(grad_j, p=2, dim=1, eps=self.eps)\n",
    "            else:\n",
    "                delta_j = torch.rand(j.size())\n",
    "\n",
    "            # Add adversarial perturbation to embeddings, now we have q+delta, p+delta\n",
    "            x_ui_adv = torch.mul(u + delta_u, i + delta_i).sum(dim=1)\n",
    "            x_uj_adv = torch.mul(u + delta_u, j + delta_j).sum(dim=1)\n",
    "\n",
    "            # find difference between pos and neg item, then clip value\n",
    "            x_uij_adv = torch.clamp(x_ui_adv - x_uj_adv,min=-80.0,max=1e8)\n",
    "\n",
    "            # Calculate APR loss with logsigmoid\n",
    "            log_prob = F.logsigmoid(x_uij_adv).sum()\n",
    "            adv_loss = self.reg_adv *(-log_prob) + loss # this is adversarial loss (equation 4 in paper)\n",
    "            adv_loss.backward()\n",
    "\n",
    "            return adv_loss\n",
    "\n",
    "## similar set of with batch size = 512\n",
    "def evaluate_k(user_emb, item_emb, train_user_list, test_user_list, klist, batch=512):\n",
    "    \"\"\"Compute HR and NDCG at k.\n",
    "\n",
    "    Args:\n",
    "        user_emb (torch.Tensor): embedding for user [user_num, dim]\n",
    "        item_emb (torch.Tensor): embedding for item [item_num, dim]\n",
    "        train_user_list (list(set)):\n",
    "        test_user_list (list(set)):\n",
    "        k (list(int)):\n",
    "    Returns:\n",
    "        (torch.Tensor, torch.Tensor) HR and NDCG at k\n",
    "    \"\"\"\n",
    "\n",
    "    # Calculate max k value\n",
    "    max_k = max(klist)\n",
    "    result = None\n",
    "\n",
    "    # no iteration = user_num / batch size (which is 512)\n",
    "    for i in range(0, user_emb.shape[0], batch):\n",
    "\n",
    "        # Construct mask for each batch\n",
    "\n",
    "        #new_ones returns a Tensor of size size filled with 1\n",
    "\n",
    "        # size of the mask vector = (min of batch or user embed) * item+embed\n",
    "        mask = user_emb.new_ones([min([batch, user_emb.shape[0]-i]), item_emb.shape[0]])\n",
    "        for j in range(batch):\n",
    "            if i+j >= user_emb.shape[0]:\n",
    "                break\n",
    "            mask[j].scatter_(dim=0, index=torch.tensor(list(train_user_list[i + j])), value=torch.tensor(0.0))\n",
    "\n",
    "        # Get current result\n",
    "        cur_result = torch.mm(user_emb[i:i+min(batch, user_emb.shape[0]-i), :], item_emb.t())\n",
    "        cur_result = torch.sigmoid(cur_result)\n",
    "        assert not torch.any(torch.isnan(cur_result))\n",
    "\n",
    "        # Make zero for already observed item\n",
    "        cur_result = torch.mul(mask, cur_result)\n",
    "        _, cur_result = torch.topk(cur_result, k=max_k, dim=1)\n",
    "        result = cur_result if result is None else torch.cat((result, cur_result), dim=0)\n",
    "\n",
    "\n",
    "    ## basically this chunk collects the results\n",
    "    result = result.cpu()\n",
    "\n",
    "    # Sort indice and get HR_NDCG_topk\n",
    "    HRs, NDCGs = [], []\n",
    "    for k in klist:\n",
    "        ndcg, hr = 0, 0\n",
    "        for i in range(user_emb.shape[0]):\n",
    "            test = set(test_user_list[i])\n",
    "            pred = set(result[i, :k].numpy().tolist())\n",
    "            val = len(test & pred)\n",
    "            hr += val / max([len(test), 1])\n",
    "            pred = list(pred)\n",
    "            x = int(test_user_list[i][0])\n",
    "            if pred.count(x) != 0:\n",
    "                position = pred.index(x)\n",
    "                ndcg += math.log(2) / math.log(position + 2) if position < k else 0\n",
    "            else:\n",
    "                ndcg += 0\n",
    "        NDCGs.append(ndcg / user_emb.shape[0])\n",
    "        HRs.append(hr / user_emb.shape[0])\n",
    "        NDCGs.append(ndcg / user_emb.shape[0])\n",
    "    return HRs, NDCGs\n",
    "\n",
    "\n",
    "def main(args):\n",
    "    # Initialize seed\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "\n",
    "    # Load preprocess data\n",
    "    with open(args.data, 'rb') as f:\n",
    "        dataset = pickle.load(f)\n",
    "        user_size, item_size = dataset['user_size'], dataset['item_size']\n",
    "        train_user_list, test_user_list = dataset['train_user_list'], dataset['test_user_list']\n",
    "        train_pair = dataset['train_pair']\n",
    "\n",
    "    # Create dataset, model, optimizer\n",
    "    dataset = GetTriplePair(item_size, train_user_list, train_pair, True, args.epochs)\n",
    "\n",
    "    # load batch of 512 item triplets\n",
    "    loader = DataLoader(dataset, batch_size=args.batch_size)\n",
    "    model = MF(user_size, item_size, args.dim, args.reg, args.reg_adv, args.eps)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
    "\n",
    "    # Training\n",
    "    start_time = time.time()\n",
    "    eval_best_loss = float('inf')\n",
    "    optimizer.zero_grad()\n",
    "    epoch = 0\n",
    "    HR_history = []\n",
    "    NDCG_history = []\n",
    "    for u, i, j in loader:\n",
    "        if epoch in range(args.epochs + args.adv_epoch):\n",
    "            loss = model(u, i, j, epoch)\n",
    "            optimizer.step()\n",
    "            HR_list, NDCG_list = evaluate_k(model.W.detach(),\n",
    "                                                               model.H.detach(),\n",
    "                                                               train_user_list,\n",
    "                                                               test_user_list,\n",
    "                                                               klist=[50, 100])\n",
    "            if epoch % args.verbose == (args.verbose - 1):\n",
    "                if epoch in range(args.epochs):\n",
    "                    print('BPR-MF Epoch [{}/{}]'.format(epoch + 1, args.epochs + args.adv_epoch))\n",
    "                if epoch in range(args.epochs, args.adv_epoch + args.epochs):\n",
    "                    print('AMF Epoch [{}/{}]'.format(epoch + 1, args.epochs + args.adv_epoch))\n",
    "                print('loss: %.4f' % loss)\n",
    "                print('HR@50: %.4f, HR@100: %.4f, NDCG@50: %.4f, NDCG@100: %.4f' % (\n",
    "                    HR_list[0], HR_list[1], NDCG_list[0], NDCG_list[1]))\n",
    "            HR_history.append(HR_list[1])\n",
    "            NDCG_history.append(NDCG_list[1])\n",
    "            if epoch % 100 == 0:\n",
    "                if loss < eval_best_loss:\n",
    "                    eval_best_loss = loss\n",
    "                    dirname = os.path.dirname(os.path.abspath(args.model))\n",
    "                    os.makedirs(dirname, exist_ok=True)\n",
    "                    torch.save(model.state_dict(), args.model)\n",
    "                    time_dif = get_time_dif(start_time)\n",
    "                    print(\"time\", time_dif)\n",
    "            epoch += 1\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    fig_HR = plt.figure(edgecolor='blue')\n",
    "    ax1 = fig_HR.add_subplot(111)\n",
    "    plt.ylabel('HR@100')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.title('Yelp')\n",
    "    ax1.plot(range(len(HR_history)), HR_history, c=np.array([255, 71, 90]) / 255.)\n",
    "    plt.show()\n",
    "    fig_P = plt.figure(edgecolor='blue')\n",
    "    ax1 = fig_P.add_subplot(111)\n",
    "    plt.ylabel('NDCG@100')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.title('Yelp')\n",
    "    ax1.plot(range(len(NDCG_history)), NDCG_history, c=np.array([255, 71, 90]) / 255.)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Parse argument\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--data',\n",
    "                        type=str,\n",
    "                        default=os.path.join('preprocessed', 'ml-1m.pickle'),\n",
    "                        help=\"File path for data\")\n",
    "    # Seed\n",
    "    parser.add_argument('--seed',\n",
    "                        type=int,\n",
    "                        default=0,\n",
    "                        help=\"Seed (For reproducability)\")\n",
    "    # Model, default embedding size = 64\n",
    "    parser.add_argument('--dim',\n",
    "                        type=int,\n",
    "                        default=64,\n",
    "                        help=\"Dimension for embedding\")\n",
    "    # Optimizer, learning rate is different @ 0.00025\n",
    "    parser.add_argument('--lr',\n",
    "                        type=float,\n",
    "                        default= 0.00025,\n",
    "                        help=\"Learning rate\")\n",
    "    parser.add_argument('--reg',\n",
    "                        type=float,\n",
    "                        default=0,\n",
    "                        help=\"Regularization for user and item embeddings.\")\n",
    "    # Training\n",
    "    parser.add_argument('--epochs',\n",
    "                        type=int,\n",
    "                        default=1000,\n",
    "                        help=\"Number of epoch during training\")\n",
    "    parser.add_argument('--batch_size',\n",
    "                        type=int,\n",
    "                        default=2000,\n",
    "                        help=\"Batch size in one iteration\")\n",
    "    parser.add_argument('--verbose',\n",
    "                        type=int,\n",
    "                        default=20,\n",
    "                        help=\"Evaluate per X epochs\")\n",
    "    parser.add_argument('--eval_every',\n",
    "                        type=int,\n",
    "                        default=20,\n",
    "                        help=\"Period for evaluating precision and recall during training\")\n",
    "    parser.add_argument('--model',\n",
    "                        type=str,\n",
    "                        default=os.path.join('output', 'bpr.pt'),\n",
    "                        help=\"File path for model\")\n",
    "    #original paper has default regularization coefficient and epsilon = 1 and 0.5 respectively\n",
    "\n",
    "    parser.add_argument('--reg_adv', type=float, default=1,\n",
    "                        help='Regularization for adversarial loss')\n",
    "    parser.add_argument('--adv_epoch', type=int, default=1000,\n",
    "                        help='Add APR in epoch X, when adv_epoch is 0, it\\'s equivalent to pure AMF.\\n '\n",
    "                             'And when adv_epoch is larger than epochs, it\\'s equivalent to pure MF model. ')\n",
    "    parser.add_argument('--eps', type=float, default=0.5,\n",
    "                        help='Epsilon for adversarial weights.')\n",
    "    args = parser.parse_args()\n",
    "    main(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08e030d6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mparameters()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model.parameters()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
