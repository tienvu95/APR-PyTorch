{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6456030",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-15 16:52:07.158290: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-15 16:52:07.258079: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "from keras.regularizers import l1,l2\n",
    "from keras.models import Sequential,Model\n",
    "from keras.layers import Embedding,Input,Dense,Flatten,Lambda, Multiply, Concatenate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c393bbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('W',\n",
       "              tensor([[ 0.1027, -0.2571,  0.2280,  ..., -0.3319,  0.1107, -0.0547],\n",
       "                      [-0.2378,  0.1023, -0.2476,  ..., -0.1183,  0.2373,  0.3497],\n",
       "                      [ 0.1866, -0.2054, -0.2772,  ..., -0.2186,  0.2774,  0.2513],\n",
       "                      ...,\n",
       "                      [ 0.1794, -0.2494,  0.2269,  ..., -0.2170,  0.0641,  0.0161],\n",
       "                      [ 0.2467, -0.1777,  0.1404,  ..., -0.2994, -0.1175, -0.0339],\n",
       "                      [-0.1238,  0.1703, -0.1914,  ...,  0.0497,  0.2560, -0.1212]])),\n",
       "             ('H',\n",
       "              tensor([[-0.1210, -0.0994, -0.2235,  ..., -0.2647, -0.2383, -0.1061],\n",
       "                      [-0.1862,  0.1676, -0.3127,  ..., -0.2307, -0.2465, -0.3151],\n",
       "                      [-0.2041,  0.2428, -0.3161,  ..., -0.2608, -0.1408, -0.2574],\n",
       "                      ...,\n",
       "                      [-0.2339, -0.0100,  0.1009,  ..., -0.0604, -0.1932, -0.1700],\n",
       "                      [ 0.0182,  0.1940, -0.2179,  ..., -0.1539, -0.0744, -0.2069],\n",
       "                      [-0.1560,  0.2314,  0.2187,  ...,  0.2337, -0.1828, -0.1758]]))])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.load('/home/vuhoang181/Documents/code_base/APR-PyTorch/output/bpr.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79641089",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(num_users,num_items,mf_dim=10,layers=[10],reg_layers=[0,0,0,0],reg_mf=0):\n",
    "    num_layer=len(layers)\n",
    "\n",
    "    user_input=Input(shape=(1,),dtype='int32')\n",
    "    item_input_pos=Input(shape=(1,),dtype='int32')\n",
    "    item_input_neg = Input(shape=(1,), dtype='int32')\n",
    "\n",
    "    ## in this context each point is projected to have 10 dimensional coordinates?\n",
    "    MF_embedding_user=Embedding(input_dim=num_users,output_dim=mf_dim,embeddings_initializer='random_normal',\n",
    "                                name='mf_user_embedding',embeddings_regularizer=l2(reg_mf),input_length=1)\n",
    "    MF_embedding_item = Embedding(input_dim=num_items, output_dim=mf_dim, embeddings_initializer='random_normal',\n",
    "                                  name='mf_item_embedding',embeddings_regularizer=l2(reg_mf), input_length=1)\n",
    "    MLP_embedding_user=Embedding(input_dim=num_users,output_dim=layers[0],embeddings_initializer='random_normal',\n",
    "                                 name='mlp_user_embedding', embeddings_regularizer=l2(reg_mf),input_length=1)\n",
    "    MLP_embedding_item = Embedding(input_dim=num_items, output_dim=layers[0], embeddings_initializer='random_normal',\n",
    "                                   name='mlp_item_embedding',embeddings_regularizer=l2(reg_mf), input_length=1)\n",
    "\n",
    "    mf_user_latent=Flatten()(MF_embedding_user(user_input))\n",
    "    mf_item_latent_pos=Flatten()(MF_embedding_item(item_input_pos))\n",
    "    mf_item_latent_neg = Flatten()(MF_embedding_item(item_input_neg))\n",
    "\n",
    "    ## merge = deprecated use keras.layers.Concatenate(axis=-1) instead\n",
    "    prefer_pos = merge([mf_user_latent, mf_item_latent_pos], mode='mul')\n",
    "    prefer_neg = merge([mf_user_latent, mf_item_latent_neg], mode='mul')\n",
    "    ## convert negative layer to negative, lambda layers should be re-written as subclass layer if too complex (eg: multiply by scale?)\n",
    "    prefer_neg = Lambda(lambda x: -x)(prefer_neg)\n",
    "    ## basically just merge 2 layer\n",
    "    mf_vector = merge([prefer_pos, prefer_neg], mode='concat')\n",
    "\n",
    "    ## flat matrix to an array\n",
    "    mlp_user_latent=Flatten()(MLP_embedding_user(user_input))\n",
    "    mlp_item_latent_pos=Flatten()(MLP_embedding_item(item_input_pos))\n",
    "    mlp_item_latent_neg=Flatten()(MLP_embedding_item(item_input_neg))\n",
    "    mlp_item_latent_neg=Lambda(lambda x:-x)(mlp_item_latent_neg)\n",
    "    mlp_vector=merge([mlp_user_latent,mlp_item_latent_pos,mlp_item_latent_neg],mode='concat')\n",
    "    for idx in range(1,num_layer):\n",
    "        #set up hidden layer of the network, why tanh and have to regularize?? L1 consider weight, l2 consider square of weight\n",
    "        layer=Dense(layers[idx],kernel_regularizer=l2(0.0000),activation='tanh',name=\"layer%d\" %idx)\n",
    "        mlp_vector=layer(mlp_vector)\n",
    "\n",
    "    ## concatenation of NBPR layer and DNCR layer\n",
    "    predict_vector=merge([mf_vector,mlp_vector],mode='concat')\n",
    "\n",
    "\n",
    "    #set up prediction --> final layer, sigmoid activate to give binary output\n",
    "    prediction=Dense(1,activation='sigmoid',kernel_initializer='lecun_uniform',name='prediction')(predict_vector)\n",
    "    model=Model(inputs=[user_input,item_input_pos,item_input_neg],outputs=prediction)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "efe4fdc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input=Input(shape=(1,),dtype='int32')\n",
    "item_input_pos=Input(shape=(1,),dtype='int32')\n",
    "item_input_neg = Input(shape=(1,), dtype='int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cdc327c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "mf_dim=10\n",
    "layers=[10]\n",
    "num_layer=len(layers)\n",
    "reg_layers=[0,0,0,0]\n",
    "reg_mf=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2e40dcb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "MF_embedding_user=Embedding(input_dim=2000,output_dim=mf_dim,embeddings_initializer='random_normal',\n",
    "                                name='mf_user_embedding',embeddings_regularizer=l2(reg_mf),input_length=1)\n",
    "MF_embedding_item = Embedding(input_dim=50000, output_dim=mf_dim, embeddings_initializer='random_normal',\n",
    "                                  name='mf_item_embedding',embeddings_regularizer=l2(reg_mf), input_length=1)\n",
    "MLP_embedding_user=Embedding(input_dim=2000,output_dim=layers[0],embeddings_initializer='random_normal',\n",
    "                                 name='mlp_user_embedding', embeddings_regularizer=l2(reg_mf),input_length=1)\n",
    "MLP_embedding_item = Embedding(input_dim=50000, output_dim=layers[0], embeddings_initializer='random_normal',\n",
    "                                   name='mlp_item_embedding',embeddings_regularizer=l2(reg_mf), input_length=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "be3b68ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-14 23:27:03.991416: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "mf_user_latent=Flatten()(MF_embedding_user(user_input))\n",
    "mf_item_latent_pos=Flatten()(MF_embedding_item(item_input_pos))\n",
    "mf_item_latent_neg = Flatten()(MF_embedding_item(item_input_neg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4fdbf484",
   "metadata": {},
   "outputs": [],
   "source": [
    "## merge = deprecated use keras.layers.Concatenate(axis=-1) instead\n",
    "prefer_pos = Multiply()([mf_user_latent, mf_item_latent_pos])\n",
    "prefer_neg = Multiply()([mf_user_latent, mf_item_latent_neg])\n",
    "## convert negative layer to negative, lambda layers should be re-written as subclass layer if too complex (eg: multiply by scale?)\n",
    "prefer_neg = Lambda(lambda x: -x)(prefer_neg)\n",
    "## basically just merge 2 layer\n",
    "mf_vector = Concatenate()([prefer_pos, prefer_neg])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b20f4e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_user_latent=Flatten()(MLP_embedding_user(user_input))\n",
    "mlp_item_latent_pos=Flatten()(MLP_embedding_item(item_input_pos))\n",
    "mlp_item_latent_neg=Flatten()(MLP_embedding_item(item_input_neg))\n",
    "mlp_item_latent_neg=Lambda(lambda x:-x)(mlp_item_latent_neg)\n",
    "mlp_vector=Concatenate()([mlp_user_latent,mlp_item_latent_pos,mlp_item_latent_neg])\n",
    "for idx in range(1,num_layer):\n",
    "    #set up hidden layer of the network, why tanh and have to regularize?? L1 consider weight, l2 consider square of weight\n",
    "    layer=Dense(layers[idx],kernel_regularizer=l2(0.0000),activation='tanh',name=\"layer%d\" %idx)\n",
    "    mlp_vector=layer(mlp_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2f9ce829",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_vector=Concatenate()([mf_vector,mlp_vector])\n",
    "\n",
    "\n",
    "#set up prediction --> final layer, sigmoid activate to give binary output\n",
    "prediction=Dense(3,kernel_initializer='lecun_uniform',name='prediction')(predict_vector)\n",
    "model=Model(inputs=[user_input,item_input_pos,item_input_neg],outputs=prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9886b91e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KerasTensor: shape=(None, 3) dtype=float32 (created by layer 'prediction')>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fe985b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer=Dense(1,kernel_regularizer=l2(0.0000),activation='tanh')\n",
    "mlp_vector=layer(mf_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "62650712",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KerasTensor: shape=(None, 1) dtype=float32 (created by layer 'dense')>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9406f5de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6d221ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layer=len(layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c08cc799",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "134400d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0071, -0.0003, -0.0036,  ..., -0.0109, -0.0021, -0.0093],\n",
       "        [-0.0168,  0.0075,  0.0166,  ...,  0.0210,  0.0088,  0.0050],\n",
       "        [ 0.0069,  0.0208, -0.0190,  ...,  0.0083,  0.0040, -0.0226],\n",
       "        ...,\n",
       "        [ 0.0188, -0.0042,  0.0058,  ..., -0.0114,  0.0023,  0.0108],\n",
       "        [ 0.0068, -0.0273,  0.0137,  ..., -0.0128,  0.0244,  0.0166],\n",
       "        [ 0.0157, -0.0429, -0.0156,  ..., -0.0312, -0.0007,  0.0045]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W = nn.Parameter(torch.empty(2000, 64))  # User embedding\n",
    "H = nn.Parameter(torch.empty(10000, 64))  # Item embedding\n",
    "nn.init.xavier_normal_(W.data)\n",
    "nn.init.xavier_normal_(H.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e9525373",
   "metadata": {},
   "outputs": [],
   "source": [
    "u=W[1,:]\n",
    "i=H[1,:]\n",
    "j=H[2,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "25dfa048",
   "metadata": {},
   "outputs": [],
   "source": [
    "u.retain_grad()\n",
    "i.retain_grad()\n",
    "j.retain_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7e5d7dfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.0072, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mul(u, i).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dd08aba2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0308,  0.0214, -0.0337, -0.0007,  0.0668,  0.0039,  0.0473, -0.0047,\n",
       "        -0.0129, -0.0313, -0.0083,  0.0202,  0.0343, -0.0094, -0.0246,  0.0225,\n",
       "        -0.0211,  0.0477, -0.0036, -0.0222, -0.0022, -0.0409, -0.0026, -0.0114,\n",
       "         0.0423, -0.0686,  0.0195,  0.0083, -0.0347,  0.0004, -0.0054,  0.0049,\n",
       "        -0.0268, -0.0576, -0.0233, -0.0235, -0.0006,  0.0016, -0.0176,  0.0523,\n",
       "        -0.0086,  0.0087,  0.0148,  0.0087,  0.0461, -0.0197,  0.0121, -0.0110,\n",
       "        -0.0299, -0.0086, -0.0063, -0.0063,  0.0055, -0.0220, -0.0304, -0.0149,\n",
       "        -0.0629,  0.0318,  0.0386, -0.0172, -0.0283, -0.0047, -0.0392, -0.0386],\n",
       "       grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "abc4a154",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a87a2b20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.6968, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.logsigmoid(torch.mul(u, i).sum()).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f942c5ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0308,  0.0214, -0.0337, -0.0007,  0.0668,  0.0039,  0.0473, -0.0047,\n",
       "        -0.0129, -0.0313, -0.0083,  0.0202,  0.0343, -0.0094, -0.0246,  0.0225,\n",
       "        -0.0211,  0.0477, -0.0036, -0.0222, -0.0022, -0.0409, -0.0026, -0.0114,\n",
       "         0.0423, -0.0686,  0.0195,  0.0083, -0.0347,  0.0004, -0.0054,  0.0049,\n",
       "        -0.0268, -0.0576, -0.0233, -0.0235, -0.0006,  0.0016, -0.0176,  0.0523,\n",
       "        -0.0086,  0.0087,  0.0148,  0.0087,  0.0461, -0.0197,  0.0121, -0.0110,\n",
       "        -0.0299, -0.0086, -0.0063, -0.0063,  0.0055, -0.0220, -0.0304, -0.0149,\n",
       "        -0.0629,  0.0318,  0.0386, -0.0172, -0.0283, -0.0047, -0.0392, -0.0386],\n",
       "       grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "49fc65b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_vector=torch.stack((u,i,j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1c8c9f5c",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'torch.Size' object has no attribute 'rank'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1161558/4081738527.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mkernel_regularizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0ml2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.0000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'tanh'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'final'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmlp_vector\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/keras/engine/input_spec.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[0;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[1;32m    246\u001b[0m                 )\n\u001b[1;32m    247\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin_ndim\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 248\u001b[0;31m             \u001b[0mndim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrank\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mndim\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mndim\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin_ndim\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m                 raise ValueError(\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'torch.Size' object has no attribute 'rank'"
     ]
    }
   ],
   "source": [
    "Dense(1,kernel_regularizer=l2(0.0000),activation='tanh',name='final')(mlp_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "1a241755",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = torch.nn.Linear(mlp_vector.shape[1], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d3cd2111",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "x = torch.tensor([[1.0, -1.0],\n",
    "                  [0.0,  1.0],\n",
    "                  [0.0,  0.0]])\n",
    "\n",
    "in_features = x.shape[1]  # = 2\n",
    "out_features = 2\n",
    "\n",
    "m = nn.Linear(in_features, out_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "4ae334f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0586],\n",
       "        [-0.0656],\n",
       "        [-0.0755]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m(mlp_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "15d43729",
   "metadata": {},
   "outputs": [],
   "source": [
    "m1 = nn.Linear(64, 32)\n",
    "m2 = nn.Linear(32,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "e23ed1c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0560],\n",
       "        [-0.0568],\n",
       "        [-0.0535]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m2(m1(mlp_vector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "c8ab21ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 1),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "884cd11d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1656],\n",
       "        [0.1633],\n",
       "        [0.1632]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_relu_stack(mlp_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "efc276a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1633], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_relu_stack(mlp_vector)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b16cf1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
