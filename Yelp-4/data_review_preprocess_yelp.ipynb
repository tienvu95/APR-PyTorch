{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "import string\n",
    "\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import argparse\n",
    "import copy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "user_dict = pickle.load(open('./user_id_dict.pkl'))\n",
    "item_dict = pickle.load(open('./item_id_dict.pkl'))\n",
    "item_genre_dict = pickle.load(open('./item_genre_dict.pkl'))\n",
    "genre_count = pickle.load(open('./genre_count.pkl'))\n",
    "\n",
    "user_idd_list = pickle.load(open('./user_idd_list.pkl'))\n",
    "item_idd_list = pickle.load(open('./item_idd_list.pkl'))\n",
    "key_genre = pickle.load(open('./key_genre.pkl'))\n",
    "item_idd_genre_list = pickle.load(open('./item_idd_genre_list.pkl'))\n",
    "\n",
    "train_df = pickle.load(open('./training_df.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/5261669\n",
      "100000/5261669\n",
      "200000/5261669\n",
      "300000/5261669\n",
      "400000/5261669\n",
      "500000/5261669\n",
      "600000/5261669\n",
      "700000/5261669\n",
      "800000/5261669\n",
      "900000/5261669\n",
      "1000000/5261669\n",
      "1100000/5261669\n",
      "1200000/5261669\n",
      "1300000/5261669\n",
      "1400000/5261669\n",
      "1500000/5261669\n",
      "1600000/5261669\n",
      "1700000/5261669\n",
      "1800000/5261669\n",
      "1900000/5261669\n",
      "2000000/5261669\n",
      "2100000/5261669\n",
      "2200000/5261669\n",
      "2300000/5261669\n",
      "2400000/5261669\n",
      "2500000/5261669\n",
      "2600000/5261669\n",
      "2700000/5261669\n",
      "2800000/5261669\n",
      "2900000/5261669\n",
      "3000000/5261669\n",
      "3100000/5261669\n",
      "3200000/5261669\n",
      "3300000/5261669\n",
      "3400000/5261669\n",
      "3500000/5261669\n",
      "3600000/5261669\n",
      "3700000/5261669\n",
      "3800000/5261669\n",
      "3900000/5261669\n",
      "4000000/5261669\n",
      "4100000/5261669\n",
      "4200000/5261669\n",
      "4300000/5261669\n",
      "4400000/5261669\n",
      "4500000/5261669\n",
      "4600000/5261669\n",
      "4700000/5261669\n",
      "4800000/5261669\n",
      "4900000/5261669\n",
      "5000000/5261669\n",
      "5100000/5261669\n",
      "5200000/5261669\n"
     ]
    }
   ],
   "source": [
    "'''HFT train set preprocessing'''\n",
    "\n",
    "from time import *\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "user_item_id_tuple_list = []\n",
    "for pair in list(train_df.loc[:, ['user_id', 'item_id']].values):\n",
    "    user_item_id_tuple_list.append(tuple(pair))\n",
    "user_item_id_tuple_set = set(user_item_id_tuple_list)\n",
    "\n",
    "ps = PorterStemmer(PorterStemmer.NLTK_EXTENSIONS)\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "f = open('./review.json')\n",
    "reviews = f.readlines()\n",
    "f.close()\n",
    "\n",
    "fw = open('./HFT_Yelp.txt', 'w')\n",
    "\n",
    "user_set = set(user_dict.keys())\n",
    "item_set = set(item_dict.keys())\n",
    "\n",
    "l_r = len(reviews)\n",
    "l = 0\n",
    "for review in reviews:\n",
    "    review = json.loads(review)\n",
    "    user = review['user_id']\n",
    "    item = review['business_id']\n",
    "    if l % 100000 == 0:\n",
    "        print(str(l) + '/' + str(l_r))\n",
    "    l += 1\n",
    "    if not (user in user_set and item in item_set):\n",
    "        continue\n",
    "    u_idx = user_dict[user]\n",
    "    i_idx = item_dict[item]\n",
    "    if tuple([u_idx, i_idx]) in user_item_id_tuple_set:\n",
    "        rating = review['stars']\n",
    "        temp = (review['text'])\n",
    "        tokens = text_to_word_sequence(temp, filters='\\'!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n', lower=True, split=' ')\n",
    "        tokens[:] = (ps.stem(t) for t in tokens if t != '' and not t.isdigit() and t not in stop_words)\n",
    "        temp = ' '.join(tokens)\n",
    "        temp = filter(lambda x: x in string.printable, temp)\n",
    "        wcount = len(temp.split())\n",
    "        fw.write(user + ' ' + item + ' ' + str(rating) + ' 0 ' + str(wcount) + ' ' + temp + '\\n')\n",
    "    \n",
    "fw.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''HFT test and vali set preprocessing'''\n",
    "user_dict = pickle.load(open('./user_id_dict.pkl'))\n",
    "item_dict = pickle.load(open('./item_id_dict.pkl'))\n",
    "user_idd_list = pickle.load(open('./user_idd_list.pkl'))\n",
    "item_idd_list = pickle.load(open('./item_idd_list.pkl'))\n",
    "test_df = pickle.load(open('./testing_df.pkl'))\n",
    "vali_df = pickle.load(open('./valiing_df.pkl'))\n",
    "fw_test = open('./HFT_Yelp_test.txt', 'w')\n",
    "fw_vali = open('./HFT_Yelp_vali.txt', 'w')\n",
    "\n",
    "user_set = set(user_dict.keys())\n",
    "item_set = set(item_dict.keys())\n",
    "\n",
    "for i in range(len(test_df)):\n",
    "    user = user_idd_list[test_df.at[i, 'user_id']]\n",
    "    item = item_idd_list[test_df.at[i, 'item_id']]\n",
    "    rating = test_df.at[i, 'rating']\n",
    "    fw_test.write(user + ' ' + item + ' ' + str(rating) + ' 0 0 ' + '\\n')\n",
    "    \n",
    "for i in range(len(vali_df)):\n",
    "    user = user_idd_list[vali_df.at[i, 'user_id']]\n",
    "    item = item_idd_list[vali_df.at[i, 'item_id']]\n",
    "    rating = vali_df.at[i, 'rating']\n",
    "    fw_vali.write(user + ' ' + item + ' ' + str(rating) + ' 0 0 ' + '\\n')\n",
    "\n",
    "fw_test.close()\n",
    "fw_vali.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
